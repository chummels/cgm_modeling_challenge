{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21c38063-dde6-421f-a5b0-8a1d3320f44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import tqdm\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b816548-85d0-44e9-abf7-9be458930aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt\n",
    "import trident\n",
    "import unyt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0164758a-30ce-4190-9afe-911c96c2dd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kalepy as kale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a5de870-5fd1-4465-b7dd-bd452f2b13cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trove\n",
    "import verdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7eadfbae-2549-4873-8e31-47c5ad3048a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.style.use( '/Users/zhafen/repos/clean-bold/clean-bold.mplstyle' )\n",
    "import palettable\n",
    "import matplotlib.patheffects as path_effects"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67801afa-d127-43dc-a211-164c72b318b0",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ede9ac4-52f5-4e59-9eeb-b720128fc41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm = {\n",
    "    # Analysis \n",
    "    'prop_keys': [ 'vlos', 'T', 'nH', 'Z' ],\n",
    "    'vel_prop_keys': [ 'vlos', 'T', 'nH', 'Z', 'NHI' ],\n",
    "    'broaden_models': True,\n",
    "    '1D_dist_estimation': 'kde',\n",
    "    '1D_dist_estimation_data': 'histogram',\n",
    "    '2D_dist_estimation': 'histogram',\n",
    "    \n",
    "    # Plotting Choices\n",
    "    'smooth_2D_dist': 0.5,\n",
    "    'upsample_2D_dist': 3,\n",
    "    '2D_dist_data_display': 'histogram',\n",
    "    'contour_levels': [ 90, 50 ],\n",
    "    'contour_linewidths': [ 1, 3 ],\n",
    "    'show_plots_in_nb': False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "59d8a89b-eb8b-4082-a33e-f5ae75481b27",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No data directory at /Users/zhafen/data/cgm_modeling_challenge/sample2/original\n",
      "Creating one.\n"
     ]
    }
   ],
   "source": [
    "# Load parameters\n",
    "pm = trove.link_params_to_config(\n",
    "    '/Users/zhafen/analysis/cgm_modeling_challenge/sample2.trove',\n",
    "    script_id = 'nb.2',\n",
    "    variation = 'original',\n",
    "    global_variation = '',\n",
    "    **pm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd61cc8-4320-4690-9cf4-93e2d6989359",
   "metadata": {},
   "outputs": [],
   "source": [
    "redshift = pm['redshift']\n",
    "prop_keys = pm['prop_keys']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4395b3-67c4-44ab-bbdb-771c8feb75dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pm['data_dir']\n",
    "base_data_dir = pm['base_data_dir']\n",
    "ray_dir = os.path.join( base_data_dir, 'rays' )\n",
    "results_dir = os.path.join( base_data_dir, 'modeling' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a587b8-112e-4bc2-a79e-5f2bd235ff38",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b33a4f-c66e-4f93-a1ca-c369081ad97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_coefficients = {\n",
    "    'one-sided': {},\n",
    "    'log one-sided': { 'logscale': True, 'subtract_mean': True },\n",
    "    'two-sided': { 'one_sided': False, },\n",
    "    'linear': { 'one_sided': False, 'subtract_mean': True },\n",
    "    'log': { 'logscale': True, 'one_sided': False, 'subtract_mean': True },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc194a2e-881d-4afa-8f01-d5eea240f85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lims = {\n",
    "    'vlos': [ -300, 300 ],\n",
    "    'T': [ 1e2, 2.5e6 ],\n",
    "    'nH': [ 1e-7, 100 ],\n",
    "    'Z': [ 1e-3, 30 ],\n",
    "    'NHI': [ 1e9, 1e17 ],\n",
    "}\n",
    "autolims = {\n",
    "    'vlos': False,\n",
    "    'T': False,\n",
    "    'nH': False,\n",
    "    'Z': False,\n",
    "    'NHI': False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a2e0cd-10e8-4d93-a75c-d1dd55ee5b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "lims_1D = {\n",
    "    'vlos': [ 3e9, 2e16 ],\n",
    "    'T': [ 1e12, 1e20 ],\n",
    "    'nH': [ 1e12, 1e20 ],\n",
    "    'Z': [ 1e12, 1e20 ],\n",
    "    'NHI': [ 1e12, 1e20 ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1293db0b-c166-453b-ac70-4b1b5e751125",
   "metadata": {},
   "outputs": [],
   "source": [
    "dvs = {\n",
    "    'vlos': 5.,\n",
    "    'T': 0.05,\n",
    "    'nH': 0.05,\n",
    "    'Z': 0.05,\n",
    "    'NHI': 0.05,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e0429a-4359-43f1-835d-09c099a9543e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logscale = {\n",
    "    'vlos': False,\n",
    "    'T': True,\n",
    "    'nH': True,\n",
    "    'Z': True,\n",
    "    'NHI': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03ea2c2-ae09-44a6-8ea4-a74a1a8f278a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_format = pm['bar_format']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0e6452-3293-4ccb-ade0-679aa24cc559",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b87a384-9e11-4a6d-8c87-2383c2ff5c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\n",
    "    'vlos': r'$v_{\\rm LOS}$ [km/s]',\n",
    "    'T': r'T [K]',\n",
    "    'nH': r'$n_{\\rm H}$ [cm$^{-3}$]',\n",
    "    'Z': r'$Z$ [$Z_{\\odot}$]',\n",
    "    'NHI': r'$N_{\\rm H\\,I}$ [cm$^{-2}$]',\n",
    "}\n",
    "labels_1D = {\n",
    "    'vlos': r'$\\frac{ d N_{\\rm H\\,I} }{d v_{\\rm LOS}}$',\n",
    "    'T': r'$\\frac{ d N_{\\rm H\\,I} }{d \\log T}$',\n",
    "    'nH': r'$\\frac{ d N_{\\rm H\\,I} }{d \\log n_{\\rm H}}$',\n",
    "    'Z': r'$\\frac{ d N_{\\rm H\\,I} }{d \\log Z}$',\n",
    "    'NHI': r'$\\frac{ d N_{\\rm H\\,I} }{d \\log N_{\\rm H\\,I}}$',\n",
    "}\n",
    "r_labels = {}\n",
    "for key, item in labels.items():\n",
    "    unitless_label = item.split( '[' )[0]\n",
    "    r_labels[key] = r'$r($ ' + unitless_label + r'$)$'\n",
    "r_labels['all'] = r'$r($ all $)$'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678ee98c-11e5-4c9f-b52f-28362b0d8fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_markers = {\n",
    "    'one-sided': '^',\n",
    "    'log one-sided': '^',\n",
    "    'two-sided': 'D',\n",
    "    'linear': 'o',\n",
    "    'log': 'o',\n",
    "}\n",
    "correlation_sizes = {\n",
    "    'one-sided': 100,\n",
    "    'log one-sided': 100,\n",
    "    'two-sided': 80,\n",
    "    'linear': 100,\n",
    "    'log': 100,\n",
    "}\n",
    "correlations_plotted = [ 'linear', 'log' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40534fa3-c089-4686-98b1-84eca25a9add",
   "metadata": {},
   "outputs": [],
   "source": [
    "mosaic = [\n",
    "    [ 'vlos', 'legend', '.', '.' ],\n",
    "    [ 'T_vlos', 'T', '.', '.' ],\n",
    "    [ 'nH_vlos', 'nH_T', 'nH', '.' ],\n",
    "    [ 'Z_vlos', 'Z_T', 'Z_nH', 'Z', ],\n",
    "]\n",
    "velocity_mosaic = [\n",
    "    [ 'nH_vlos', 'vlos', ],\n",
    "    [ 'Z_vlos', 'T_vlos', ],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43876f83-da1e-4084-a084-6442fa2c3bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "panel_length = 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546af2d4-0d00-4be7-ae12-48586f50b69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = palettable.cartocolors.qualitative.Safe_10.mpl_colors\n",
    "corr_cmap = palettable.cartocolors.diverging.Temps_2_r.mpl_colormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fdaaed-d447-429e-b69d-d6b32dfc08b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_norm = matplotlib.colors.Normalize( vmin=0, vmax=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580b42fd-3340-4754-a0aa-f4f8e5f24314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_color_linear_cmap( color, name, f_white=0.95, f_saturated=1.0, ):\n",
    "    '''A function that turns a single color into linear colormap that\n",
    "    goes from a color that is whiter than the original color to a color\n",
    "    that is more saturated than the original color.\n",
    "    '''\n",
    "    \n",
    "    color_hsv = matplotlib.colors.rgb_to_hsv( color )\n",
    "    start_color_hsv = copy.copy( color_hsv )\n",
    "    \n",
    "    start_color_hsv = copy.copy( color_hsv )\n",
    "    start_color_hsv[1] -= f_white * start_color_hsv[1]\n",
    "    start_color_hsv[2] += f_white * ( 1. - start_color_hsv[2] )\n",
    "    start_color = matplotlib.colors.hsv_to_rgb( start_color_hsv )\n",
    "    \n",
    "    end_color_hsv = copy.copy( color_hsv )\n",
    "    end_color_hsv[1] += f_saturated * ( 1. - end_color_hsv[1] )\n",
    "    end_color = matplotlib.colors.hsv_to_rgb( end_color_hsv )\n",
    "    \n",
    "    return matplotlib.colors.LinearSegmentedColormap.from_list( name, [ start_color, end_color ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf264d50-746a-4fdb-a00a-fdb886e535e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_modeled = cmap[0]\n",
    "color_data = cmap[1]\n",
    "color_data = ( 0, 0, 0 )\n",
    "cmap_modeled = one_color_linear_cmap( color_modeled, 'modeled' )\n",
    "cmap_data = one_color_linear_cmap( color_data, 'data' )\n",
    "cmap_data = matplotlib.colors.LinearSegmentedColormap.from_list( 'data', [ 'w', color_data ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f924f57-6164-498a-a332-95742c695ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_levels = pm['contour_levels']\n",
    "contour_linewidths = pm['contour_linewidths']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6412343f-f4f3-4354-9f0b-bbaec6742fce",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6689cb9-a83a-451a-a2f8-69904c7d45a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldb = trident.LineDatabase(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429e5062-1c75-4e21-9ddd-d669402c2960",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeff_to_vel( zeff ):\n",
    "    \n",
    "    ainv2 = ( ( 1. + zeff ) / ( 1. + redshift ) )**2.\n",
    "    \n",
    "    v_div_c = ( ainv2 - 1. ) / ( ainv2 + 1. )\n",
    "    return ( v_div_c * unyt.c ).to( 'km/s' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1600ccba-a75e-460b-ac46-21db0888e377",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b1f640-983b-4c87-a1b3-713f1b1808ec",
   "metadata": {},
   "source": [
    "## Modeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91da438d-84f0-4942-b249-556506474395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sightline filepaths\n",
    "sl_fps = []\n",
    "sls = []\n",
    "other_files = []\n",
    "for sl_fp in glob.glob( os.path.join( results_dir, '*' ) ):\n",
    "    \n",
    "    if not os.path.isdir( sl_fp ):\n",
    "        other_files.append( sl_fp )\n",
    "        continue\n",
    "    \n",
    "    i = os.path.split( sl_fp )[-1]\n",
    "    \n",
    "    if int( i ) in pm['selected_sightlines']:\n",
    "\n",
    "        sl_fps.append( sl_fp )\n",
    "        sls.append( i )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4db585e3-39d9-4557-b943-3ecf638052df",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample_turb = pm['n_sample_turb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a494c20f-521a-47fb-8857-b1a2499b8888",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeled_data_formats = {\n",
    "    'v0': {\n",
    "        'col_names': [ 'prob', 'likelihood', 'Z', 'nH', 'T', 'NHI', 'bturb', 'z', ],\n",
    "        'col_units':  [ 1., 1., unyt.Zsun, unyt.cm**-3, unyt.K, unyt.cm**-2, unyt.km / unyt.s, 1. ],\n",
    "        'vlos_from_z': True,\n",
    "    },\n",
    "    'v1': {\n",
    "        'col_names': [ 'prob', 'likelihood', 'Z', 'nH', 'T', 'NHI', 'vlos', 'bturb', 'thickness' ],\n",
    "        'col_units':  [ 1., -0.5, unyt.Zsun, unyt.cm**-3, unyt.K, unyt.cm**-2, unyt.km / unyt.s,  unyt.km / unyt.s, 1. ],\n",
    "        'vlos_from_z': False,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60666e13-a12b-4a43-aa7b-5c9d63af07fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeled_data_format = modeled_data_formats[pm['modeled_data_format']]\n",
    "col_names = modeled_data_format['col_names']\n",
    "col_units = modeled_data_format['col_units']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417bb82e-449a-4faf-b67b-42a2e2de2ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "sl_datas = []\n",
    "sl_useds = []\n",
    "sl_compkeys = []\n",
    "sl_stackeds = []\n",
    "model_weightss = []\n",
    "stacked_weightss = []\n",
    "total_weightss = []\n",
    "median_NHIs = []\n",
    "for i, sl in enumerate( sls ):\n",
    "    \n",
    "    sl_fp = sl_fps[i]\n",
    "    \n",
    "    # Get text files\n",
    "\n",
    "    sl_data = {}\n",
    "    for component_fp in glob.glob( os.path.join( sl_fp, '*' ) ):\n",
    "        component_key = os.path.splitext( os.path.split( component_fp )[-1] )[0]\n",
    "        sl_data[component_key] = pd.read_csv( component_fp, sep=' ', names=col_names )\n",
    "        \n",
    "    # Add LOS velocity and reformat\n",
    "    for component_key, df in sl_data.items():\n",
    "\n",
    "        # Reformat\n",
    "        new_entry = {}\n",
    "        for name in col_names:\n",
    "            values = unyt.unyt_array( df[name].values )\n",
    "            if name in [ 'nH', 'T', 'Z', 'NHI', 'thickness' ]:\n",
    "                new_entry[name] = 10.**values\n",
    "            else:\n",
    "                new_entry[name] = values\n",
    "\n",
    "        # Add LOS velocity\n",
    "        if modeled_data_format['vlos_from_z']:\n",
    "            new_entry['vlos'] = zeff_to_vel( df['z'].values )\n",
    "\n",
    "        # Setup units\n",
    "        for j, name in enumerate( col_names ):\n",
    "            new_entry[name] *= col_units[j]\n",
    "\n",
    "        sl_data[component_key] = new_entry\n",
    "    if modeled_data_format['vlos_from_z']:\n",
    "        col_names.append( 'vlos' )\n",
    "        \n",
    "    # Turn samples into a list\n",
    "    keys = sorted( list( sl_data.keys() ) )\n",
    "    sl_formatted = {}\n",
    "    for name in col_names:\n",
    "        sl_formatted[name] = [ sl_data[_][name] for _ in keys ]\n",
    "\n",
    "    # Generate modeled sample to plot (\"generate\" because we're sampling the doppler broadening)\n",
    "    if pm['broaden_models']:\n",
    "        sl_tiled = {}\n",
    "        for name in col_names:\n",
    "            sl_tiled[name] = []\n",
    "\n",
    "        for j, vlos_j in enumerate( tqdm.tqdm( sl_formatted['vlos'], bar_format=bar_format ) ):\n",
    "            sample_dist = scipy.stats.norm( loc=vlos_j, scale=sl_formatted['bturb'][j]/np.sqrt( 2. ) )\n",
    "            sampled_values = sample_dist.rvs( ( n_sample_turb, vlos_j.size ) )\n",
    "\n",
    "            for name in col_names:\n",
    "                if name != 'vlos':\n",
    "                    arr_tiled = np.hstack( np.tile( sl_formatted[name][j], ( n_sample_turb, 1 ),  ) )\n",
    "                else:\n",
    "                    arr_tiled = np.hstack( sampled_values )\n",
    "\n",
    "                arr_tiled *= sl_formatted[name][j].units\n",
    "\n",
    "                sl_tiled[name].append( arr_tiled )\n",
    "\n",
    "        sl_used = sl_tiled\n",
    "    else:\n",
    "        sl_used = sl_formatted\n",
    "\n",
    "    # Minimum weighting is to make sure no component is overweighted due to number of samples\n",
    "    n_samples_max = np.max([ _.size for _ in sl_used['nH'] ])\n",
    "    if pm['weighting'] is None:\n",
    "        model_weights = [ np.full( _.size, n_samples_max / _.size ) for _ in sl_used['nH'] ]\n",
    "    else:\n",
    "        model_weights = [ np.full( _.size, np.nanmedian( _ ) * n_samples_max / _.size ) for _ in sl_used[pm['weighting']] ]\n",
    "    model_weightss.append( model_weights )\n",
    "    stacked_weightss.append( np.hstack( model_weights ) )\n",
    "    total_weightss.append( np.array([ _.sum() for _ in model_weights ]) )\n",
    "    median_NHIs.append( np.array([ np.nanmedian( _ ) for _ in sl_used['NHI'] ]) )\n",
    "\n",
    "    sl_stacked = {}\n",
    "    for key, item in sl_used.items():\n",
    "        sl_stacked[key] = np.hstack( item ) * item[0].units\n",
    "    \n",
    "    sl_datas.append( sl_data )\n",
    "    sl_useds.append( sl_used )\n",
    "    sl_compkeys.append( keys )\n",
    "    sl_stackeds.append( sl_stacked )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af49e4a-dc51-450a-903a-998d1031dab8",
   "metadata": {},
   "source": [
    "### Best Estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47f8d77a-a4ce-41e9-9bd5-4ee640b3de08",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_mle_data = pd.read_csv( other_files[0], sep=',', )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1314bf57-8764-4155-b199-bc3c98275de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "mle_data = verdict.Dict({})\n",
    "for column_name in raw_mle_data.columns:\n",
    "    \n",
    "    # Get value\n",
    "    value = raw_mle_data[column_name].values[0]\n",
    "    comp_ion, comp_num, name = column_name.split( '_' )\n",
    "    comp_key = '_'.join( [comp_ion, comp_num] )\n",
    "    \n",
    "    # Logscale\n",
    "    if name in [ 'nH', 'T', 'Z', 'NHI', 'thickness' ]:\n",
    "        value = 10.**value\n",
    "    \n",
    "    # Store data\n",
    "    mle_data.setitem( name, value, comp_key, )\n",
    "    \n",
    "        \n",
    "    if name == 'z':\n",
    "        vlos_value = zeff_to_vel( value )\n",
    "        mle_data.setitem( 'vlos', vlos_value, comp_key, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "613da57a-e1c6-48ed-9d71-a3cddde73ef5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup units\n",
    "for j, name in enumerate( col_names ):\n",
    "    \n",
    "    if ( name not in mle_data ) or ( name == 'vlos' ):\n",
    "        continue\n",
    "    \n",
    "    mle_data[name] *= col_units[j]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8094aca-220b-4b0d-8d46-c50d11157138",
   "metadata": {},
   "source": [
    "## Actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ad3c8e-64e9-4a25-841f-55df6fa617ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rays\n",
    "ray_fps = [ os.path.join( ray_dir, 'ray_{}.h5'.format( _[1:] ) ) for _ in sls ]\n",
    "rays = [ yt.load( _ ) for _ in ray_fps ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa36c63-10d6-40ac-a680-02a24a6f637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray_datas = []\n",
    "ray_weights = []\n",
    "for ray in rays:\n",
    "    # Ray properties\n",
    "    trident.add_ion_fields(ray, ions=[ 'H I', ], line_database=ldb)\n",
    "    den = ray.r[('gas', 'number_density')] * 0.75\n",
    "    ray_data = {\n",
    "        'nH': den,\n",
    "        'NH': ( den * ray.r[('gas', 'dl')] ),\n",
    "        'NHI': ray.r[('gas', 'H_p0_number_density')] * ray.r[('gas', 'dl')],\n",
    "        'z': ray.r[('gas', 'redshift_eff')],\n",
    "        'T': ray.r[('gas', 'temperature')],\n",
    "        'Z': ray.r[('gas', 'metallicity')],\n",
    "    }\n",
    "    ray_data['vlos'] = zeff_to_vel( ray_data['z'] )\n",
    "    \n",
    "    if pm['sim_weighting'] is None:\n",
    "        weights = np.ones( ray_data['NH'].shape )\n",
    "    else:\n",
    "        weights = copy.copy( ray_data[pm['sim_weighting']].value )\n",
    "        weights[np.isclose(weights,0.)] = np.nan\n",
    "    \n",
    "    ray_datas.append( ray_data )\n",
    "    ray_weights.append( weights )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73772698-1f28-4f08-b14f-8cd87622c93b",
   "metadata": {},
   "source": [
    "## Bins and Limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585f891c-b83f-40c1-81a1-4312c90a17b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_binss = []\n",
    "all_dxs = []\n",
    "all_centerss = []\n",
    "for i, ray_data in enumerate( tqdm.tqdm( ray_datas, bar_format=bar_format ) ):\n",
    "    \n",
    "    sl_stacked = sl_stackeds[i]\n",
    "\n",
    "    # Make bins, dx, and centers\n",
    "    all_bins = {}\n",
    "    all_dx = {}\n",
    "    all_centers = {}\n",
    "    for n_bins_key in [ 'n_bins_1D', 'n_bins_2D', 'n_bins_data_1D', 'n_bins_data_2D', 'n_bins_convolve' ]:\n",
    "        n_bins = pm[n_bins_key]\n",
    "        bins = {}\n",
    "        for key, item in lims.items():\n",
    "            if autolims[key]:\n",
    "                low = np.nanmin(np.hstack([ sl_stacked[key], ray_data[key] ]))\n",
    "                high = np.nanmax(np.hstack([ sl_stacked[key], ray_data[key] ]))\n",
    "            else:\n",
    "                low = item[0]\n",
    "                high = item[1]\n",
    "            if logscale[key]:\n",
    "                bins[key] = np.logspace( np.log10( low ), np.log10( high ), n_bins )\n",
    "            else:\n",
    "                bins[key] = np.linspace( low, high, n_bins )\n",
    "\n",
    "            bins[key] *= sl_stacked[key].units\n",
    "        all_bins[n_bins_key] = bins\n",
    "\n",
    "        dx = {}\n",
    "        for key, bins_j in bins.items():\n",
    "            if logscale[key]:\n",
    "                dx[key] = np.log10( bins_j[1] ) - np.log10( bins_j[0] )\n",
    "            else:\n",
    "                dx[key] = float( ( bins_j[1] - bins_j[0] ).value )\n",
    "        all_dx[n_bins_key] = dx\n",
    "\n",
    "        centers = {}\n",
    "        for key, bins_j in bins.items():\n",
    "\n",
    "            if logscale[key]:\n",
    "                bins_j = np.log10( bins_j )\n",
    "\n",
    "            centers[key] = bins_j[:-1] + 0.5 * np.diff( bins_j )\n",
    "\n",
    "            if logscale[key]:\n",
    "                centers[key] = 10.**centers[key]\n",
    "        all_centers[n_bins_key] = centers\n",
    "        \n",
    "    all_binss.append( all_bins )\n",
    "    all_dxs.append( all_dx )\n",
    "    all_centerss.append( all_centers )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dc4f11-67d6-4347-ad74-e225d46523fc",
   "metadata": {},
   "source": [
    "# Comparison Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80991533-89d1-4e92-98b1-a4429a38540a",
   "metadata": {},
   "source": [
    "## Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce6a343-fb23-447b-b09d-15a7ba511857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_correlation( found, actual, logscale=False, one_sided=True, subtract_mean=False, ):\n",
    "    '''Calculate the correlation coefficient.\n",
    "    \n",
    "    Args:\n",
    "        found (array-like):\n",
    "            Modeled data.\n",
    "            \n",
    "        actual (array-like):\n",
    "            Actual data.\n",
    "            \n",
    "        logscale (bool):\n",
    "            If True, do comparison after putting input arrays in logspace.\n",
    "            \n",
    "        one_sided (bool):\n",
    "            If True, normalize by actual.sum()**2, not sqrt( found.sum()**2 * actual.sum()**2 ).\n",
    "            \n",
    "        subtract_mean (bool):\n",
    "            If True, subtract the mean prior to calculating correlations.\n",
    "    '''\n",
    "    \n",
    "    found = copy.copy( found )\n",
    "    actual = copy.copy( actual )\n",
    "    \n",
    "    # Logscale\n",
    "    if logscale:\n",
    "        found = np.log10( found )\n",
    "        actual = np.log10( actual )\n",
    "    \n",
    "    # Identify valid values\n",
    "    found_valid = np.isfinite( found )\n",
    "    actual_valid = np.isfinite( actual )\n",
    "    is_valid = found_valid & actual_valid\n",
    "    \n",
    "    if subtract_mean:\n",
    "        found -= np.mean( found[found_valid] )\n",
    "        actual -= np.mean( actual[actual_valid] )\n",
    "    \n",
    "    # Calculate the correlation\n",
    "    top = found * actual\n",
    "    top_sum = top[is_valid].sum()\n",
    "    \n",
    "    # Calculate the normalization\n",
    "    bottom = actual**2.\n",
    "    bottom_sum = bottom[actual_valid].sum()\n",
    "    if not one_sided:\n",
    "        bottom_found = found**2.\n",
    "        bottom_sum_found = bottom_found[found_valid].sum()\n",
    "        bottom_sum = np.sqrt( bottom_sum * bottom_sum_found )\n",
    "    \n",
    "    return top_sum / bottom_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab713997-6111-4f21-adf3-88dbedcd8fc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data structure for storing correlations\n",
    "correlations_fp = os.path.join( pm['data_dir'], 'correlation.h5' )\n",
    "correlations = verdict.Dict.from_hdf5( correlations_fp, create_nonexistent=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e82f03-9a7d-4518-92c0-c8ec7c395a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sl in enumerate( tqdm.tqdm( sls, bar_format=bar_format ) ):\n",
    "\n",
    "    # Setup correlation matrix\n",
    "    for c_key in correlation_coefficients.keys():\n",
    "        correlation_matrix = np.full( ( len( prop_keys ), len( prop_keys ) ), np.nan )\n",
    "        correlations.setitem( c_key, correlation_matrix, 'matrix', sls[i] )\n",
    "\n",
    "    # Get ray data\n",
    "    ray_data = ray_datas[i]\n",
    "    weights = ray_weights[i]\n",
    "\n",
    "    # Get modeled data\n",
    "    sl_stacked = sl_stackeds[i]\n",
    "    model_weights = model_weightss[i]\n",
    "    stacked_weights = stacked_weightss[i]\n",
    "\n",
    "    # Get bins\n",
    "    bins = all_binss[i]['n_bins_convolve']\n",
    "    dx = all_dxs[i]['n_bins_convolve']\n",
    "    centers = all_centerss[i]['n_bins_convolve']\n",
    "\n",
    "    # Format for calculating distributions\n",
    "    ray_data_histdd = []\n",
    "    sl_stacked_histdd = []\n",
    "    bins_histdd = []\n",
    "    for key in prop_keys:\n",
    "\n",
    "        arr_modeled = sl_stacked[key].value\n",
    "        arr = ray_data[key].value\n",
    "        bins_key = bins[key]\n",
    "\n",
    "        if logscale[key]:\n",
    "            arr_modeled = np.log10( arr_modeled )\n",
    "            arr = np.log10( arr )\n",
    "            bins_key = np.log10( bins_key )\n",
    "\n",
    "        sl_stacked_histdd.append( arr_modeled )\n",
    "        ray_data_histdd.append( arr )\n",
    "        bins_histdd.append( bins_key )\n",
    "\n",
    "    sl_stacked_histdd = np.array( sl_stacked_histdd ).transpose()\n",
    "    ray_data_histdd = np.array( ray_data_histdd ).transpose()\n",
    "    bins_histdd = np.array( bins_histdd )\n",
    "\n",
    "    # Calculate ND distribution for ray data\n",
    "    ray_dist_dd, bins_dd = np.histogramdd(\n",
    "        ray_data_histdd,\n",
    "        bins = bins_histdd,\n",
    "        weights = weights,\n",
    "    )\n",
    "    ray_dist_dd /= ray_dist_dd.sum()\n",
    "\n",
    "    # Calculate ND distribution for modeled data\n",
    "    modeled_dist_dd, bins_dd = np.histogramdd(\n",
    "        sl_stacked_histdd,\n",
    "        bins = bins_histdd,\n",
    "        weights = stacked_weights,\n",
    "    )\n",
    "    modeled_dist_dd /= modeled_dist_dd.sum()\n",
    "\n",
    "    # Calculate net correlation coefficients\n",
    "    for c_key, kwargs in correlation_coefficients.items():\n",
    "        corr_ndim = calc_correlation( modeled_dist_dd, ray_dist_dd, **kwargs )\n",
    "        correlations.setitem( c_key, corr_ndim, 'ndim', sls[i] )\n",
    "\n",
    "    # Calculate correlation coefficients for each property\n",
    "    for j, x_key in enumerate( prop_keys ):\n",
    "        for k, y_key in enumerate( prop_keys ):\n",
    "\n",
    "            # Avoid duplicates\n",
    "            if k < j:\n",
    "                continue\n",
    "\n",
    "            ray_dist_2d, _, _ = np.histogram2d(\n",
    "                ray_data_histdd[:,j],\n",
    "                ray_data_histdd[:,k],\n",
    "                bins = [ bins_histdd[j], bins_histdd[k] ],\n",
    "                weights = weights,\n",
    "            )\n",
    "            ray_dist_2d /= ray_dist_2d.sum()\n",
    "\n",
    "            modeled_dist_2d, _, _ = np.histogram2d(\n",
    "                sl_stacked_histdd[:,j],\n",
    "                sl_stacked_histdd[:,k],\n",
    "                bins = [ bins_histdd[j], bins_histdd[k] ],\n",
    "                weights = stacked_weights,\n",
    "            )\n",
    "            modeled_dist_2d /= modeled_dist_2d.sum()\n",
    "\n",
    "            for c_key, kwargs in correlation_coefficients.items():\n",
    "\n",
    "                correlation_matrix = correlations[c_key]['matrix'][sls[i]]\n",
    "                correlation_matrix[j,k] = calc_correlation( modeled_dist_2d, ray_dist_2d, **kwargs )\n",
    "                correlation_matrix[k,j] = correlation_matrix[j,k]\n",
    "                correlations[c_key]['matrix'][sls[i]] = correlation_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3afa82e-f22f-484d-b9b6-183f00938408",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations.to_hdf5( correlations_fp )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47f224e-887a-4bab-9c57-93685b4aa33d",
   "metadata": {},
   "source": [
    "## Corner Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10b0b44-b83c-425f-98b2-6548ea09373c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Figure\n",
    "n_cols = len( prop_keys )\n",
    "fig = plt.figure( figsize=( panel_length*n_cols, panel_length*n_cols ), facecolor='w' )\n",
    "ax_dict = fig.subplot_mosaic(\n",
    "    mosaic,\n",
    "    gridspec_kw = { 'hspace': 0.1, 'wspace': 0.1 },\n",
    ")\n",
    "\n",
    "# Loop through all properties\n",
    "for j, x_key in enumerate( tqdm.tqdm( prop_keys, bar_format=bar_format ) ):\n",
    "    for k, y_key in enumerate( prop_keys ):\n",
    "\n",
    "        # Avoid duplicates\n",
    "        if k < j:\n",
    "            continue\n",
    "            \n",
    "        # Comparison to ndim\n",
    "        if j == k:\n",
    "            ax = ax_dict[x_key]\n",
    "            subplotspec = ax.get_subplotspec()\n",
    "\n",
    "            x_label = r_labels[x_key]\n",
    "            y_label = r_labels['all']\n",
    "            \n",
    "            for c_key in correlations_plotted:\n",
    "                c_params = correlation_coefficients[c_key]\n",
    "                facecolors = 'k'\n",
    "                if 'logscale' in c_params:\n",
    "                    if c_params['logscale']:\n",
    "                        facecolors = 'none'\n",
    "                \n",
    "                scatter = ax.scatter(\n",
    "                    correlations[c_key]['matrix'].array()[:,j,k],\n",
    "                    correlations[c_key]['ndim'].array(),\n",
    "                    label = c_key,\n",
    "                    edgecolors = 'k',\n",
    "                    facecolors = facecolors,\n",
    "                    marker = correlation_markers[c_key],\n",
    "                    s = correlation_sizes[c_key],\n",
    "                    linewidth = 2,\n",
    "                )\n",
    "            \n",
    "            if x_key in [ 'T', 'nH', 'Z' ]:\n",
    "                ax.yaxis.set_label_position( 'right' )\n",
    "                ax.set_ylabel( y_label, fontsize=16 )\n",
    "\n",
    "            ax.tick_params(\n",
    "                which = 'both',\n",
    "                right = True,\n",
    "                labelright = True,\n",
    "            )\n",
    "          \n",
    "        # 2D comparisons\n",
    "        else:\n",
    "            try:\n",
    "                ax = ax_dict['{}_{}'.format( x_key, y_key )]\n",
    "            except KeyError:\n",
    "                ax = ax_dict['{}_{}'.format( y_key, x_key )]\n",
    "            subplotspec = ax.get_subplotspec()\n",
    "            \n",
    "            x_label = r_labels[x_key]\n",
    "            y_label = r_labels[y_key]\n",
    "            \n",
    "            for c_key in correlations_plotted:\n",
    "                colors = correlations[c_key]['matrix'].array()[:,j,k]\n",
    "                colors = corr_cmap( corr_norm( colors ) )\n",
    "                \n",
    "                # Choose facecolor\n",
    "                facecolors = colors\n",
    "                c_params = correlation_coefficients[c_key]  \n",
    "                if 'logscale' in c_params:\n",
    "                    if c_params['logscale']:\n",
    "                        facecolors = 'None'\n",
    "                        \n",
    "                ax.scatter(\n",
    "                    correlations[c_key]['matrix'].array()[:,j,j],\n",
    "                    correlations[c_key]['matrix'].array()[:,k,k],\n",
    "                    edgecolors = colors,\n",
    "                    facecolors = facecolors,\n",
    "                    marker = correlation_markers[c_key],\n",
    "                    s = correlation_sizes[c_key],\n",
    "#                     norm = ,\n",
    "                    linewidth = 2,\n",
    "                )\n",
    "        \n",
    "        # Guiding lines\n",
    "        for fn in [ ax.axvline, ax.axhline ]:\n",
    "            for value in [ -1, 0, 1 ]:\n",
    "                fn(\n",
    "                    value,\n",
    "                    color = pm['background_linecolor'],\n",
    "                    linewidth = 1,\n",
    "                    zorder = -100,\n",
    "                )\n",
    "        \n",
    "        for fn in [ ax.set_xticks, ax.set_yticks ]:\n",
    "            fn( np.arange( -1, 1.01, 0.25 ) )\n",
    "        ax.tick_params(\n",
    "            which = 'both',\n",
    "            right = True,\n",
    "            labelleft = subplotspec.is_first_col(),\n",
    "            labelbottom = subplotspec.is_last_row(),\n",
    "        )\n",
    "        \n",
    "        ax.set_xlim( -0.3, 1.1 )\n",
    "        ax.set_ylim( -0.3, 1.1 )\n",
    "\n",
    "        if subplotspec.is_last_row():\n",
    "            ax.set_xlabel( x_label, fontsize=16 )\n",
    "        if subplotspec.is_first_col():\n",
    "            ax.set_ylabel( y_label, fontsize=16 )\n",
    "            \n",
    "# Add a legend\n",
    "h, l = ax_dict['vlos'].get_legend_handles_labels()\n",
    "ax_dict['legend'].legend( h, l, loc='lower center', prop={'size': 14}, )\n",
    "ax_dict['legend'].axis( 'off' )\n",
    "\n",
    "# Save\n",
    "savedir = os.path.join( pm['data_dir'], 'figures' )\n",
    "os.makedirs( savedir, exist_ok=True )\n",
    "savefile = 'correlations_corner.png'\n",
    "save_fp = os.path.join( savedir, savefile )\n",
    "print( 'Saving figure to {}'.format( save_fp ) )\n",
    "plt.savefig( save_fp, bbox_inches='tight' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748d6b97-c65a-4919-a82c-31a8c2dcb9d9",
   "metadata": {},
   "source": [
    "## Clean Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e016b1a2-fde5-406b-8aaa-e036eae29bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sls = len( sls )\n",
    "xs = np.linspace( -0.5, 0.5, n_sls ) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cd0677-d7ea-43bd-996a-2f71812a5abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_mosaic = [\n",
    "    [ 'all', 'all', 'all', 'legend' ],\n",
    "    [ 'vlos', 'vlos', 'T', 'T', ],\n",
    "    [ 'nH', 'nH', 'Z', 'Z', ],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50461f2f-f186-452a-8a09-302cc6062d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Figure\n",
    "n_rows_clean = len( clean_mosaic )\n",
    "n_cols_clean = 2\n",
    "fig = plt.figure( figsize=(n_rows_clean*panel_length, n_cols_clean*panel_length), facecolor='w' )\n",
    "ax_dict = fig.subplot_mosaic(\n",
    "    clean_mosaic,\n",
    "    gridspec_kw = { 'wspace': 0.7 },\n",
    ")\n",
    "\n",
    "def r_scatter( ax, ys, c_key ):\n",
    "    c_params = correlation_coefficients[c_key]\n",
    "    facecolors = 'k'\n",
    "    if 'logscale' in c_params:\n",
    "        if c_params['logscale']:\n",
    "            facecolors = 'none'\n",
    "\n",
    "    scatter = ax.scatter(\n",
    "        xs,\n",
    "        ys,\n",
    "        label = c_key,\n",
    "        edgecolors = 'k',\n",
    "        facecolors = facecolors,\n",
    "        marker = correlation_markers[c_key],\n",
    "        s = correlation_sizes[c_key],\n",
    "        linewidth = 2,\n",
    "    )\n",
    "    \n",
    "# Overall\n",
    "for c_key in correlations_plotted:\n",
    "    r_scatter(\n",
    "        ax_dict['all'],\n",
    "        correlations[c_key]['ndim'].array(),\n",
    "        c_key\n",
    "    )\n",
    "\n",
    "# Each property\n",
    "for j, x_key in enumerate( tqdm.tqdm( prop_keys, bar_format=bar_format ) ):\n",
    "    \n",
    "    ax = ax_dict[x_key]\n",
    "    \n",
    "    for c_key in correlations_plotted:\n",
    "        r_scatter(\n",
    "            ax,\n",
    "            correlations[c_key]['matrix'].array()[:,j,j],\n",
    "            c_key\n",
    "        )\n",
    "    \n",
    "        \n",
    "# Add a legend\n",
    "h, l = ax_dict['vlos'].get_legend_handles_labels()\n",
    "ax_dict['legend'].legend( h, l, loc='lower left', prop={'size': 14}, )\n",
    "ax_dict['legend'].axis( 'off' )\n",
    "ax_dict['legend'].annotate(\n",
    "    text = r'$r = \\frac{ \\langle {\\rm actual } \\vert  {\\rm found } \\rangle }{ \\vert {\\rm actual} \\vert \\vert {\\rm found } \\vert }$',\n",
    "    xy = ( 0, 1 ),\n",
    "    xycoords = 'axes fraction',\n",
    "    xytext = ( 5, -5 ),\n",
    "    textcoords = 'offset points',\n",
    "    ha = 'center',\n",
    "    va = 'top',\n",
    "    fontsize = 18,\n",
    ")\n",
    "        \n",
    "# Cleanup\n",
    "for x_key, ax in ax_dict.items():\n",
    "    \n",
    "    if x_key == 'legend':\n",
    "        continue\n",
    "    \n",
    "    subplotspec = ax.get_subplotspec()\n",
    "    \n",
    "    for value in [ -1, 0, 1 ]:\n",
    "        ax.axhline(\n",
    "            value,\n",
    "            color = pm['background_linecolor'],\n",
    "            linewidth = 1,\n",
    "            zorder = -100,\n",
    "        )\n",
    "        \n",
    "    ax.set_ylabel( r_labels[x_key], fontsize=16 )\n",
    "    if subplotspec.is_last_row():\n",
    "        ax.set_xlabel( 'sightline ID', fontsize=16 )\n",
    "        \n",
    "    ax.set_xticks( xs )\n",
    "    xtick_labels = [ _[-2:] for _ in correlations[c_key]['ndim'].keys_array() ]\n",
    "    ax.set_xticklabels( xtick_labels )\n",
    "        \n",
    "    ax.set_ylim( -0.3, 1.1 )\n",
    "    \n",
    "# Save\n",
    "savedir = os.path.join( pm['data_dir'], 'figures' )\n",
    "os.makedirs( savedir, exist_ok=True )\n",
    "savefile = 'correlations.png'\n",
    "save_fp = os.path.join( savedir, savefile )\n",
    "print( 'Saving figure to {}'.format( save_fp ) )\n",
    "plt.savefig( save_fp, bbox_inches='tight' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6266e09-ed13-46af-9a1b-880c90c2cb31",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Ray-by-Ray Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98604e08-5caa-457e-97bf-76969f7e2246",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a838c557-9ed1-4038-b44c-950a45d993a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Other Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a256508-86d9-46fe-baff-b990b6e3c6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContourCalc( object ):\n",
    "    \n",
    "    def __init__( self, arr ):\n",
    "        \n",
    "        is_not_nan = np.invert( np.isnan( arr ) )\n",
    "        is_finite = np.invert( np.isinf( arr ) )\n",
    "        is_valid = is_not_nan & is_finite\n",
    "        self.values_sorted = np.sort( arr[is_valid] )[::-1]\n",
    "        \n",
    "        self.values_fraction = np.cumsum( self.values_sorted )\n",
    "        self.values_fraction /= self.values_fraction[-1]\n",
    "        \n",
    "        self.interp_fn = scipy.interpolate.interp1d( self.values_fraction, self.values_sorted )\n",
    "        \n",
    "    def get_level( self, q, f_min_is_average=True ):\n",
    "        \n",
    "        f = np.array( q ) / 100.\n",
    "        \n",
    "        if f_min_is_average:\n",
    "            f_min = 0.5 * ( self.values_fraction[0] + self.values_fraction[1] )\n",
    "        else:\n",
    "            f_min = self.values_fraction[0]\n",
    "        \n",
    "        if pd.api.types.is_list_like( f ):\n",
    "            f = np.array( f )\n",
    "            f[f<f_min] = f_min\n",
    "        else:\n",
    "            if f < f_min:\n",
    "                f = f_min\n",
    "\n",
    "        return self.interp_fn( f ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7262beef-4dd7-4c3f-9588-871db33d3512",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Corner Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fac918-84e7-4fac-b05f-f65cbbd7e01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, ray in enumerate( rays ):\n",
    "    \n",
    "    print( '\\nMaking comparison for ray {}\\n'.format( i ) )\n",
    "\n",
    "    ray_data = ray_datas[i]\n",
    "    weights = ray_weights[i]\n",
    "\n",
    "    # Modeled sightline\n",
    "    sl_data = sl_datas[i]\n",
    "    sl_used = sl_useds[i]\n",
    "    sl_stacked = sl_stackeds[i]\n",
    "    model_weights = model_weightss[i]\n",
    "    stacked_weights = stacked_weightss[i]\n",
    "    total_weights = total_weightss[i]\n",
    "    median_NHI = median_NHIs[i]\n",
    "    \n",
    "    all_bins = all_binss[i]\n",
    "    all_dx = all_dxs[i]\n",
    "    all_centers = all_centerss[i]\n",
    "    \n",
    "    bins = all_bins['n_bins_1D']\n",
    "    dx = all_dx['n_bins_1D']\n",
    "    centers = all_centers['n_bins_1D']\n",
    "    \n",
    "    # Setup Figurea\n",
    "    n_cols = len( prop_keys )\n",
    "    fig = plt.figure( figsize=( panel_length*n_cols, panel_length*n_cols ), facecolor='w' )\n",
    "    ax_dict = fig.subplot_mosaic( mosaic )\n",
    "\n",
    "    # Loop through all properties\n",
    "    for j, x_key in enumerate( tqdm.tqdm( prop_keys, bar_format=bar_format ) ):\n",
    "        for k, y_key in enumerate( prop_keys ):\n",
    "\n",
    "            # Avoid duplicates\n",
    "            if k < j:\n",
    "                continue\n",
    "            \n",
    "#             # DEBUG\n",
    "#             if j != 1 and k != 0:\n",
    "#                 continue\n",
    "\n",
    "            # Check for out-of-bounds\n",
    "            oob_labels = [ 'modeled', 'ray' ]\n",
    "            for ii, key in enumerate([ x_key, y_key ]):\n",
    "                for jj, values in enumerate([ sl_stacked[key], ray_data[key] ]):\n",
    "                    n_low = ( values < bins[key][0] ).sum()\n",
    "                    n_high = ( values > bins[key][-1] ).sum()\n",
    "                    bounds = [ 'below', 'above' ]\n",
    "                    for kk, n_oob in enumerate([ n_low, n_high ]):\n",
    "                        if n_oob / values.size > 0.02:\n",
    "                            warnings.warn(\n",
    "                                '{} {} points ({:.2g}%) with {} {} {:.3g}'.format(\n",
    "                                    n_oob,\n",
    "                                    oob_labels[jj],\n",
    "                                    n_oob / values.size * 100,\n",
    "                                    key,\n",
    "                                    bounds[kk],\n",
    "                                    lims[key][kk],\n",
    "                                )\n",
    "                            )\n",
    "\n",
    "            # 1D histogram\n",
    "            if j == k:\n",
    "                ax = ax_dict[x_key]\n",
    "                subplotspec = ax.get_subplotspec()\n",
    "\n",
    "                x_label = labels[x_key]\n",
    "                y_label = labels_1D[x_key]\n",
    "                \n",
    "                bins = all_bins['n_bins_1D']\n",
    "                dx = all_dx['n_bins_1D']\n",
    "                centers = all_centers['n_bins_1D']\n",
    "\n",
    "                # Observational\n",
    "                if pm['1D_dist_estimation'] == 'histogram':\n",
    "                    hist_o, edges = np.histogram(\n",
    "                        sl_stacked[x_key],\n",
    "                        bins = bins[x_key],\n",
    "                        weights = stacked_weights,\n",
    "                        density = False,\n",
    "                    )\n",
    "                    hist_o /= hist_o.sum() * dx[x_key]\n",
    "                    ax.step(\n",
    "                        edges[:-1],\n",
    "                        hist_o,\n",
    "                        color = color_modeled,\n",
    "                        where = 'post',\n",
    "                        linewidth = 2,\n",
    "                    )\n",
    "                elif pm['1D_dist_estimation'] == 'kde':\n",
    "                    # Change to logspace for kde\n",
    "                    if logscale[x_key]:\n",
    "                        sl_kde = np.log10( sl_stacked[x_key] )\n",
    "                        kde_centers = np.log10( centers[x_key] )\n",
    "                    else:\n",
    "                        sl_kde = sl_stacked[x_key].value\n",
    "                        kde_centers = centers[x_key].value\n",
    "                    kde_centers, hist_o = kale.density(\n",
    "                            sl_kde,\n",
    "                            points = kde_centers,\n",
    "                            weights = stacked_weights,\n",
    "                            probability = False,\n",
    "                        )\n",
    "                    hist_o /= hist_o.sum() * dx[x_key]\n",
    "                    hist_o *= median_NHI.sum()\n",
    "                    ax.plot(\n",
    "                        centers[x_key],\n",
    "                        hist_o,\n",
    "                        linewidth = 5,\n",
    "                        color = 'k',\n",
    "                        label = 'modeled',\n",
    "                    )\n",
    "                    \n",
    "                    # Individual components\n",
    "                    for kk, sl_used_x in enumerate( sl_used[x_key] ):       \n",
    "                        if logscale[x_key]:\n",
    "                            sl_used_x = np.log10( sl_used_x )\n",
    "                        else:\n",
    "                            sl_used_x = sl_used_x.value\n",
    "                        kde_centers, hist_o = kale.density(\n",
    "                                sl_used_x,\n",
    "                                points = kde_centers,\n",
    "                                weights = model_weights[kk],\n",
    "                                probability = False,\n",
    "                            )\n",
    "                        hist_o /= hist_o.sum() * dx[x_key]\n",
    "                        hist_o *= median_NHI[kk]\n",
    "                        \n",
    "                        # Cycle through colors, skipping the color reserved for the ray data\n",
    "                        if kk == 0:\n",
    "                            cmap_kk = kk\n",
    "                        else:\n",
    "                            cmap_kk = kk + 1\n",
    "                    \n",
    "                        ax.plot(\n",
    "                            centers[x_key],\n",
    "                            hist_o,\n",
    "                            linewidth = 2,\n",
    "                            color = cmap[cmap_kk],\n",
    "                            label = r'    component $\\log N_{\\rm H\\,I}=$' + '{:.3g}'.format( np.log10( median_NHI[kk] ) ),\n",
    "                        )\n",
    "\n",
    "                # Ray\n",
    "                bins = all_bins['n_bins_data_1D']\n",
    "                dx = all_dx['n_bins_data_1D']\n",
    "                centers = all_centers['n_bins_data_1D']\n",
    "                \n",
    "                if pm['1D_dist_estimation_data'] == 'histogram':\n",
    "                    hist_r, edges = np.histogram(\n",
    "                        ray_data[x_key],\n",
    "                        bins = bins[x_key],\n",
    "                        weights = weights,\n",
    "                        density = False,\n",
    "                    )\n",
    "                    hist_r /= hist_r.sum() * dx[x_key]\n",
    "                    hist_r *= ray_data['NHI'].sum()\n",
    "                    ax.fill_between(\n",
    "                        edges[:-1],\n",
    "                        hist_r,\n",
    "                        color = color_data,\n",
    "                        step = 'post',\n",
    "                        label = 'data',\n",
    "                    )\n",
    "                elif pm['1D_dist_estimation_data'] == 'kde':\n",
    "                    # Change to logspace for kde\n",
    "                    if logscale[x_key]:\n",
    "                        sl_kde = np.log10( ray_data[x_key] )\n",
    "                        kde_centers = np.log10( centers[x_key] )\n",
    "                    else:\n",
    "                        sl_kde = ray_data[x_key].value\n",
    "                        kde_centers = centers[x_key].value\n",
    "                    kde_centers, hist_r = kale.density(\n",
    "                            sl_kde,\n",
    "                            points = kde_centers,\n",
    "                            weights = weights,\n",
    "                            probability = False,\n",
    "                        )\n",
    "                    hist_r /= hist_r.sum() * dx[x_key]\n",
    "                    hist_r *= ray_data['NHI'].sum()\n",
    "                    ax.fill_between(\n",
    "                        centers[x_key],\n",
    "                        hist_r,\n",
    "                        color = color_data,\n",
    "                    )\n",
    "\n",
    "#                 y_min = 10.**np.nanmin( [ np.nanmin( np.log10( hist_o[hist_o>0] ) ), np.nanmin( np.log10( hist_r[hist_r>0] ) ) ] )\n",
    "#                 y_min = lims_1D[x_key]\n",
    "#                 y_max = np.nanmax([ np.nanmax( hist_r ), np.nanmax( hist_o ) ])\n",
    "#                 ax.set_ylim( y_min, y_max * 1.05 )\n",
    "                ax.set_ylim( lims_1D[x_key] )\n",
    "                ax.set_xlim( bins[x_key][0], bins[x_key][-1] )\n",
    "\n",
    "                if logscale[x_key]:\n",
    "                    ax.set_xscale( 'log' )\n",
    "                ax.set_yscale( 'log' )\n",
    "                \n",
    "                if x_key in [ 'T', 'nH', 'Z' ]:\n",
    "                    ax.yaxis.set_label_position( 'right' )\n",
    "                    ax.set_ylabel( y_label, fontsize=16 )\n",
    "\n",
    "                ax.tick_params(\n",
    "                    which = 'both',\n",
    "                    right = True,\n",
    "                    labelright = True,\n",
    "                )\n",
    "\n",
    "            # 2D histogram\n",
    "            else:\n",
    "                \n",
    "                bins = all_bins['n_bins_2D']\n",
    "                dx = all_dx['n_bins_2D']\n",
    "                centers = all_centers['n_bins_2D']\n",
    "                \n",
    "                centers_x = copy.copy( centers[x_key] )\n",
    "                centers_y = copy.copy( centers[y_key] )\n",
    "                \n",
    "                try:\n",
    "                    ax = ax_dict['{}_{}'.format( x_key, y_key )]\n",
    "                except KeyError:\n",
    "                    ax = ax_dict['{}_{}'.format( y_key, x_key )]\n",
    "                subplotspec = ax.get_subplotspec()\n",
    "                              \n",
    "                # Upsample centers\n",
    "                upsample = pm['upsample_2D_dist']\n",
    "                if upsample is not None:\n",
    "                    centers_x = scipy.ndimage.zoom( centers_x, upsample )\n",
    "                    centers_y = scipy.ndimage.zoom( centers_y, upsample )\n",
    "                \n",
    "                # Observational per component\n",
    "                img_arr_comps = []\n",
    "                    \n",
    "                for kk, sl_used_x in enumerate( sl_used[x_key] ):\n",
    "                    sl_used_y = sl_used[y_key][kk]\n",
    "                    norm_kk = total_weights[kk] * dx[x_key] * dx[y_key]\n",
    "                    \n",
    "                    # Histogram version\n",
    "                    if pm['2D_dist_estimation'] == 'histogram':\n",
    "                        hist2d_kk, x_edges, y_edges = np.histogram2d(\n",
    "                            sl_used_x,\n",
    "                            sl_used_y,\n",
    "                            bins = [ bins[x_key], bins[y_key], ],\n",
    "                            weights = model_weights[kk] / norm_kk,\n",
    "                        )\n",
    "                        img_arr_kk = np.transpose( hist2d_kk )\n",
    "                        \n",
    "                    # KDE version\n",
    "                    elif pm['2D_dist_estimation'] == 'kde':\n",
    "                        \n",
    "                        # Change to logspace for kde\n",
    "                        if logscale[x_key]:\n",
    "                            sl_used_x = np.log10( sl_used_x )\n",
    "                            kde_centers_x = np.log10( centers_x )\n",
    "                        else:\n",
    "                            kde_centers_x = centers[x_key]\n",
    "                        if logscale[y_key]:\n",
    "                            sl_used_y = np.log10( sl_used_y )\n",
    "                            kde_centers_y = np.log10( centers_y )\n",
    "                        else:\n",
    "                            kde_centers_y = centers[y_key]\n",
    "                            \n",
    "                        kde_data = np.array([ sl_used_x, sl_used_y ])\n",
    "                        points, img_arr_kk = kale.density(\n",
    "                            kde_data,\n",
    "                            points = [ kde_centers_x, kde_centers_y ],\n",
    "                            grid = True,\n",
    "                            weights = model_weights[kk] / norm_kk,\n",
    "                        )\n",
    "                        \n",
    "                    # Upsample and smooth\n",
    "                    if upsample is not None:\n",
    "                        img_arr_kk = scipy.ndimage.zoom( img_arr_kk, upsample )                                                                                                \n",
    "                    \n",
    "                    if pm['smooth_2D_dist'] is not None:\n",
    "                        if upsample is not None:                                                   \n",
    "                            sigma = upsample * pm['smooth_2D_dist']\n",
    "                        else:\n",
    "                            sigma = pm['smooth_2D_dist']\n",
    "                        img_arr_kk = scipy.ndimage.filters.gaussian_filter( img_arr_kk, sigma )\n",
    "        \n",
    "                    # Get levels corresponding to percentages enclose\n",
    "                    c_calc_kk = ContourCalc( img_arr_kk )\n",
    "                    levels = c_calc_kk.get_level( contour_levels )\n",
    "                \n",
    "                    # Cycle through colors, skipping the color reserved for the ray data\n",
    "                    if kk == 0:\n",
    "                        cmap_kk = kk\n",
    "                    else:\n",
    "                        cmap_kk = kk + 1\n",
    "                    contour_colors = [ cmap[cmap_kk], ] * len( levels )\n",
    "                        \n",
    "                    # Prevent invisible low-contribution components\n",
    "                    alpha_min = 0.5\n",
    "                    contour_alpha = ( total_weights[kk] / total_weights.max() ) * ( 1. - alpha_min ) + alpha_min\n",
    "                    \n",
    "                    ax.contour(\n",
    "                        centers_x,\n",
    "                        centers_y,\n",
    "                        img_arr_kk,\n",
    "                        levels,\n",
    "                        colors = contour_colors,\n",
    "                        linewidths = contour_linewidths,\n",
    "                        alpha = contour_alpha\n",
    "                    )\n",
    "                    \n",
    "                # Ray\n",
    "                bins = all_bins['n_bins_data_2D']\n",
    "                dx = all_dx['n_bins_data_2D']\n",
    "                centers = all_centers['n_bins_data_2D']\n",
    "                \n",
    "                used_weights = weights / ( weights.sum() * dx[x_key] * dx[y_key] )\n",
    "                hist2d_r, x_edges, y_edges = np.histogram2d(\n",
    "                    ray_data[x_key],\n",
    "                    ray_data[y_key],\n",
    "                    bins = [ bins[x_key], bins[y_key], ],\n",
    "                    weights = used_weights,\n",
    "                )\n",
    "                img_arr_r = np.transpose( hist2d_r )\n",
    "                \n",
    "                if pm['2D_dist_data_display'] == 'histogram':\n",
    "                    ax.pcolormesh(\n",
    "                        centers[x_key],\n",
    "                        centers[y_key],\n",
    "                        img_arr_r,\n",
    "                        cmap = cmap_data,\n",
    "                        shading = 'nearest',\n",
    "                        norm = matplotlib.colors.LogNorm(),\n",
    "                    )\n",
    "                elif pm['2D_dist_data_display'] == 'contour':\n",
    "                    \n",
    "                    contour_centers_x = copy.copy( all_centers['n_bins_data_2D'][x_key] )\n",
    "                    contour_centers_y = copy.copy( all_centers['n_bins_data_2D'][y_key] )\n",
    "\n",
    "                    # Upsample and smooth\n",
    "                    if upsample is not None:\n",
    "                        img_arr_r = scipy.ndimage.zoom( img_arr_r, upsample )\n",
    "                        \n",
    "                        contour_centers_x = scipy.ndimage.zoom( contour_centers_x, upsample )\n",
    "                        contour_centers_y = scipy.ndimage.zoom( contour_centers_y, upsample )\n",
    "\n",
    "                    if pm['smooth_2D_dist'] is not None:\n",
    "                        if upsample is not None:                                                   \n",
    "                            sigma = upsample * pm['smooth_2D_dist']\n",
    "                        else:\n",
    "                            sigma = pm['smooth_2D_dist']\n",
    "                        img_arr_r = scipy.ndimage.filters.gaussian_filter( img_arr_r, sigma )\n",
    "\n",
    "                    # Get levels corresponding to percentages enclose\n",
    "                    c_calc_kk = ContourCalc( img_arr_r )\n",
    "                    levels = c_calc_kk.get_level( contour_levels + [ -1, ], False )\n",
    "                    \n",
    "                    ax.contourf(\n",
    "                        contour_centers_x,\n",
    "                        contour_centers_y,\n",
    "                        img_arr_r,\n",
    "                        levels,\n",
    "                        cmap = cmap_data,\n",
    "                        zorder = -100,\n",
    "                    )\n",
    "\n",
    "                ax.set_xlim( bins[x_key][0], bins[x_key][-1] )\n",
    "                ax.set_ylim( bins[y_key][0], bins[y_key][-1] )\n",
    "\n",
    "                if logscale[x_key]:\n",
    "                    ax.set_xscale( 'log' )\n",
    "                if logscale[y_key]:\n",
    "                    ax.set_yscale( 'log' )\n",
    "\n",
    "                x_label = labels[x_key]\n",
    "                y_label = labels[y_key]\n",
    "\n",
    "            if subplotspec.is_last_row():\n",
    "                ax.set_xlabel( x_label, fontsize=16 )\n",
    "            if subplotspec.is_first_col():\n",
    "                ax.set_ylabel( y_label, fontsize=16 )\n",
    "                \n",
    "            # Correlation coefficient annotation\n",
    "            coeff = correlations['log']['matrix'][sls[i]][j,k]\n",
    "            text = r'$r =$' + '{:.2f}'.format( coeff ) \n",
    "            annot = ax.annotate(\n",
    "                text = text,\n",
    "                xy = ( 1, 1 ),\n",
    "                xycoords = 'axes fraction',\n",
    "                xytext = ( -5, -5 ),\n",
    "                textcoords = 'offset points',\n",
    "                va = 'top',\n",
    "                ha = 'right',\n",
    "                fontsize = 14,\n",
    "                color = 'w',\n",
    "            )\n",
    "            coeff_color = corr_cmap( corr_norm( coeff ) )\n",
    "            annot.set_path_effects([\n",
    "                path_effects.Stroke( linewidth=3, foreground=coeff_color ),\n",
    "                path_effects.Normal()\n",
    "            ])\n",
    "                \n",
    "    # Add a legend\n",
    "    h, l = ax_dict['vlos'].get_legend_handles_labels()\n",
    "    ax_dict['legend'].legend( h, l, loc='lower left', prop={'size': 12}, )\n",
    "    ax_dict['legend'].axis( 'off' )\n",
    "    \n",
    "    # Ray number\n",
    "    annot = ax_dict['legend'].annotate(\n",
    "        text = 'sightline {}'.format( sls[i][-2:] ),\n",
    "        xy = ( 0.5, 1 ),\n",
    "        xycoords = 'axes fraction',\n",
    "        xytext = ( 5, -5 ),\n",
    "        textcoords = 'offset points',\n",
    "        va = 'top',\n",
    "        ha = 'center',\n",
    "        fontsize = 16,\n",
    "        color = 'k',\n",
    "    )\n",
    "\n",
    "    # Save\n",
    "    savedir = os.path.join( pm['data_dir'], 'figures' )\n",
    "    os.makedirs( savedir, exist_ok=True )\n",
    "    savefile = 'sightline_{}.png'.format( os.path.basename( sl_fps[i] ) )\n",
    "    save_fp = os.path.join( savedir, savefile )\n",
    "    print( 'Saving figure to {}'.format( save_fp ) )\n",
    "    plt.savefig( save_fp, bbox_inches='tight' )\n",
    "    \n",
    "    if not pm['show_plots_in_nb']:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90f59d55-1217-4980-8fcb-7f1af1266a0b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Properties at a Given $v_{LOS}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e084b3e-fdc0-4782-ad6e-e7eb8a3ae23a",
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_levels = [ 50, ]\n",
    "contour_linewidth = [ 2, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b244891e-283f-4a49-b570-fe7499504563",
   "metadata": {},
   "outputs": [],
   "source": [
    "vel_prop_keys = pm['vel_prop_keys']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f61ffbb5-f664-43e5-926a-ddc024e2a146",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray_hists_1d = {}\n",
    "ray_hists_2d = {}\n",
    "for i, ray in enumerate( rays ):\n",
    "    \n",
    "    print( '\\nMaking comparison for ray {}\\n'.format( i ) )\n",
    "\n",
    "    ray_data = ray_datas[i]\n",
    "    weights = ray_weights[i]\n",
    "\n",
    "    # Modeled sightline\n",
    "    sl_data = sl_datas[i]\n",
    "    sl_used = sl_useds[i]\n",
    "    compkeys = sl_compkeys[i]\n",
    "    sl_stacked = sl_stackeds[i]\n",
    "    model_weights = model_weightss[i]\n",
    "    stacked_weights = stacked_weightss[i]\n",
    "    total_weights = total_weightss[i]\n",
    "    median_NHI = median_NHIs[i]\n",
    "    \n",
    "    all_bins = all_binss[i]\n",
    "    all_dx = all_dxs[i]\n",
    "    all_centers = all_centerss[i]\n",
    "    \n",
    "    bins = all_bins['n_bins_1D']\n",
    "    dx = all_dx['n_bins_1D']\n",
    "    centers = all_centers['n_bins_1D']\n",
    "    \n",
    "    # Setup Figurea\n",
    "    n_cols = len( vel_prop_keys )\n",
    "    fig = plt.figure( figsize=( panel_length*n_cols/2., panel_length*n_cols/2. ), facecolor='w' )\n",
    "    ax_dict = fig.subplot_mosaic(\n",
    "        velocity_mosaic,\n",
    "        gridspec_kw = { 'wspace': 0.32 },\n",
    "    )\n",
    "\n",
    "    # Loop through all properties\n",
    "    for j, x_key in enumerate( tqdm.tqdm( vel_prop_keys, bar_format=bar_format ) ):\n",
    "        for k, y_key in enumerate( vel_prop_keys ):\n",
    "\n",
    "            # Avoid duplicates\n",
    "            if k < j:\n",
    "                continue\n",
    "            \n",
    "#             # DEBUG\n",
    "#             if j != 1 and k != 0:\n",
    "#                 continue\n",
    "\n",
    "            # Check for out-of-bounds\n",
    "            oob_labels = [ 'modeled', 'ray' ]\n",
    "            for ii, key in enumerate([ x_key, y_key ]):\n",
    "                for jj, values in enumerate([ sl_stacked[key], ray_data[key] ]):\n",
    "                    n_low = ( values < bins[key][0] ).sum()\n",
    "                    n_high = ( values > bins[key][-1] ).sum()\n",
    "                    bounds = [ 'below', 'above' ]\n",
    "                    for kk, n_oob in enumerate([ n_low, n_high ]):\n",
    "                        if n_oob / values.size > 0.02:\n",
    "                            warnings.warn(\n",
    "                                '{} {} points ({:.2g}%) with {} {} {:.3g}'.format(\n",
    "                                    n_oob,\n",
    "                                    oob_labels[jj],\n",
    "                                    n_oob / values.size * 100,\n",
    "                                    key,\n",
    "                                    bounds[kk],\n",
    "                                    lims[key][kk],\n",
    "                                )\n",
    "                            )\n",
    "\n",
    "            # 1D histogram\n",
    "            if j == k:\n",
    "\n",
    "                x_label = labels[x_key]\n",
    "                y_label = labels_1D[x_key]\n",
    "                \n",
    "                bins = all_bins['n_bins_1D']\n",
    "                dx = all_dx['n_bins_1D']\n",
    "                centers = all_centers['n_bins_1D']\n",
    "                \n",
    "                if x_key not in ax_dict:\n",
    "                    continue\n",
    "                    \n",
    "                ax = ax_dict[x_key]\n",
    "                subplotspec = ax.get_subplotspec()\n",
    "\n",
    "                # Observational\n",
    "                if pm['1D_dist_estimation'] == 'histogram':\n",
    "                    hist_o, edges = np.histogram(\n",
    "                        sl_stacked[x_key],\n",
    "                        bins = bins[x_key],\n",
    "                        weights = stacked_weights,\n",
    "                        density = False,\n",
    "                    )\n",
    "                    hist_o /= hist_o.sum() * dx[x_key]\n",
    "                    ax.step(\n",
    "                        edges[:-1],\n",
    "                        hist_o,\n",
    "                        color = color_modeled,\n",
    "                        where = 'post',\n",
    "                        linewidth = 2,\n",
    "                    )\n",
    "                elif pm['1D_dist_estimation'] == 'kde':\n",
    "                    # Change to logspace for kde\n",
    "                    if logscale[x_key]:\n",
    "                        sl_kde = np.log10( sl_stacked[x_key] )\n",
    "                        kde_centers = np.log10( centers[x_key] )\n",
    "                    else:\n",
    "                        sl_kde = sl_stacked[x_key].value\n",
    "                        kde_centers = centers[x_key].value\n",
    "                    kde_centers, hist_o = kale.density(\n",
    "                            sl_kde,\n",
    "                            points = kde_centers,\n",
    "                            weights = stacked_weights,\n",
    "                            probability = False,\n",
    "                        )\n",
    "                    hist_o /= hist_o.sum() * dx[x_key]\n",
    "                    hist_o *= median_NHI.sum()\n",
    "                    ax.plot(\n",
    "                        centers[x_key],\n",
    "                        hist_o,\n",
    "                        linewidth = 5,\n",
    "                        color = 'k',\n",
    "                        label = 'modeled',\n",
    "                    )\n",
    "                    \n",
    "                    # Individual components\n",
    "                    for kk, sl_used_x in enumerate( sl_used[x_key] ):       \n",
    "                        if logscale[x_key]:\n",
    "                            sl_used_x = np.log10( sl_used_x )\n",
    "                        else:\n",
    "                            sl_used_x = sl_used_x.value\n",
    "                        kde_centers, hist_o = kale.density(\n",
    "                                sl_used_x,\n",
    "                                points = kde_centers,\n",
    "                                weights = model_weights[kk],\n",
    "                                probability = False,\n",
    "                            )\n",
    "                        hist_o /= hist_o.sum() * dx[x_key]\n",
    "                        hist_o *= median_NHI[kk]\n",
    "                        \n",
    "                        # Cycle through colors, skipping the color reserved for the ray data\n",
    "                        if kk == 0:\n",
    "                            cmap_kk = kk\n",
    "                        else:\n",
    "                            cmap_kk = kk + 1\n",
    "                    \n",
    "                        ax.plot(\n",
    "                            centers[x_key],\n",
    "                            hist_o,\n",
    "                            linewidth = 2.5,\n",
    "                            color = cmap[cmap_kk],\n",
    "                            label = r'    component $\\log N_{\\rm H\\,I}=$' + '{:.3g}'.format( np.log10( median_NHI[kk] ) ),\n",
    "                        )\n",
    "\n",
    "                # Ray\n",
    "                bins = all_bins['n_bins_data_1D']\n",
    "                dx = all_dx['n_bins_data_1D']\n",
    "                centers = all_centers['n_bins_data_1D']\n",
    "                \n",
    "                if pm['1D_dist_estimation_data'] == 'histogram':\n",
    "                    hist_r, edges = np.histogram(\n",
    "                        ray_data[x_key],\n",
    "                        bins = bins[x_key],\n",
    "                        weights = weights,\n",
    "                        density = False,\n",
    "                    )\n",
    "                    hist_r /= hist_r.sum() * dx[x_key]\n",
    "                    hist_r *= ray_data['NHI'].sum()\n",
    "                    ax.fill_between(\n",
    "                        edges[:-1],\n",
    "                        hist_r,\n",
    "                        color = color_data,\n",
    "                        step = 'post',\n",
    "                        label = 'data',\n",
    "                    )\n",
    "                elif pm['1D_dist_estimation_data'] == 'kde':\n",
    "                    # Change to logspace for kde\n",
    "                    if logscale[x_key]:\n",
    "                        sl_kde = np.log10( ray_data[x_key] )\n",
    "                        kde_centers = np.log10( centers[x_key] )\n",
    "                    else:\n",
    "                        sl_kde = ray_data[x_key].value\n",
    "                        kde_centers = centers[x_key].value\n",
    "                    kde_centers, hist_r = kale.density(\n",
    "                            sl_kde,\n",
    "                            points = kde_centers,\n",
    "                            weights = weights,\n",
    "                            probability = False,\n",
    "                        )\n",
    "                    hist_r /= hist_r.sum() * dx[x_key]\n",
    "                    hist_r *= ray_data['NHI'].sum()\n",
    "                    ax.fill_between(\n",
    "                        centers[x_key],\n",
    "                        hist_r,\n",
    "                        color = color_data,\n",
    "                    )\n",
    "                    \n",
    "                ray_hists_1d[x_key] = hist_r\n",
    "\n",
    "#                 y_min = 10.**np.nanmin( [ np.nanmin( np.log10( hist_o[hist_o>0] ) ), np.nanmin( np.log10( hist_r[hist_r>0] ) ) ] )\n",
    "#                 y_min = lims_1D[x_key]\n",
    "#                 y_max = np.nanmax([ np.nanmax( hist_r ), np.nanmax( hist_o ) ])\n",
    "#                 ax.set_ylim( y_min, y_max * 1.05 )\n",
    "                ax.set_ylim( lims_1D[x_key] )\n",
    "                ax.set_xlim( bins[x_key][0], bins[x_key][-1] )\n",
    "\n",
    "                if logscale[x_key]:\n",
    "                    ax.set_xscale( 'log' )\n",
    "                ax.set_yscale( 'log' )\n",
    "                \n",
    "                if x_key in [ 'T', 'nH', 'Z' ]:\n",
    "                    ax.yaxis.set_label_position( 'right' )\n",
    "                    ax.set_ylabel( y_label, fontsize=16 )\n",
    "\n",
    "            # 2D histogram\n",
    "            else:\n",
    "                \n",
    "                bins = all_bins['n_bins_2D']\n",
    "                dx = all_dx['n_bins_2D']\n",
    "                centers = all_centers['n_bins_2D']\n",
    "                \n",
    "                centers_x = copy.copy( centers[x_key] )\n",
    "                centers_y = copy.copy( centers[y_key] )\n",
    "                \n",
    "                try:\n",
    "                    ax = ax_dict['{}_{}'.format( x_key, y_key )]\n",
    "                except KeyError:\n",
    "                    try:\n",
    "                        ax = ax_dict['{}_{}'.format( y_key, x_key )]\n",
    "                    except KeyError:\n",
    "                        continue\n",
    "                subplotspec = ax.get_subplotspec()\n",
    "                              \n",
    "                # Upsample centers\n",
    "                upsample = pm['upsample_2D_dist']\n",
    "                if upsample is not None:\n",
    "                    centers_x = scipy.ndimage.zoom( centers_x, upsample )\n",
    "                    centers_y = scipy.ndimage.zoom( centers_y, upsample )\n",
    "                \n",
    "                # Observational per component\n",
    "                img_arr_comps = []\n",
    "                    \n",
    "                for kk, sl_used_x in enumerate( sl_used[x_key] ):\n",
    "                    sl_used_y = sl_used[y_key][kk]\n",
    "                    norm_kk = total_weights[kk] * dx[x_key] * dx[y_key]\n",
    "                    \n",
    "                    # Histogram version\n",
    "                    if pm['2D_dist_estimation'] == 'histogram':\n",
    "                        hist2d_kk, x_edges, y_edges = np.histogram2d(\n",
    "                            sl_used_x,\n",
    "                            sl_used_y,\n",
    "                            bins = [ bins[x_key], bins[y_key], ],\n",
    "                            weights = model_weights[kk] / norm_kk,\n",
    "                        )\n",
    "                        img_arr_kk = np.transpose( hist2d_kk )\n",
    "                        \n",
    "                    # KDE version\n",
    "                    elif pm['2D_dist_estimation'] == 'kde':\n",
    "                        \n",
    "                        # Change to logspace for kde\n",
    "                        if logscale[x_key]:\n",
    "                            sl_used_x = np.log10( sl_used_x )\n",
    "                            kde_centers_x = np.log10( centers_x )\n",
    "                        else:\n",
    "                            kde_centers_x = centers[x_key]\n",
    "                        if logscale[y_key]:\n",
    "                            sl_used_y = np.log10( sl_used_y )\n",
    "                            kde_centers_y = np.log10( centers_y )\n",
    "                        else:\n",
    "                            kde_centers_y = centers[y_key]\n",
    "                            \n",
    "                        kde_data = np.array([ sl_used_x, sl_used_y ])\n",
    "                        points, img_arr_kk = kale.density(\n",
    "                            kde_data,\n",
    "                            points = [ kde_centers_x, kde_centers_y ],\n",
    "                            grid = True,\n",
    "                            weights = model_weights[kk] / norm_kk,\n",
    "                        )\n",
    "                        \n",
    "                    # Upsample and smooth\n",
    "                    if upsample is not None:\n",
    "                        img_arr_kk = scipy.ndimage.zoom( img_arr_kk, upsample )                                                                                                \n",
    "                    \n",
    "                    if pm['smooth_2D_dist'] is not None:\n",
    "                        if upsample is not None:                                                   \n",
    "                            sigma = upsample * pm['smooth_2D_dist']\n",
    "                        else:\n",
    "                            sigma = pm['smooth_2D_dist']\n",
    "                        img_arr_kk = scipy.ndimage.filters.gaussian_filter( img_arr_kk, sigma )\n",
    "        \n",
    "                    # Get levels corresponding to percentages enclose\n",
    "                    c_calc_kk = ContourCalc( img_arr_kk )\n",
    "                    levels = c_calc_kk.get_level( contour_levels )\n",
    "                \n",
    "                    # Cycle through colors, skipping the color reserved for the ray data\n",
    "                    if kk == 0:\n",
    "                        cmap_kk = kk\n",
    "                    else:\n",
    "                        cmap_kk = kk + 1\n",
    "                    contour_colors = [ cmap[cmap_kk], ] * len( levels )\n",
    "                        \n",
    "                    # Prevent invisible low-contribution components\n",
    "                    alpha_min = 0.1\n",
    "                    alpha_weight = matplotlib.colors.LogNorm( total_weights.min(), total_weights.max() )( total_weights[kk] )\n",
    "                    contour_alpha = alpha_weight * ( 1. - alpha_min ) + alpha_min\n",
    "                    \n",
    "                    ax.contour(\n",
    "                        centers_x,\n",
    "                        centers_y,\n",
    "                        img_arr_kk,\n",
    "                        levels,\n",
    "                        colors = contour_colors,\n",
    "                        linewidths = contour_linewidths,\n",
    "                        alpha = contour_alpha\n",
    "                    )\n",
    "                    \n",
    "                    # Best estimate\n",
    "                    ax.scatter(\n",
    "                        mle_data[x_key][compkeys[kk]],\n",
    "                        mle_data[y_key][compkeys[kk]],\n",
    "                        color = cmap[cmap_kk],\n",
    "                        s = 50,\n",
    "                        zorder = 100,\n",
    "                        alpha = contour_alpha,\n",
    "                        edgecolor = 'none',\n",
    "                    )\n",
    "                    \n",
    "                # Ray\n",
    "                bins = all_bins['n_bins_data_2D']\n",
    "                dx = all_dx['n_bins_data_2D']\n",
    "                centers = all_centers['n_bins_data_2D']\n",
    "                \n",
    "                used_weights = weights / ( weights.sum() * dx[x_key] * dx[y_key] )\n",
    "                hist2d_r, x_edges, y_edges = np.histogram2d(\n",
    "                    ray_data[x_key],\n",
    "                    ray_data[y_key],\n",
    "                    bins = [ bins[x_key], bins[y_key], ],\n",
    "                    weights = used_weights,\n",
    "                )\n",
    "                img_arr_r = np.transpose( hist2d_r )\n",
    "                ray_hists_2d['{}_{}'.format( y_key, x_key )] = hist2d_r\n",
    "                \n",
    "                if pm['2D_dist_data_display'] == 'histogram':\n",
    "                    ax.pcolormesh(\n",
    "                        centers[x_key],\n",
    "                        centers[y_key],\n",
    "                        img_arr_r,\n",
    "                        cmap = cmap_data,\n",
    "                        shading = 'nearest',\n",
    "                        norm = matplotlib.colors.LogNorm(),\n",
    "                    )\n",
    "                elif pm['2D_dist_data_display'] == 'contour':\n",
    "                    \n",
    "                    contour_centers_x = copy.copy( all_centers['n_bins_data_2D'][x_key] )\n",
    "                    contour_centers_y = copy.copy( all_centers['n_bins_data_2D'][y_key] )\n",
    "\n",
    "                    # Upsample and smooth\n",
    "                    if upsample is not None:\n",
    "                        img_arr_r = scipy.ndimage.zoom( img_arr_r, upsample )\n",
    "                        \n",
    "                        contour_centers_x = scipy.ndimage.zoom( contour_centers_x, upsample )\n",
    "                        contour_centers_y = scipy.ndimage.zoom( contour_centers_y, upsample )\n",
    "\n",
    "                    if pm['smooth_2D_dist'] is not None:\n",
    "                        if upsample is not None:                                                   \n",
    "                            sigma = upsample * pm['smooth_2D_dist']\n",
    "                        else:\n",
    "                            sigma = pm['smooth_2D_dist']\n",
    "                        img_arr_r = scipy.ndimage.filters.gaussian_filter( img_arr_r, sigma )\n",
    "\n",
    "                    # Get levels corresponding to percentages enclose\n",
    "                    c_calc_kk = ContourCalc( img_arr_r )\n",
    "                    levels = c_calc_kk.get_level( contour_levels + [ -1, ], False )\n",
    "                    \n",
    "                    ax.contourf(\n",
    "                        contour_centers_x,\n",
    "                        contour_centers_y,\n",
    "                        img_arr_r,\n",
    "                        levels,\n",
    "                        cmap = cmap_data,\n",
    "                        zorder = -100,\n",
    "                    )\n",
    "\n",
    "                ax.set_xlim( bins[x_key][0], bins[x_key][-1] )\n",
    "                ax.set_ylim( bins[y_key][0], bins[y_key][-1] )\n",
    "\n",
    "                if logscale[x_key]:\n",
    "                    ax.set_xscale( 'log' )\n",
    "                if logscale[y_key]:\n",
    "                    ax.set_yscale( 'log' )\n",
    "\n",
    "                x_label = labels[x_key]\n",
    "                y_label = labels[y_key]\n",
    "\n",
    "            ax.set_xlabel( x_label, fontsize=16 )\n",
    "            ax.set_ylabel( y_label, fontsize=16 )\n",
    "            \n",
    "            ax.tick_params( labelsize=14, )\n",
    "            ax.tick_params( which='major', length=7, width=1.5 )\n",
    "            ax.tick_params( which='minor', length=4, width=1 )\n",
    "                \n",
    "#     # Add a legend\n",
    "#     h, l = ax_dict['T_vlos'].get_legend_handles_labels()\n",
    "#     ax_dict['legend'].legend( h, l, loc='lower left', prop={'size': 12}, )\n",
    "#     ax_dict['legend'].axis( 'off' )\n",
    "    \n",
    "#     # Ray number\n",
    "#     annot = ax_dict['legend'].annotate(\n",
    "#         text = 'sightline {}'.format( sls[i][-2:] ),\n",
    "#         xy = ( 0.5, 1 ),\n",
    "#         xycoords = 'axes fraction',\n",
    "#         xytext = ( 5, -5 ),\n",
    "#         textcoords = 'offset points',\n",
    "#         va = 'top',\n",
    "#         ha = 'center',\n",
    "#         fontsize = 16,\n",
    "#         color = 'k',\n",
    "#     )\n",
    "\n",
    "    # Save\n",
    "    savedir = os.path.join( pm['data_dir'], 'figures' )\n",
    "    os.makedirs( savedir, exist_ok=True )\n",
    "    savefile = 'losprops_{}.png'.format( os.path.basename( sl_fps[i] ) )\n",
    "    save_fp = os.path.join( savedir, savefile )\n",
    "    print( 'Saving figure to {}'.format( save_fp ) )\n",
    "    plt.savefig( save_fp, bbox_inches='tight' )\n",
    "    \n",
    "    if not pm['show_plots_in_nb']:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11758693-c828-46d5-9e2f-22a8fbc4757e",
   "metadata": {},
   "source": [
    "### Save Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a87775b-294e-4bcf-9fb1-6a35ad21d6e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_proposal = verdict.Dict({})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd5364a8-96ce-4668-8a4b-354d7db83cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_proposal['bins'] = bins\n",
    "data_for_proposal['centers'] = centers\n",
    "data_for_proposal['raw data'] = ray_data\n",
    "data_for_proposal['histograms 2D'] = ray_hists_2d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b67d0d1-e27f-41fb-8c82-476aca689bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_proposal['histograms 1D'] = {}\n",
    "for x_key in vel_prop_keys:\n",
    "    hist_r, edges = np.histogram(\n",
    "        ray_data[x_key],\n",
    "        bins = bins[x_key],\n",
    "        weights = weights,\n",
    "        density = False,\n",
    "    )\n",
    "    hist_r /= hist_r.sum() * dx[x_key]\n",
    "    hist_r *= ray_data['NHI'].sum()\n",
    "    data_for_proposal['histograms 1D'][x_key] = hist_r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b1bf28d-4564-429a-9168-1f00c8c9db4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_for_proposal.to_hdf5( '/Users/zhafen/Downloads/proposal_data_for_sameer.h5' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301c13b5-e08c-4795-bf73-aa8955cb0a77",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
