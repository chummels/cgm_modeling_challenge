{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c38063-dde6-421f-a5b0-8a1d3320f44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import tqdm\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b816548-85d0-44e9-abf7-9be458930aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt\n",
    "import trident\n",
    "import unyt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eadfbae-2549-4873-8e31-47c5ad3048a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.style.use( '/Users/zhafen/repos/clean-bold/clean-bold.mplstyle' )\n",
    "import palettable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67801afa-d127-43dc-a211-164c72b318b0",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ede9ac4-52f5-4e59-9eeb-b720128fc41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm = {\n",
    "    'broaden_models': True,\n",
    "    'weighting': 'density',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3da5af-8e34-4c9d-bb9d-c3e46a4743fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "redshift = 0.13"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3584033f-645e-4873-a1f9-fdf379b651b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = './data/modeling_results/sameer_charlton/sample2'\n",
    "ray_dir = './data/synthetic_data/sample2/rays'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b859bafe-757b-4df3-a5bc-3a392cf2596b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting parameters\n",
    "cmap = palettable.cartocolors.qualitative.Safe_10.mpl_colors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1600ccba-a75e-460b-ac46-21db0888e377",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91da438d-84f0-4942-b249-556506474395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sightline filepaths\n",
    "sl_fps = []\n",
    "sls = []\n",
    "for sl_fp in glob.glob( os.path.join( data_dir, '*' ) ):\n",
    "    sl_fps.append( sl_fp )\n",
    "    sls.append( os.path.split( sl_fp )[-1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa36c63-10d6-40ac-a680-02a24a6f637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rays\n",
    "ray_fps = [ os.path.join( ray_dir, 'ray_{}.h5'.format( _[1:] ) ) for _ in sls ]\n",
    "rays = [ yt.load( _ ) for _ in ray_fps ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6266e09-ed13-46af-9a1b-880c90c2cb31",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98604e08-5caa-457e-97bf-76969f7e2246",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1df7212-1baf-4a65-9dac-72ee8ac7d760",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccb4e562-6ac5-4e53-a9c3-e8f02d6ca09a",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample_turb = 1000\n",
    "n_bins = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fdbc53-bfbe-4433-968f-56b5e859a4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mosaic = [\n",
    "    [ 'vlos', '.', '.', '.' ],\n",
    "    [ 'T_vlos', 'T', '.', '.' ],\n",
    "    [ 'nH_vlos', 'nH_T', 'nH', '.' ],\n",
    "    [ 'Z_vlos', 'Z_T', 'Z_nH', 'Z', ],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d027be-f4d2-4b9e-b5ce-d1c4907b7d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\n",
    "    'vlos': r'$v_{\\rm LOS}$ [km/s]',\n",
    "    'T': r'T [K]',\n",
    "    'nH': r'$n_{\\rm H}$ [cm$^{-3}$]',\n",
    "    'Z': r'$Z$ [$Z_{\\odot}$]',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f6567e2-e36f-479e-a423-65bbb76dfad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "lims = {\n",
    "    'vlos': [ -100, 100 ],\n",
    "    'T': [ 7e3, 2.5e6 ],\n",
    "    'nH': [ 1e-4, 100 ],\n",
    "    'Z': [ 1e-3, 1.5 ],\n",
    "}\n",
    "autolims = {\n",
    "    'vlos': True,\n",
    "    'T': True,\n",
    "    'nH': True,\n",
    "    'Z': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b05110c-b203-4da8-b685-3379af4c4079",
   "metadata": {},
   "outputs": [],
   "source": [
    "dvs = {\n",
    "    'vlos': 5.,\n",
    "    'T': 0.05,\n",
    "    'nH': 0.05,\n",
    "    'Z': 0.05,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dfc8ec9-9beb-4443-aa97-a1642ef9f197",
   "metadata": {},
   "outputs": [],
   "source": [
    "logscale = {\n",
    "    'vlos': False,\n",
    "    'T': True,\n",
    "    'nH': True,\n",
    "    'Z': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fd8049-150e-4b53-a309-1d4ad2c43029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_color_linear_cmap( color, name, f_white=0.95, f_saturated=1.0, ):\n",
    "    '''A function that turns a single color into linear colormap that\n",
    "    goes from a color that is whiter than the original color to a color\n",
    "    that is more saturated than the original color.\n",
    "    '''\n",
    "    \n",
    "    color_hsv = matplotlib.colors.rgb_to_hsv( color )\n",
    "    start_color_hsv = copy.copy( color_hsv )\n",
    "    \n",
    "    start_color_hsv = copy.copy( color_hsv )\n",
    "    start_color_hsv[1] -= f_white * start_color_hsv[1]\n",
    "    start_color_hsv[2] += f_white * ( 1. - start_color_hsv[2] )\n",
    "    start_color = matplotlib.colors.hsv_to_rgb( start_color_hsv )\n",
    "    \n",
    "    end_color_hsv = copy.copy( color_hsv )\n",
    "    end_color_hsv[1] += f_saturated * ( 1. - end_color_hsv[1] )\n",
    "    end_color = matplotlib.colors.hsv_to_rgb( end_color_hsv )\n",
    "    \n",
    "    return matplotlib.colors.LinearSegmentedColormap.from_list( name, [ start_color, end_color ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761f1f2e-0fa1-4586-9814-bc04435fb229",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_modeled = cmap[0]\n",
    "color_data = cmap[1]\n",
    "cmap_modeled = one_color_linear_cmap( color_modeled, 'modeled' )\n",
    "cmap_data = one_color_linear_cmap( color_data, 'data' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ba3ffa-8b81-4556-aaec-30986c4fa34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "panel_length = 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a838c557-9ed1-4038-b44c-950a45d993a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Other Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6449d265-d661-49d0-b9b7-912180497035",
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_keys = list( labels.keys() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4eecdd16-27c4-4962-a825-e8fd245b6c48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeff_to_vel( zeff ):\n",
    "    \n",
    "    ainv2 = ( ( 1. + zeff ) / ( 1. + redshift ) )**2.\n",
    "    \n",
    "    v_div_c = ( ainv2 - 1. ) / ( ainv2 + 1. )\n",
    "    return ( v_div_c * unyt.c ).to( 'km/s' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7262beef-4dd7-4c3f-9588-871db33d3512",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fac918-84e7-4fac-b05f-f65cbbd7e01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, ray in enumerate( rays ):\n",
    "    \n",
    "    print( '\\nMaking comparison for ray {}\\n'.format( i ) )\n",
    "\n",
    "    # Ray properties\n",
    "    den = ray.r[('gas', 'number_density')] * 0.75\n",
    "    ray_data = {\n",
    "        'nH': den,\n",
    "        'NH': ( den * ray.r[('gas', 'dl')] ),\n",
    "        'z': ray.r[('gas', 'redshift_eff')],\n",
    "        'T': ray.r[('gas', 'temperature')],\n",
    "        'Z': ray.r[('gas', 'metallicity')],\n",
    "    }\n",
    "    ray_data['vlos'] = zeff_to_vel( ray_data['z'] )\n",
    "\n",
    "    # Modeled sightline\n",
    "    sl = sls[i]\n",
    "    sl_fp = sl_fps[i]\n",
    "\n",
    "    # Get text files\n",
    "    sl_data = {}\n",
    "    col_names = [ 'prob', 'likelihood', 'Z', 'nH', 'T', 'NHI', 'bturb', 'z', ]\n",
    "    col_units = [ 1., 1., unyt.Zsun, unyt.cm**-3, unyt.K, unyt.cm**-2, unyt.km / unyt.s, 1. ]\n",
    "    for component_fp in glob.glob( os.path.join( sl_fp, '*' ) ):\n",
    "        component_key = os.path.splitext( os.path.split( component_fp )[-1] )[0]\n",
    "        sl_data[component_key] = pd.read_csv( component_fp, sep=' ', names=col_names )\n",
    "\n",
    "    # Add LOS velocity and reformat\n",
    "    for component_key, df in sl_data.items():\n",
    "\n",
    "        # Reformat\n",
    "        new_entry = {}\n",
    "        for name in col_names:\n",
    "            values = unyt.unyt_array( df[name].values )\n",
    "            if name in [ 'nH', 'T', 'Z', 'NHI', ]:\n",
    "                new_entry[name] = 10.**values\n",
    "            else:\n",
    "                new_entry[name] = values\n",
    "\n",
    "        # Add LOS velocity\n",
    "        new_entry['vlos'] = zeff_to_vel( df['z'].values )\n",
    "\n",
    "        # Setup units\n",
    "        for j, name in enumerate( col_names ):\n",
    "            new_entry[name] *= col_units[j]\n",
    "\n",
    "        sl_data[component_key] = new_entry\n",
    "\n",
    "    col_names.append( 'vlos' )\n",
    "\n",
    "    # Turn samples into histograms, formatted for a violin plot\n",
    "    keys = list( sl_data.keys() )\n",
    "    sl_formatted = {}\n",
    "    sl_hists = {}\n",
    "    for name in col_names:\n",
    "        sl_formatted[name] = [ sl_data[_][name] for _ in keys ]\n",
    "        sl_hists[name] = [\n",
    "            np.histogram( _, bins=64 )\n",
    "            for _ in sl_formatted[name]\n",
    "        ]\n",
    "\n",
    "    # Generate modeled sample to plot (\"generate\" because we're sampling the doppler broadening)\n",
    "    if pm['broaden_models']:\n",
    "        sl_tiled = {}\n",
    "        for name in col_names:\n",
    "            sl_tiled[name] = []\n",
    "\n",
    "        for j, vlos_j in enumerate( tqdm.tqdm( sl_formatted['vlos'] ) ):\n",
    "            sample_dist = scipy.stats.norm( loc=vlos_j, scale=sl_formatted['bturb'][j]/np.sqrt( 2. ) )\n",
    "            sampled_values = sample_dist.rvs( ( n_sample_turb, vlos_j.size ) )\n",
    "\n",
    "            for name in col_names:\n",
    "                if name != 'vlos':\n",
    "                    arr_tiled = np.hstack( np.tile( sl_formatted[name][j], ( n_sample_turb, 1 ),  ) )\n",
    "                else:\n",
    "                    arr_tiled = np.hstack( sampled_values )\n",
    "\n",
    "                arr_tiled *= sl_formatted[name][j].units\n",
    "\n",
    "                sl_tiled[name].append( arr_tiled )\n",
    "\n",
    "        sl_used = sl_tiled\n",
    "    else:\n",
    "        sl_used = sl_formatted\n",
    "\n",
    "    # Minimum weighting is to make sure no component is overweighted due to number of samples\n",
    "    n_samples_max = np.max([ _.size for _ in sl_used['nH'] ])\n",
    "    if pm['weighting'] is None:\n",
    "        model_weights = [ np.full( _.size, n_samples_max / _.size ) for _ in sl_used['nH'] ]\n",
    "    if pm['weighting'] == 'total_column':\n",
    "        model_weights = [ np.full( _.size, np.nanmedian( _ ) * n_samples_max / _.size ) for _ in sl_used['nH'] ]\n",
    "    stacked_weights = np.hstack( model_weights )\n",
    "\n",
    "    sl_stacked = {}\n",
    "    for key, item in sl_used.items():\n",
    "        sl_stacked[key] = np.hstack( item ) * item[0].units\n",
    "\n",
    "    # Get the weights for the ray\n",
    "    weights = copy.copy( ray_data['NH'].value )\n",
    "    weights[np.isclose(ray_data['NH'],0.)] = np.nan\n",
    "\n",
    "    bins = {}\n",
    "    for key, item in lims.items():\n",
    "        if autolims[key]:\n",
    "            low = np.nanmin(np.hstack([ sl_stacked[key], ray_data[key] ]))\n",
    "            high = np.nanmax(np.hstack([ sl_stacked[key], ray_data[key] ]))\n",
    "        else:\n",
    "            low = item[0]\n",
    "            high = item[1]\n",
    "        if logscale[key]:\n",
    "            bins[key] = np.logspace( np.log10( low ), np.log10( high ), n_bins )\n",
    "        else:\n",
    "            bins[key] = np.linspace( low, high, n_bins )\n",
    "\n",
    "        bins[key] *= sl_stacked[key].units       \n",
    "\n",
    "    dx = {}\n",
    "    for key, bins_j in bins.items():\n",
    "\n",
    "        if logscale[key]:\n",
    "            dx[key] = np.log10( bins_j[1] ) - np.log10( bins_j[0] )\n",
    "        else:\n",
    "            dx[key] = float( ( bins_j[1] - bins_j[0] ).value )\n",
    "\n",
    "    centers = {}\n",
    "    for key, bins_j in bins.items():\n",
    "\n",
    "        if logscale[key]:\n",
    "            bins_j = np.log10( bins_j )\n",
    "\n",
    "        centers[key] = bins_j[:-1] + 0.5 * np.diff( bins_j )\n",
    "\n",
    "        if logscale[key]:\n",
    "            centers[key] = 10.**centers[key]\n",
    "\n",
    "    # Setup Figure\n",
    "    n_cols = len( prop_keys )\n",
    "    fig = plt.figure( figsize=( panel_length*n_cols, panel_length*n_cols ), )\n",
    "    ax_dict = fig.subplot_mosaic( mosaic )\n",
    "\n",
    "    # Loop through all properties\n",
    "    for j, x_key in enumerate( tqdm.tqdm( prop_keys ) ):\n",
    "        for k, y_key in enumerate( prop_keys ):\n",
    "\n",
    "            # Avoid duplicates\n",
    "            if k < j:\n",
    "                continue\n",
    "\n",
    "            # Check for out-of-bounds\n",
    "            oob_labels = [ 'modeled', 'ray' ]\n",
    "            for ii, key in enumerate([ x_key, y_key ]):\n",
    "                for jj, values in enumerate([ sl_stacked[key], ray_data[key] ]):\n",
    "                    n_low = ( values < bins[key][0] ).sum()\n",
    "                    n_high = ( values > bins[key][-1] ).sum()\n",
    "                    bounds = [ 'below', 'above' ]\n",
    "                    for kk, n_oob in enumerate([ n_low, n_high ]):\n",
    "                        if n_oob > 0:\n",
    "                            warnings.warn(\n",
    "                                '{} {} points ({:.2g}%) with {} {} {:.3g}'.format(\n",
    "                                    n_oob,\n",
    "                                    oob_labels[jj],\n",
    "                                    n_oob / values.size * 100,\n",
    "                                    key,\n",
    "                                    bounds[kk],\n",
    "                                    lims[key][kk],\n",
    "                                )\n",
    "                            )\n",
    "\n",
    "            # 1D histogram\n",
    "            if j == k:\n",
    "                ax = ax_dict[x_key]\n",
    "                subplotspec = ax.get_subplotspec()\n",
    "\n",
    "                x_label = labels[x_key]\n",
    "                y_label = 'probability'\n",
    "\n",
    "                # Observational\n",
    "                hist_o, edges = np.histogram(\n",
    "                    sl_stacked[x_key],\n",
    "                    bins = bins[x_key],\n",
    "                    weights = stacked_weights,\n",
    "                    density = False,\n",
    "                )\n",
    "                hist_o /= hist_o.sum() * dx[x_key]\n",
    "                ax.fill_between(\n",
    "                    edges[:-1],\n",
    "                    hist_o,\n",
    "                    color = color_modeled,\n",
    "                    step = 'post',\n",
    "                )\n",
    "\n",
    "                # Ray\n",
    "                hist_r, edges = np.histogram(\n",
    "                    ray_data[x_key],\n",
    "                    bins = bins[x_key],\n",
    "                    weights = weights,\n",
    "                    density = False,\n",
    "                )\n",
    "                hist_r /= hist_r.sum() * dx[x_key]\n",
    "                ax.step(\n",
    "                    edges[:-1],\n",
    "                    hist_r,\n",
    "                    color = color_data,\n",
    "                    where = 'post',\n",
    "                    linewidth = 2,\n",
    "                )\n",
    "\n",
    "                y_max = np.nanmax([ hist_r, hist_o ])\n",
    "                ax.set_ylim( 0, y_max * 1.05 )\n",
    "                ax.set_xlim( bins[x_key][0], bins[x_key][-1] )\n",
    "\n",
    "                if logscale[x_key]:\n",
    "                    ax.set_xscale( 'log' )\n",
    "\n",
    "                ax.tick_params(\n",
    "                    which = 'both',\n",
    "                    labelleft = subplotspec.is_first_col(),\n",
    "                    right = True,\n",
    "                    labelright = True,\n",
    "                )\n",
    "\n",
    "            # 2D histogram\n",
    "            else:\n",
    "                try:\n",
    "                    ax = ax_dict['{}_{}'.format( x_key, y_key )]\n",
    "                except KeyError:\n",
    "                    ax = ax_dict['{}_{}'.format( y_key, x_key )]\n",
    "                subplotspec = ax.get_subplotspec()\n",
    "\n",
    "                # Construct histograms to plot\n",
    "                used_weights = stacked_weights / ( stacked_weights.sum() * dx[x_key] * dx[y_key] )\n",
    "                hist2d_o, x_edges, y_edges = np.histogram2d(\n",
    "                    sl_stacked[x_key],\n",
    "                    sl_stacked[y_key],\n",
    "                    bins = [ bins[x_key], bins[y_key], ],\n",
    "                    weights = used_weights,\n",
    "                )\n",
    "                img_arr_o = np.transpose( hist2d_o )\n",
    "                used_weights = weights / ( weights.sum() * dx[x_key] * dx[y_key] )\n",
    "                hist2d_r, x_edges, y_edges = np.histogram2d(\n",
    "                    ray_data[x_key],\n",
    "                    ray_data[y_key],\n",
    "                    bins = [ bins[x_key], bins[y_key], ],\n",
    "                    weights = used_weights,\n",
    "                )\n",
    "                img_arr_r = np.transpose( hist2d_r )\n",
    "\n",
    "                # Choose when to plot (plot the higher in the bin)\n",
    "                o_greater = img_arr_o > img_arr_r\n",
    "                img_arr_r[o_greater] = np.nan\n",
    "                img_arr_o[np.invert(o_greater)] = np.nan\n",
    "                img_arr_r[img_arr_r < 1e-15] = np.nan\n",
    "                img_arr_o[img_arr_o < 1e-15] = np.nan\n",
    "\n",
    "                # Plot\n",
    "                ax.pcolormesh(\n",
    "                    centers[x_key],\n",
    "                    centers[y_key],\n",
    "                    img_arr_o,\n",
    "                    cmap = cmap_modeled,\n",
    "                    shading = 'nearest',\n",
    "                )\n",
    "                ax.pcolormesh(\n",
    "                    centers[x_key],\n",
    "                    centers[y_key],\n",
    "                    img_arr_r,\n",
    "                    cmap = cmap_data,\n",
    "                    shading = 'nearest',\n",
    "                )\n",
    "\n",
    "                # Observational models\n",
    "    #             used_weights = stacked_weights / ( stacked_weights.sum() * dx[x_key] * dx[y_key] )\n",
    "    #             hist2d_o, x_edges, y_edges, img = ax.hist2d(\n",
    "    #                 sl_stacked[x_key],\n",
    "    #                 sl_stacked[y_key],\n",
    "    #                 bins = [ bins[x_key], bins[y_key] ],\n",
    "    #                 cmap = cmap_modeled,\n",
    "    #                 weights = used_weights,\n",
    "    # #                 norm = matplotlib.colors.LogNorm(),\n",
    "    #                 cmin = 1e-15,\n",
    "    #             )\n",
    "    #             hist2d_o, x_edges, y_edges = np.histogram2d(\n",
    "    #                 sl_stacked[x_key],\n",
    "    #                 sl_stacked[y_key],\n",
    "    #                 bins = [ bins[x_key], bins[y_key], ],\n",
    "    #                 weights = used_weights,\n",
    "    #             )\n",
    "    #             img_arr_o = np.transpose( hist2d_o )\n",
    "    #             ax.pcolormesh(\n",
    "    #                 centers[x_key],\n",
    "    #                 centers[y_key],\n",
    "    #                 img_arr_o,\n",
    "    #                 cmap = cmap_modeled,\n",
    "    #                 shading = 'nearest',\n",
    "    #             )\n",
    "    # #             ax.contour(\n",
    "    # #                 centers[x_key],\n",
    "    # #                 centers[y_key],\n",
    "    # #                 np.transpose( hist2d_o ),\n",
    "    # #                 levels = [ 0.98, ],\n",
    "    # #             )\n",
    "\n",
    "    #             # Plot the ray\n",
    "    #             used_weights = weights / ( weights.sum() * dx[x_key] * dx[y_key] )\n",
    "    #             hist2d_r, x_edges, y_edges, img = ax.hist2d(\n",
    "    #                 ray_data[x_key],\n",
    "    #                 ray_data[y_key],\n",
    "    #                 bins = [ bins[x_key], bins[y_key] ],\n",
    "    #                 cmap = cmap_data,\n",
    "    #                 weights = used_weights,\n",
    "    # #                 norm = matplotlib.colors.LogNorm(),\n",
    "    #                 cmin = 1e-15,\n",
    "    #             )\n",
    "\n",
    "                ax.set_xlim( bins[x_key][0], bins[x_key][-1] )\n",
    "                ax.set_ylim( bins[y_key][0], bins[y_key][-1] )\n",
    "\n",
    "                if logscale[x_key]:\n",
    "                    ax.set_xscale( 'log' )\n",
    "                if logscale[y_key]:\n",
    "                    ax.set_yscale( 'log' )\n",
    "\n",
    "                x_label = labels[x_key]\n",
    "                y_label = labels[y_key]\n",
    "\n",
    "            if subplotspec.is_last_row():\n",
    "                ax.set_xlabel( x_label, fontsize=16 )\n",
    "            if subplotspec.is_first_col():\n",
    "                ax.set_ylabel( y_label, fontsize=16 )\n",
    "\n",
    "    # Save\n",
    "    savedir = './figures/sample2/comparison'\n",
    "    if pm['weighting'] == 'density':\n",
    "        savedir = os.path.join( savedir, 'density_weighting' )\n",
    "    os.makedirs( savedir, exist_ok=True )\n",
    "    savefile = 'sightline_{}.png'.format( os.path.basename( sl_fps[i] ) )\n",
    "    save_fp = os.path.join( savedir, savefile )\n",
    "    plt.savefig( save_fp, bbox_inches='tight' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "643fdca5-8ba3-40f8-bbf5-448756c3fc8c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
