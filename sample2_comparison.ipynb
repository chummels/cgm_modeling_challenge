{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c38063-dde6-421f-a5b0-8a1d3320f44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import tqdm\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b816548-85d0-44e9-abf7-9be458930aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt\n",
    "import trident\n",
    "import unyt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0164758a-30ce-4190-9afe-911c96c2dd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kalepy as kale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5de870-5fd1-4465-b7dd-bd452f2b13cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trove\n",
    "import verdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7eadfbae-2549-4873-8e31-47c5ad3048a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.style.use( '/Users/zhafen/repos/clean-bold/clean-bold.mplstyle' )\n",
    "import palettable\n",
    "import matplotlib.patheffects as path_effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d2298c-a236-4416-8597-2e9b9bc3d016",
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67801afa-d127-43dc-a211-164c72b318b0",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ede9ac4-52f5-4e59-9eeb-b720128fc41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm = {\n",
    "    # Analysis \n",
    "    'prop_keys': [ 'vlos', 'T', 'nH', 'Z', 'x' ],\n",
    "    'vel_prop_keys': [ 'vlos', 'T', 'nH', 'Z', 'NHI' ],\n",
    "    'broaden_models': False,\n",
    "    '1D_dist_estimation': 'kde',\n",
    "    '1D_dist_estimation_data': 'histogram',\n",
    "    '2D_dist_estimation': 'histogram',\n",
    "    \n",
    "    # Plotting Choices\n",
    "    'smooth_2D_dist': 0.5,\n",
    "    'upsample_2D_dist': 3,\n",
    "    '2D_dist_data_display': 'histogram',\n",
    "    'contour_levels': [ 68, ],\n",
    "    'contour_linewidths': [ 3, ],\n",
    "    'show_plots_in_nb': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59d8a89b-eb8b-4082-a33e-f5ae75481b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parameters\n",
    "pm = trove.link_params_to_config(\n",
    "    '/Users/zhafen/analysis/cgm_modeling_challenge/sample2.trove',\n",
    "    script_id = 'nb.2',\n",
    "    variation = 'high-z',\n",
    "    global_variation = '',\n",
    "    **pm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a19c28c-ffda-44cf-86e7-5a0ffb334316",
   "metadata": {},
   "outputs": [],
   "source": [
    "redshift = pm['redshift']\n",
    "prop_keys = pm['prop_keys']\n",
    "n_sample_turb = pm['n_sample_turb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4395b3-67c4-44ab-bbdb-771c8feb75dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pm['data_dir']\n",
    "base_data_dir = pm['base_data_dir']\n",
    "ray_dir = os.path.join( base_data_dir, 'rays' )\n",
    "results_dir = os.path.join( pm['results_dir'], pm['variation'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa102182-b083-45cd-bf91-4e030dae407c",
   "metadata": {},
   "outputs": [],
   "source": [
    "calc_correlations = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a587b8-112e-4bc2-a79e-5f2bd235ff38",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6b33a4f-c66e-4f93-a1ca-c369081ad97c",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_coefficients = {\n",
    "    'one-sided': {},\n",
    "    'log one-sided': { 'logscale': True, 'subtract_mean': True },\n",
    "    'two-sided': { 'one_sided': False, },\n",
    "    'linear': { 'one_sided': False, 'subtract_mean': True },\n",
    "    'log': { 'logscale': True, 'one_sided': False, 'subtract_mean': True },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89384288-a000-493c-8aef-5449ba1075b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "lims = helpers.lims\n",
    "lims_1D = helpers.lims_1D\n",
    "autolims = helpers.autolims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1293db0b-c166-453b-ac70-4b1b5e751125",
   "metadata": {},
   "outputs": [],
   "source": [
    "dvs = {\n",
    "    'vlos': 5.,\n",
    "    'T': 0.05,\n",
    "    'nH': 0.05,\n",
    "    'Z': 0.05,\n",
    "    'NHI': 0.05,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8e0429a-4359-43f1-835d-09c099a9543e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logscale = helpers.logscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b03ea2c2-ae09-44a6-8ea4-a74a1a8f278a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_format = pm['bar_format']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb0e6452-3293-4ccb-ade0-679aa24cc559",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ecdfad-1324-4c0a-aa85-14c0827e7c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "variation_plotting_params = {\n",
    "    'original': {\n",
    "        'color': helpers.blinded_color,\n",
    "    },\n",
    "    'high-z': {\n",
    "        'color': helpers.revised_color,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b87a384-9e11-4a6d-8c87-2383c2ff5c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = helpers.property_labels\n",
    "labels_1D = helpers.property_labels_1D\n",
    "r_labels = helpers.correlation_coefficient_property_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "678ee98c-11e5-4c9f-b52f-28362b0d8fce",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_markers = {\n",
    "    'one-sided': '^',\n",
    "    'log one-sided': '^',\n",
    "    'two-sided': 'D',\n",
    "    'linear': 'o',\n",
    "    'log': 'o',\n",
    "}\n",
    "correlation_sizes = {\n",
    "    'one-sided': 100,\n",
    "    'log one-sided': 100,\n",
    "    'two-sided': 80,\n",
    "    'linear': 100,\n",
    "    'log': 100,\n",
    "}\n",
    "correlations_plotted = [ 'linear', 'log' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40534fa3-c089-4686-98b1-84eca25a9add",
   "metadata": {},
   "outputs": [],
   "source": [
    "mosaic = [\n",
    "    [ 'vlos', 'legend', '.', '.', '.' ],\n",
    "    [ 'T_vlos', 'T', '.', '.', '.' ],\n",
    "    [ 'nH_vlos', 'nH_T', 'nH', '.', '.' ],\n",
    "    [ 'Z_vlos', 'Z_T', 'Z_nH', 'Z', '.' ],\n",
    "    [ 'x_vlos', 'x_T', 'x_nH', 'x_Z', 'x' ],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43876f83-da1e-4084-a084-6442fa2c3bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "panel_length = 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "546af2d4-0d00-4be7-ae12-48586f50b69d",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = palettable.cartocolors.qualitative.Safe_10.mpl_colors[2:]\n",
    "corr_cmap = palettable.cartocolors.diverging.Temps_2_r.mpl_colormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59fdaaed-d447-429e-b69d-d6b32dfc08b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_norm = matplotlib.colors.Normalize( vmin=0, vmax=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "580b42fd-3340-4754-a0aa-f4f8e5f24314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_color_linear_cmap( color, name, f_white=0.95, f_saturated=1.0, ):\n",
    "    '''A function that turns a single color into linear colormap that\n",
    "    goes from a color that is whiter than the original color to a color\n",
    "    that is more saturated than the original color.\n",
    "    '''\n",
    "    \n",
    "    color_hsv = matplotlib.colors.rgb_to_hsv( color )\n",
    "    start_color_hsv = copy.copy( color_hsv )\n",
    "    \n",
    "    start_color_hsv = copy.copy( color_hsv )\n",
    "    start_color_hsv[1] -= f_white * start_color_hsv[1]\n",
    "    start_color_hsv[2] += f_white * ( 1. - start_color_hsv[2] )\n",
    "    start_color = matplotlib.colors.hsv_to_rgb( start_color_hsv )\n",
    "    \n",
    "    end_color_hsv = copy.copy( color_hsv )\n",
    "    end_color_hsv[1] += f_saturated * ( 1. - end_color_hsv[1] )\n",
    "    end_color = matplotlib.colors.hsv_to_rgb( end_color_hsv )\n",
    "    \n",
    "    return matplotlib.colors.LinearSegmentedColormap.from_list( name, [ start_color, end_color ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf264d50-746a-4fdb-a00a-fdb886e535e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_estimated = variation_plotting_params[pm['variation']]['color']\n",
    "color_data = ( 0, 0, 0 )\n",
    "cmap_estimated = one_color_linear_cmap( color_estimated, 'estimated' )\n",
    "cmap_data = one_color_linear_cmap( color_data, 'source' )\n",
    "cmap_data = matplotlib.colors.LinearSegmentedColormap.from_list( 'source', [ 'w', color_data ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8e3c8ad-092a-4b13-a80e-299fa7106912",
   "metadata": {},
   "outputs": [],
   "source": [
    "public_label = pm['public_label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f924f57-6164-498a-a332-95742c695ffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_levels = pm['contour_levels']\n",
    "contour_linewidths = pm['contour_linewidths']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6412343f-f4f3-4354-9f0b-bbaec6742fce",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6689cb9-a83a-451a-a2f8-69904c7d45a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ldb = trident.LineDatabase(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429e5062-1c75-4e21-9ddd-d669402c2960",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeff_to_vel( zeff ):\n",
    "    \n",
    "    ainv2 = ( ( 1. + zeff ) / ( 1. + redshift ) )**2.\n",
    "    \n",
    "    v_div_c = ( ainv2 - 1. ) / ( ainv2 + 1. )\n",
    "    return ( v_div_c * unyt.c ).to( 'km/s' ).value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ea16a6b-4379-4e12-9910-30ba7ef769d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "unyt.Zsun"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1600ccba-a75e-460b-ac46-21db0888e377",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b1f640-983b-4c87-a1b3-713f1b1808ec",
   "metadata": {},
   "source": [
    "## Estimated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a494c20f-521a-47fb-8857-b1a2499b8888",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_data_formats = {\n",
    "    'v0': {\n",
    "        'col_names': [ 'prob', 'likelihood', 'Z', 'nH', 'T', 'NHI', 'bturb', 'z', ],\n",
    "        'col_units':  [ 1., 1., unyt.Zsun, unyt.cm**-3, unyt.K, unyt.cm**-2, unyt.km / unyt.s, 1. ],\n",
    "        'vlos_from_z': True,\n",
    "        'dir_structure': 'one_dir_per_sl',\n",
    "    },\n",
    "    'v1': {\n",
    "        'col_names': [ 'prob', 'likelihood', 'Z', 'nH', 'T', 'NHI', 'vlos', 'bturb', 'thickness' ],\n",
    "        'col_units':  [ 1., -0.5, unyt.Zsun, unyt.cm**-3, unyt.K, unyt.cm**-2, unyt.km / unyt.s,  unyt.km / unyt.s, 1. ],\n",
    "        'vlos_from_z': False,\n",
    "        'dir_structure': 'one_dir_per_sl',\n",
    "    },\n",
    "    'v2': {\n",
    "        'raw_col_names': [ 'Z', 'nH', 'T', 'NHI', 'bturb', 'L', 'V', 'btherm', 'btherm_HI' ],\n",
    "        'col_names': [ 'Z', 'nH', 'T', 'NHI', 'bturb', 'L', 'vlos', 'btherm', 'btherm_HI' ],\n",
    "        'col_units':  [ unyt.Zsun, unyt.cm**-3, unyt.K, unyt.cm**-2, unyt.km / unyt.s, unyt.kpc, unyt.km / unyt.s, unyt.km / unyt.s, unyt.km / unyt.s, ],\n",
    "        'vlos_from_z': False,\n",
    "        'dir_structure': 'one_dir',\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60666e13-a12b-4a43-aa7b-5c9d63af07fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "estimated_data_format = estimated_data_formats[pm['estimated_data_format']]\n",
    "col_names = copy.copy( estimated_data_format['col_names'] )\n",
    "col_units = copy.copy( estimated_data_format['col_units'] )\n",
    "if estimated_data_format['vlos_from_z']:\n",
    "    col_names.append( 'vlos' )\n",
    "    col_units.append( unyt.km / unyt.s )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f509047-0365-44f7-b8d6-9053dbd94ee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sls = []\n",
    "sl_fps = []\n",
    "mle_fps = []\n",
    "other_fps = []\n",
    "sl_datas = []\n",
    "mle_datas = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5e7e56a-cba9-44c2-80f4-e7a05207dd34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the old data format\n",
    "if estimated_data_format['dir_structure'] == 'one_dir_per_sl':\n",
    "    for sl_fp in glob.glob( os.path.join( results_dir, '*' ) ):\n",
    "\n",
    "        if not os.path.isdir( sl_fp ):\n",
    "            other_files.append( sl_fp )\n",
    "            continue\n",
    "\n",
    "        i = os.path.split( sl_fp )[-1]\n",
    "\n",
    "        if int( i ) in pm['selected_sightlines']:\n",
    "\n",
    "            sl_fps.append( sl_fp )\n",
    "            sls.append( i )\n",
    "            \n",
    "        sl_data = {}\n",
    "        for component_fp in glob.glob( os.path.join( sl_fp, '*' ) ):\n",
    "            component_key = os.path.splitext( os.path.split( component_fp )[-1] )[0]\n",
    "            sl_data[component_key] = pd.read_csv( component_fp, sep=' ', names=col_names )\n",
    "        sl_datas.append( sl_data )\n",
    "            \n",
    "    mle_data_found = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91da438d-84f0-4942-b249-556506474395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the new format data\n",
    "if estimated_data_format['dir_structure'] == 'one_dir':\n",
    "    \n",
    "    for fp in glob.glob( os.path.join( results_dir, '*' ) ):\n",
    "        \n",
    "        fn = os.path.split( fp )[-1]\n",
    "        \n",
    "        if os.path.splitext( fn )[-1] != '.pkl':\n",
    "            other_fps.append( fp )\n",
    "            continue\n",
    "            \n",
    "        i, ext = fn.split( '_' )\n",
    "        \n",
    "        if int( i ) not in pm['selected_sightlines']:\n",
    "            continue\n",
    "            \n",
    "        # Load and do initial processing of full SL data\n",
    "        if ext == 'full.pkl':\n",
    "                       \n",
    "            # Load the file\n",
    "            with open( fp, 'rb' ) as f:\n",
    "                sl_raw_data = pickle.load( f )\n",
    "                \n",
    "            # Extract the data\n",
    "            sl_data = {}\n",
    "            for column_name in sl_raw_data.columns:\n",
    "\n",
    "                # Get component\n",
    "                ion = '_'.join( column_name.split( '_' )[:2] )\n",
    "\n",
    "                # Don't re-add\n",
    "                if ion in sl_data.keys():\n",
    "                    continue\n",
    "\n",
    "                # Store\n",
    "                ion_col_names = [ '{}_{}'.format( ion, _ ) for _ in estimated_data_format['raw_col_names'] ]    \n",
    "                sl_data[ion] = sl_raw_data[ion_col_names]\n",
    "\n",
    "                # Rename\n",
    "                sl_data[ion].columns=estimated_data_format['col_names']\n",
    "            \n",
    "            sl_datas.append( sl_data )\n",
    "            sl_fps.append( fp )\n",
    "            sls.append( i )\n",
    "                \n",
    "        elif ext == 'mle.pkl':\n",
    "            mle_fps.append( fp )\n",
    "            \n",
    "mle_data_found = len( mle_datas ) > 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "417bb82e-449a-4faf-b67b-42a2e2de2ba2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process data\n",
    "sl_useds = []\n",
    "sl_compkeys = []\n",
    "sl_stackeds = []\n",
    "model_weightss = []\n",
    "stacked_weightss = []\n",
    "total_weightss = []\n",
    "median_NHIs = []\n",
    "for i, sl in enumerate( sls ):\n",
    "    \n",
    "    sl_fp = sl_fps[i]\n",
    "    \n",
    "    # Get text files\n",
    "\n",
    "    sl_data= sl_datas[i]\n",
    "        \n",
    "    # Add LOS velocity and reformat\n",
    "    for component_key, df in sl_data.items():\n",
    "\n",
    "        # Reformat\n",
    "        new_entry = {}\n",
    "        for name in col_names:\n",
    "            values = unyt.unyt_array( df[name].values )\n",
    "            if name in [ 'nH', 'T', 'Z', 'NHI', 'thickness' ]:\n",
    "                new_entry[name] = 10.**values\n",
    "            else:\n",
    "                new_entry[name] = values\n",
    "\n",
    "        # Add LOS velocity\n",
    "        if estimated_data_format['vlos_from_z']:\n",
    "            new_entry['vlos'] = zeff_to_vel( df['z'].values )\n",
    "\n",
    "        # Set up units\n",
    "        for j, name in enumerate( col_names ):\n",
    "            new_entry[name] *= col_units[j]\n",
    "\n",
    "        sl_data[component_key] = new_entry\n",
    "        \n",
    "    # Turn samples into a list\n",
    "    keys = sorted( list( sl_data.keys() ) )\n",
    "    sl_formatted = {}\n",
    "    for name in col_names:\n",
    "        sl_formatted[name] = [ sl_data[_][name] for _ in keys ]\n",
    "\n",
    "    # Generate estimated sample to plot (\"generate\" because we're sampling the doppler broadening)\n",
    "    if pm['broaden_models']:\n",
    "        sl_tiled = {}\n",
    "        for name in col_names:\n",
    "            sl_tiled[name] = []\n",
    "\n",
    "        for j, vlos_j in enumerate( tqdm.tqdm( sl_formatted['vlos'], bar_format=bar_format ) ):\n",
    "            sample_dist = scipy.stats.norm( loc=vlos_j, scale=sl_formatted['bturb'][j]/np.sqrt( 2. ) )\n",
    "            sampled_values = sample_dist.rvs( ( n_sample_turb, vlos_j.size ) )\n",
    "\n",
    "            for name in col_names:\n",
    "                if name != 'vlos':\n",
    "                    arr_tiled = np.hstack( np.tile( sl_formatted[name][j], ( n_sample_turb, 1 ),  ) )\n",
    "                else:\n",
    "                    arr_tiled = np.hstack( sampled_values )\n",
    "\n",
    "                arr_tiled *= sl_formatted[name][j].units\n",
    "\n",
    "                sl_tiled[name].append( arr_tiled )\n",
    "\n",
    "        sl_used = sl_tiled\n",
    "    else:\n",
    "        sl_used = sl_formatted\n",
    "\n",
    "    # Minimum weighting is to make sure no component is overweighted due to number of samples\n",
    "    n_samples_max = np.max([ _.size for _ in sl_used['nH'] ])\n",
    "    if pm['weighting'] is None:\n",
    "        model_weights = [ np.full( _.size, n_samples_max / _.size ) for _ in sl_used['nH'] ]\n",
    "    else:\n",
    "        model_weights = [ np.full( _.size, np.nanmedian( _ ) * n_samples_max / _.size ) for _ in sl_used[pm['weighting']] ]\n",
    "    model_weightss.append( model_weights )\n",
    "    stacked_weightss.append( np.hstack( model_weights ) )\n",
    "    total_weightss.append( np.array([ _.sum() for _ in model_weights ]) )\n",
    "    median_NHIs.append( np.array([ np.nanmedian( _ ) for _ in sl_used['NHI'] ]) )\n",
    "\n",
    "    sl_stacked = {}\n",
    "    for key, item in sl_used.items():\n",
    "        sl_stacked[key] = np.hstack( item ) * item[0].units\n",
    "    \n",
    "    sl_datas.append( sl_data )\n",
    "    sl_useds.append( sl_used )\n",
    "    sl_compkeys.append( keys )\n",
    "    sl_stackeds.append( sl_stacked )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8094aca-220b-4b0d-8d46-c50d11157138",
   "metadata": {},
   "source": [
    "## Actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81ad3c8e-64e9-4a25-841f-55df6fa617ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get rays\n",
    "ray_fps = [ os.path.join( ray_dir, 'ray_{}.h5'.format( _[1:] ) ) for _ in sls ]\n",
    "rays = [ yt.load( _ ) for _ in ray_fps ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa36c63-10d6-40ac-a680-02a24a6f637c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray_datas = []\n",
    "ray_weights = []\n",
    "for ray in rays:\n",
    "    # Ray properties\n",
    "    trident.add_ion_fields(ray, ions=[ 'H I', ], line_database=ldb)\n",
    "    den = ray.r[('gas', 'number_density')] * 0.75\n",
    "    ray_data = {\n",
    "        'nH': den,\n",
    "        'NH': ( den * ray.r[('gas', 'dl')] ),\n",
    "        'NHI': ray.r[('gas', 'H_p0_number_density')] * ray.r[('gas', 'dl')],\n",
    "        'z': ray.r[('gas', 'redshift_eff')],\n",
    "        'T': ray.r[('gas', 'temperature')],\n",
    "        'Z': ray.r[('gas', 'metallicity')],\n",
    "        'x': ray.r[('gas', 'z')] - 48. * unyt.kpc,\n",
    "    }\n",
    "    ray_data['vlos'] = zeff_to_vel( ray_data['z'] ) * unyt.km / unyt.s\n",
    "    \n",
    "    if pm['sim_weighting'] is None:\n",
    "        weights = np.ones( ray_data['NH'].shape )\n",
    "    else:\n",
    "        weights = copy.copy( ray_data[pm['sim_weighting']].value )\n",
    "        weights[np.isclose(weights,0.)] = np.nan\n",
    "    \n",
    "    ray_datas.append( ray_data )\n",
    "    ray_weights.append( weights )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73772698-1f28-4f08-b14f-8cd87622c93b",
   "metadata": {},
   "source": [
    "## Bins and Limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585f891c-b83f-40c1-81a1-4312c90a17b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_binss = []\n",
    "all_dxs = []\n",
    "all_centerss = []\n",
    "for i, ray_data in enumerate( tqdm.tqdm( ray_datas, bar_format=bar_format ) ):\n",
    "    \n",
    "    sl_stacked = sl_stackeds[i]\n",
    "\n",
    "    # Make bins, dx, and centers\n",
    "    all_bins = {}\n",
    "    all_dx = {}\n",
    "    all_centers = {}\n",
    "    for n_bins_key in [ 'n_bins_1D', 'n_bins_2D', 'n_bins_data_1D', 'n_bins_data_2D', 'n_bins_convolve' ]:\n",
    "        n_bins = pm[n_bins_key]\n",
    "        bins = {}\n",
    "        for key, item in lims.items():\n",
    "            if autolims[key]:\n",
    "                if key in sl_stacked:\n",
    "                    values_for_limits = np.hstack([ sl_stacked[key], ray_data[key] ])\n",
    "                else:\n",
    "                    values_for_limits = ray_data[key].value\n",
    "                low = np.nanmin(values_for_limits)\n",
    "                high = np.nanmax(values_for_limits)\n",
    "            else:\n",
    "                low = item[0]\n",
    "                high = item[1]\n",
    "            if logscale[key]:\n",
    "                bins[key] = np.logspace( np.log10( low ), np.log10( high ), n_bins )\n",
    "            else:\n",
    "                bins[key] = np.linspace( low, high, n_bins )\n",
    "\n",
    "            bins[key] *= ray_data[key].units\n",
    "        all_bins[n_bins_key] = bins\n",
    "\n",
    "        dx = {}\n",
    "        for key, bins_j in bins.items():\n",
    "            if logscale[key]:\n",
    "                dx[key] = np.log10( bins_j[1] ) - np.log10( bins_j[0] )\n",
    "            else:\n",
    "                dx[key] = float( ( bins_j[1] - bins_j[0] ).value )\n",
    "        all_dx[n_bins_key] = dx\n",
    "\n",
    "        centers = {}\n",
    "        for key, bins_j in bins.items():\n",
    "\n",
    "            if logscale[key]:\n",
    "                bins_j = np.log10( bins_j )\n",
    "\n",
    "            centers[key] = bins_j[:-1] + 0.5 * np.diff( bins_j )\n",
    "\n",
    "            if logscale[key]:\n",
    "                centers[key] = 10.**centers[key]\n",
    "        all_centers[n_bins_key] = centers\n",
    "        \n",
    "    all_binss.append( all_bins )\n",
    "    all_dxs.append( all_dx )\n",
    "    all_centerss.append( all_centers )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dc4f11-67d6-4347-ad74-e225d46523fc",
   "metadata": {},
   "source": [
    "# Comparison Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80991533-89d1-4e92-98b1-a4429a38540a",
   "metadata": {},
   "source": [
    "## Calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ce6a343-fb23-447b-b09d-15a7ba511857",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_correlation( found, actual, logscale=False, one_sided=True, subtract_mean=False, ):\n",
    "    '''Calculate the correlation coefficient.\n",
    "    \n",
    "    Args:\n",
    "        found (array-like):\n",
    "            estimated data.\n",
    "            \n",
    "        actual (array-like):\n",
    "            Actual data.\n",
    "            \n",
    "        logscale (bool):\n",
    "            If True, do comparison after putting input arrays in logspace.\n",
    "            \n",
    "        one_sided (bool):\n",
    "            If True, normalize by actual.sum()**2, not sqrt( found.sum()**2 * actual.sum()**2 ).\n",
    "            \n",
    "        subtract_mean (bool):\n",
    "            If True, subtract the mean prior to calculating correlations.\n",
    "    '''\n",
    "    \n",
    "    found = copy.copy( found )\n",
    "    actual = copy.copy( actual )\n",
    "    \n",
    "    # Logscale\n",
    "    if logscale:\n",
    "        found = np.log10( found )\n",
    "        actual = np.log10( actual )\n",
    "    \n",
    "    # Identify valid values\n",
    "    found_valid = np.isfinite( found )\n",
    "    actual_valid = np.isfinite( actual )\n",
    "    is_valid = found_valid & actual_valid\n",
    "    \n",
    "    if subtract_mean:\n",
    "        found -= np.mean( found[found_valid] )\n",
    "        actual -= np.mean( actual[actual_valid] )\n",
    "    \n",
    "    # Calculate the correlation\n",
    "    top = found * actual\n",
    "    top_sum = top[is_valid].sum()\n",
    "    \n",
    "    # Calculate the normalization\n",
    "    bottom = actual**2.\n",
    "    bottom_sum = bottom[actual_valid].sum()\n",
    "    if not one_sided:\n",
    "        bottom_found = found**2.\n",
    "        bottom_sum_found = bottom_found[found_valid].sum()\n",
    "        bottom_sum = np.sqrt( bottom_sum * bottom_sum_found )\n",
    "    \n",
    "    return top_sum / bottom_sum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64e82f03-9a7d-4518-92c0-c8ec7c395a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if calc_correlations:\n",
    "    correlations = verdict.Dict({})\n",
    "    for i, sl in enumerate( tqdm.tqdm( sls, bar_format=bar_format ) ):\n",
    "\n",
    "        # Setup correlation matrix\n",
    "        for c_key in correlation_coefficients.keys():\n",
    "            correlation_matrix = np.full( ( len( prop_keys ), len( prop_keys ) ), np.nan )\n",
    "            correlations.setitem( c_key, correlation_matrix, 'matrix', sls[i] )\n",
    "            correlations.setitem( c_key, prop_keys, 'matrix properties', sls[i] )\n",
    "\n",
    "        # Get ray data\n",
    "        ray_data = ray_datas[i]\n",
    "        weights = ray_weights[i]\n",
    "\n",
    "        # Get estimated data\n",
    "        sl_stacked = sl_stackeds[i]\n",
    "        model_weights = model_weightss[i]\n",
    "        stacked_weights = stacked_weightss[i]\n",
    "\n",
    "        # Get bins\n",
    "        bins = all_binss[i]['n_bins_convolve']\n",
    "        dx = all_dxs[i]['n_bins_convolve']\n",
    "        centers = all_centerss[i]['n_bins_convolve']\n",
    "\n",
    "        # Format for calculating distributions\n",
    "        ray_data_histdd = []\n",
    "        sl_stacked_histdd = []\n",
    "        bins_histdd = []\n",
    "        for key in prop_keys:\n",
    "\n",
    "            arr_estimated = sl_stacked[key].value\n",
    "            arr = ray_data[key].value\n",
    "            bins_key = bins[key]\n",
    "\n",
    "            if logscale[key]:\n",
    "                arr_estimated = np.log10( arr_estimated )\n",
    "                arr = np.log10( arr )\n",
    "                bins_key = np.log10( bins_key )\n",
    "\n",
    "            sl_stacked_histdd.append( arr_estimated )\n",
    "            ray_data_histdd.append( arr )\n",
    "            bins_histdd.append( bins_key )\n",
    "\n",
    "        sl_stacked_histdd = np.array( sl_stacked_histdd ).transpose()\n",
    "        ray_data_histdd = np.array( ray_data_histdd ).transpose()\n",
    "        bins_histdd = np.array( bins_histdd )\n",
    "\n",
    "        # Calculate ND distribution for ray data\n",
    "        ray_dist_dd, bins_dd = np.histogramdd(\n",
    "            ray_data_histdd,\n",
    "            bins = bins_histdd,\n",
    "            weights = weights,\n",
    "        )\n",
    "        ray_dist_dd /= ray_dist_dd.sum()\n",
    "\n",
    "        # Calculate ND distribution for estimated data\n",
    "        estimated_dist_dd, bins_dd = np.histogramdd(\n",
    "            sl_stacked_histdd,\n",
    "            bins = bins_histdd,\n",
    "            weights = stacked_weights,\n",
    "        )\n",
    "        estimated_dist_dd /= estimated_dist_dd.sum()\n",
    "\n",
    "        # Calculate net correlation coefficients\n",
    "        for c_key, kwargs in correlation_coefficients.items():\n",
    "            corr_ndim = calc_correlation( estimated_dist_dd, ray_dist_dd, **kwargs )\n",
    "            correlations.setitem( c_key, corr_ndim, 'ndim', sls[i] )\n",
    "\n",
    "        # Calculate correlation coefficients for each property\n",
    "        for j, x_key in enumerate( prop_keys ):\n",
    "            for k, y_key in enumerate( prop_keys ):\n",
    "\n",
    "                # Avoid duplicates\n",
    "                if k < j:\n",
    "                    continue\n",
    "\n",
    "                ray_dist_2d, _, _ = np.histogram2d(\n",
    "                    ray_data_histdd[:,j],\n",
    "                    ray_data_histdd[:,k],\n",
    "                    bins = [ bins_histdd[j], bins_histdd[k] ],\n",
    "                    weights = weights,\n",
    "                )\n",
    "                ray_dist_2d /= ray_dist_2d.sum()\n",
    "\n",
    "                estimated_dist_2d, _, _ = np.histogram2d(\n",
    "                    sl_stacked_histdd[:,j],\n",
    "                    sl_stacked_histdd[:,k],\n",
    "                    bins = [ bins_histdd[j], bins_histdd[k] ],\n",
    "                    weights = stacked_weights,\n",
    "                )\n",
    "                estimated_dist_2d /= estimated_dist_2d.sum()\n",
    "\n",
    "                for c_key, kwargs in correlation_coefficients.items():\n",
    "\n",
    "                    correlation_matrix = correlations[c_key]['matrix'][sls[i]]\n",
    "                    correlation_matrix[j,k] = calc_correlation( estimated_dist_2d, ray_dist_2d, **kwargs )\n",
    "                    correlation_matrix[k,j] = correlation_matrix[j,k]\n",
    "                    correlations[c_key]['matrix'][sls[i]] = correlation_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3afa82e-f22f-484d-b9b6-183f00938408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data\n",
    "if calc_correlations & ( pm['global_variation'] == '' ):\n",
    "    correlations_fp = os.path.join( pm['polished_data_dir'], 'correlation_coefficients.h5' )\n",
    "    correlations_all = verdict.Dict.from_hdf5( correlations_fp, create_nonexistent=True )\n",
    "    correlations_all[public_label] = correlations\n",
    "    correlations_all.to_hdf5( correlations_fp )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47f224e-887a-4bab-9c57-93685b4aa33d",
   "metadata": {},
   "source": [
    "## Corner Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d10b0b44-b83c-425f-98b2-6548ea09373c",
   "metadata": {},
   "outputs": [],
   "source": [
    "if calc_correlations:\n",
    "    # Setup Figure\n",
    "    n_cols = len( prop_keys )\n",
    "    fig = plt.figure( figsize=( panel_length*n_cols, panel_length*n_cols ), facecolor='w' )\n",
    "    ax_dict = fig.subplot_mosaic(\n",
    "        mosaic,\n",
    "        gridspec_kw = { 'hspace': 0.1, 'wspace': 0.1 },\n",
    "    )\n",
    "\n",
    "    # Loop through all properties\n",
    "    for j, x_key in enumerate( tqdm.tqdm( prop_keys, bar_format=bar_format ) ):\n",
    "        for k, y_key in enumerate( prop_keys ):\n",
    "\n",
    "            # Avoid duplicates\n",
    "            if k < j:\n",
    "                continue\n",
    "\n",
    "            # Comparison to ndim\n",
    "            if j == k:\n",
    "                ax = ax_dict[x_key]\n",
    "                subplotspec = ax.get_subplotspec()\n",
    "\n",
    "                x_label = r_labels[x_key]\n",
    "                y_label = r_labels['all']\n",
    "\n",
    "                for c_key in correlations_plotted:\n",
    "                    c_params = correlation_coefficients[c_key]\n",
    "                    facecolors = 'k'\n",
    "                    if 'logscale' in c_params:\n",
    "                        if c_params['logscale']:\n",
    "                            facecolors = 'none'\n",
    "\n",
    "                    scatter = ax.scatter(\n",
    "                        correlations[c_key]['matrix'].array()[:,j,k],\n",
    "                        correlations[c_key]['ndim'].array(),\n",
    "                        label = c_key,\n",
    "                        edgecolors = 'k',\n",
    "                        facecolors = facecolors,\n",
    "                        marker = correlation_markers[c_key],\n",
    "                        s = correlation_sizes[c_key],\n",
    "                        linewidth = 2,\n",
    "                    )\n",
    "\n",
    "                if x_key in [ 'T', 'nH', 'Z' ]:\n",
    "                    ax.yaxis.set_label_position( 'right' )\n",
    "                    ax.set_ylabel( y_label, fontsize=16 )\n",
    "\n",
    "                ax.tick_params(\n",
    "                    which = 'both',\n",
    "                    right = True,\n",
    "                    labelright = True,\n",
    "                )\n",
    "\n",
    "            # 2D comparisons\n",
    "            else:\n",
    "                try:\n",
    "                    ax = ax_dict['{}_{}'.format( x_key, y_key )]\n",
    "                except KeyError:\n",
    "                    ax = ax_dict['{}_{}'.format( y_key, x_key )]\n",
    "                subplotspec = ax.get_subplotspec()\n",
    "\n",
    "                x_label = r_labels[x_key]\n",
    "                y_label = r_labels[y_key]\n",
    "\n",
    "                for c_key in correlations_plotted:\n",
    "                    colors = correlations[c_key]['matrix'].array()[:,j,k]\n",
    "                    colors = corr_cmap( corr_norm( colors ) )\n",
    "\n",
    "                    # Choose facecolor\n",
    "                    facecolors = colors\n",
    "                    c_params = correlation_coefficients[c_key]  \n",
    "                    if 'logscale' in c_params:\n",
    "                        if c_params['logscale']:\n",
    "                            facecolors = 'None'\n",
    "\n",
    "                    ax.scatter(\n",
    "                        correlations[c_key]['matrix'].array()[:,j,j],\n",
    "                        correlations[c_key]['matrix'].array()[:,k,k],\n",
    "                        edgecolors = colors,\n",
    "                        facecolors = facecolors,\n",
    "                        marker = correlation_markers[c_key],\n",
    "                        s = correlation_sizes[c_key],\n",
    "    #                     norm = ,\n",
    "                        linewidth = 2,\n",
    "                    )\n",
    "\n",
    "            # Guiding lines\n",
    "            for fn in [ ax.axvline, ax.axhline ]:\n",
    "                for value in [ -1, 0, 1 ]:\n",
    "                    fn(\n",
    "                        value,\n",
    "                        color = pm['background_linecolor'],\n",
    "                        linewidth = 1,\n",
    "                        zorder = -100,\n",
    "                    )\n",
    "\n",
    "            for fn in [ ax.set_xticks, ax.set_yticks ]:\n",
    "                fn( np.arange( -1, 1.01, 0.25 ) )\n",
    "            ax.tick_params(\n",
    "                which = 'both',\n",
    "                right = True,\n",
    "                labelleft = subplotspec.is_first_col(),\n",
    "                labelbottom = subplotspec.is_last_row(),\n",
    "            )\n",
    "\n",
    "            ax.set_xlim( -0.3, 1.1 )\n",
    "            ax.set_ylim( -0.3, 1.1 )\n",
    "\n",
    "            if subplotspec.is_last_row():\n",
    "                ax.set_xlabel( x_label, fontsize=16 )\n",
    "            if subplotspec.is_first_col():\n",
    "                ax.set_ylabel( y_label, fontsize=16 )\n",
    "\n",
    "    # Add a legend\n",
    "    h, l = ax_dict['vlos'].get_legend_handles_labels()\n",
    "    ax_dict['legend'].legend( h, l, loc='lower center', prop={'size': 14}, )\n",
    "    ax_dict['legend'].axis( 'off' )\n",
    "\n",
    "    # Save\n",
    "    savedir = os.path.join( pm['data_dir'], 'figures' )\n",
    "    os.makedirs( savedir, exist_ok=True )\n",
    "    savefile = 'correlations_corner.png'\n",
    "    save_fp = os.path.join( savedir, savefile )\n",
    "    print( 'Saving figure to {}'.format( save_fp ) )\n",
    "    plt.savefig( save_fp, bbox_inches='tight' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748d6b97-c65a-4919-a82c-31a8c2dcb9d9",
   "metadata": {},
   "source": [
    "## Clean Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e016b1a2-fde5-406b-8aaa-e036eae29bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sls = len( sls )\n",
    "xs = np.linspace( -0.5, 0.5, n_sls ) / 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8cd0677-d7ea-43bd-996a-2f71812a5abb",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_mosaic = [\n",
    "    [ 'all', 'all', 'all', 'legend' ],\n",
    "    [ 'vlos', 'vlos', 'T', 'T', ],\n",
    "    [ 'nH', 'nH', 'Z', 'Z', ],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50461f2f-f186-452a-8a09-302cc6062d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "if calc_correlations:\n",
    "    # Setup Figure\n",
    "    n_rows_clean = len( clean_mosaic )\n",
    "    n_cols_clean = 2\n",
    "    fig = plt.figure( figsize=(n_rows_clean*panel_length, n_cols_clean*panel_length), facecolor='w' )\n",
    "    ax_dict = fig.subplot_mosaic(\n",
    "        clean_mosaic,\n",
    "        gridspec_kw = { 'wspace': 0.7 },\n",
    "    )\n",
    "\n",
    "    def r_scatter( ax, ys, c_key ):\n",
    "        c_params = correlation_coefficients[c_key]\n",
    "        facecolors = 'k'\n",
    "        if 'logscale' in c_params:\n",
    "            if c_params['logscale']:\n",
    "                facecolors = 'none'\n",
    "\n",
    "        scatter = ax.scatter(\n",
    "            xs,\n",
    "            ys,\n",
    "            label = c_key,\n",
    "            edgecolors = 'k',\n",
    "            facecolors = facecolors,\n",
    "            marker = correlation_markers[c_key],\n",
    "            s = correlation_sizes[c_key],\n",
    "            linewidth = 2,\n",
    "        )\n",
    "\n",
    "    # Overall\n",
    "    for c_key in correlations_plotted:\n",
    "        r_scatter(\n",
    "            ax_dict['all'],\n",
    "            correlations[c_key]['ndim'].array(),\n",
    "            c_key\n",
    "        )\n",
    "\n",
    "    # Each property\n",
    "    for j, x_key in enumerate( tqdm.tqdm( prop_keys, bar_format=bar_format ) ):\n",
    "\n",
    "        ax = ax_dict[x_key]\n",
    "\n",
    "        for c_key in correlations_plotted:\n",
    "            r_scatter(\n",
    "                ax,\n",
    "                correlations[c_key]['matrix'].array()[:,j,j],\n",
    "                c_key\n",
    "            )\n",
    "\n",
    "\n",
    "    # Add a legend\n",
    "    h, l = ax_dict['vlos'].get_legend_handles_labels()\n",
    "    ax_dict['legend'].legend( h, l, loc='lower left', prop={'size': 14}, )\n",
    "    ax_dict['legend'].axis( 'off' )\n",
    "    ax_dict['legend'].annotate(\n",
    "        text = r'$r = \\frac{ \\langle {\\rm actual } \\vert  {\\rm found } \\rangle }{ \\vert {\\rm actual} \\vert \\vert {\\rm found } \\vert }$',\n",
    "        xy = ( 0, 1 ),\n",
    "        xycoords = 'axes fraction',\n",
    "        xytext = ( 5, -5 ),\n",
    "        textcoords = 'offset points',\n",
    "        ha = 'center',\n",
    "        va = 'top',\n",
    "        fontsize = 18,\n",
    "    )\n",
    "\n",
    "    # Cleanup\n",
    "    for x_key, ax in ax_dict.items():\n",
    "\n",
    "        if x_key == 'legend':\n",
    "            continue\n",
    "\n",
    "        subplotspec = ax.get_subplotspec()\n",
    "\n",
    "        for value in [ -1, 0, 1 ]:\n",
    "            ax.axhline(\n",
    "                value,\n",
    "                color = pm['background_linecolor'],\n",
    "                linewidth = 1,\n",
    "                zorder = -100,\n",
    "            )\n",
    "\n",
    "        ax.set_ylabel( r_labels[x_key], fontsize=16 )\n",
    "        if subplotspec.is_last_row():\n",
    "            ax.set_xlabel( 'sightline ID', fontsize=16 )\n",
    "\n",
    "        ax.set_xticks( xs )\n",
    "        xtick_labels = [ _[-2:] for _ in correlations[c_key]['ndim'].keys_array() ]\n",
    "        ax.set_xticklabels( xtick_labels )\n",
    "\n",
    "        ax.set_ylim( -0.3, 1.1 )\n",
    "\n",
    "    # Save\n",
    "    savedir = os.path.join( pm['data_dir'], 'figures' )\n",
    "    os.makedirs( savedir, exist_ok=True )\n",
    "    savefile = 'correlations.pdf'\n",
    "    save_fp = os.path.join( savedir, savefile )\n",
    "    print( 'Saving figure to {}'.format( save_fp ) )\n",
    "    plt.savefig( save_fp, bbox_inches='tight' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6266e09-ed13-46af-9a1b-880c90c2cb31",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Ray-by-Ray Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98604e08-5caa-457e-97bf-76969f7e2246",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a256508-86d9-46fe-baff-b990b6e3c6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContourCalc( object ):\n",
    "    \n",
    "    def __init__( self, arr ):\n",
    "        \n",
    "        is_not_nan = np.invert( np.isnan( arr ) )\n",
    "        is_finite = np.invert( np.isinf( arr ) )\n",
    "        is_valid = is_not_nan & is_finite\n",
    "        self.values_sorted = np.sort( arr[is_valid] )[::-1]\n",
    "        \n",
    "        self.values_fraction = np.cumsum( self.values_sorted )\n",
    "        self.values_fraction /= self.values_fraction[-1]\n",
    "        \n",
    "        self.interp_fn = scipy.interpolate.interp1d( self.values_fraction, self.values_sorted )\n",
    "        \n",
    "    def get_level( self, q, f_min_is_average=True ):\n",
    "        \n",
    "        f = np.array( q ) / 100.\n",
    "        \n",
    "        if f_min_is_average:\n",
    "            f_min = 0.5 * ( self.values_fraction[0] + self.values_fraction[1] )\n",
    "        else:\n",
    "            f_min = self.values_fraction[0]\n",
    "        \n",
    "        if pd.api.types.is_list_like( f ):\n",
    "            f = np.array( f )\n",
    "            f[f<f_min] = f_min\n",
    "        else:\n",
    "            if f < f_min:\n",
    "                f = f_min\n",
    "\n",
    "        return self.interp_fn( f ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7262beef-4dd7-4c3f-9588-871db33d3512",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Corner Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fda7159-8a66-4cd4-8d5b-1f8427b62938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup save data\n",
    "figure_data_fp = os.path.join( pm['polished_data_dir'], 'absorption_system_properties.h5' )\n",
    "figure_data = verdict.Dict.from_hdf5( figure_data_fp, create_nonexistent=True )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fac918-84e7-4fac-b05f-f65cbbd7e01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, ray in enumerate( rays ):\n",
    "    \n",
    "    print( '\\nMaking comparison for ray {}\\n'.format( i ) )\n",
    "\n",
    "    ray_data = ray_datas[i]\n",
    "    weights = ray_weights[i]\n",
    "\n",
    "    # estimated sightline\n",
    "    sl_data = sl_datas[i]\n",
    "    sl_used = sl_useds[i]\n",
    "    sl_stacked = sl_stackeds[i]\n",
    "    model_weights = model_weightss[i]\n",
    "    stacked_weights = stacked_weightss[i]\n",
    "    total_weights = total_weightss[i]\n",
    "    median_NHI = median_NHIs[i]\n",
    "    \n",
    "    all_bins = all_binss[i]\n",
    "    all_dx = all_dxs[i]\n",
    "    all_centers = all_centerss[i]\n",
    "    \n",
    "    bins = all_bins['n_bins_1D']\n",
    "    dx = all_dx['n_bins_1D']\n",
    "    centers = all_centers['n_bins_1D']\n",
    "    \n",
    "    # Setup Figurea\n",
    "    n_cols = len( prop_keys )\n",
    "    fig = plt.figure( figsize=( panel_length*n_cols, panel_length*n_cols ), facecolor='w' )\n",
    "    ax_dict = fig.subplot_mosaic( mosaic )\n",
    "\n",
    "    # Loop through all properties\n",
    "    for j, x_key in enumerate( tqdm.tqdm( prop_keys, bar_format=bar_format ) ):\n",
    "        for k, y_key in enumerate( prop_keys ):\n",
    "\n",
    "            # Avoid duplicates\n",
    "            if k < j:\n",
    "                continue\n",
    "                \n",
    "            xy_key = '{}_{}'.format( x_key, y_key )\n",
    "\n",
    "            # Check for out-of-bounds\n",
    "            oob_labels = [ 'ray', 'estimated' ]\n",
    "            for ii, key in enumerate([ x_key, y_key ]):\n",
    "            \n",
    "                if key in sl_stacked:\n",
    "                    values_for_limits = [ ray_data[key], sl_stacked[key] ]\n",
    "                else:\n",
    "                    values_for_limits = [ ray_data[key], ]\n",
    "                    \n",
    "                weights_for_limits = [ weights, stacked_weights ]\n",
    "                \n",
    "                bins_for_limits = bins[key]\n",
    "                if helpers.logscale[key]:\n",
    "                    bins_for_limits = np.log10( bins_for_limits )\n",
    "                \n",
    "                for jj, values in enumerate( values_for_limits ):\n",
    "                    \n",
    "                    if helpers.logscale[key]:\n",
    "                        values = np.log10( values )\n",
    "                    else:\n",
    "                        values = values.value\n",
    "                    \n",
    "                    values_weighted = values * weights_for_limits[jj]\n",
    "                    \n",
    "                    is_low = values < bins_for_limits[0]\n",
    "                    sum_low = values_weighted[is_low].sum()\n",
    "                    is_high = values > bins_for_limits[-1]\n",
    "                    sum_high = values_weighted[is_high].sum()\n",
    "                    sum_total = values_weighted.sum()\n",
    "                    \n",
    "                    bounds = [ 'below', 'above' ]\n",
    "                    bools = [ is_low, is_high ]\n",
    "                    for kk, sum_oob in enumerate([ sum_low, sum_high ]):\n",
    "                        frac_oob = ( sum_oob / sum_total )\n",
    "                        if frac_oob > 0.05:\n",
    "                            warnings.warn(\n",
    "                                '{} {} points ({:.2g}% of weight) with {} {} {:.3g}'.format(\n",
    "                                    bools[kk].sum(),\n",
    "                                    oob_labels[jj],\n",
    "                                    frac_oob * 100,\n",
    "                                    key,\n",
    "                                    bounds[kk],\n",
    "                                    lims[key][kk],\n",
    "                                )\n",
    "                            )\n",
    "\n",
    "            # 1D histogram\n",
    "            if j == k:\n",
    "                ax = ax_dict[x_key]\n",
    "                subplotspec = ax.get_subplotspec()\n",
    "\n",
    "                x_label = labels[x_key]\n",
    "                y_label = labels_1D[x_key]\n",
    "                \n",
    "                bins = all_bins['n_bins_1D']\n",
    "                dx = all_dx['n_bins_1D']\n",
    "                centers = all_centers['n_bins_1D']\n",
    "\n",
    "                # Observational\n",
    "                if x_key in sl_stacked:\n",
    "                    \n",
    "                    if pm['1D_dist_estimation'] == 'histogram':\n",
    "                        hist_o, edges = np.histogram(\n",
    "                            sl_stacked[x_key],\n",
    "                            bins = bins[x_key],\n",
    "                            weights = stacked_weights,\n",
    "                            density = False,\n",
    "                        )\n",
    "                        hist_o /= hist_o.sum() * dx[x_key]\n",
    "                        ax.step(\n",
    "                            edges[:-1],\n",
    "                            hist_o,\n",
    "                            color = color_estimated,\n",
    "                            where = 'post',\n",
    "                            linewidth = 2,\n",
    "                        )\n",
    "                    elif pm['1D_dist_estimation'] == 'kde':\n",
    "                        # Change to logspace for kde\n",
    "                        if logscale[x_key]:\n",
    "                            sl_kde = np.log10( sl_stacked[x_key] )\n",
    "                            kde_centers = np.log10( centers[x_key] )\n",
    "                        else:\n",
    "                            sl_kde = sl_stacked[x_key].value\n",
    "                            kde_centers = centers[x_key].value\n",
    "                        kde_centers, hist_o = kale.density(\n",
    "                            sl_kde,\n",
    "                            points = kde_centers,\n",
    "                            weights = stacked_weights,\n",
    "                            probability = False,\n",
    "                        )\n",
    "                        hist_o /= hist_o.sum() * dx[x_key]\n",
    "                        hist_o *= median_NHI.sum()\n",
    "                        ax.plot(\n",
    "                            centers[x_key],\n",
    "                            hist_o,\n",
    "                            linewidth = 5,\n",
    "                            color = color_estimated,\n",
    "                            label = 'estimated',\n",
    "                        )\n",
    "\n",
    "\n",
    "                        # Individual components\n",
    "                        for kk, sl_used_x in enumerate( sl_used[x_key] ):       \n",
    "                            if logscale[x_key]:\n",
    "                                sl_used_x = np.log10( sl_used_x )\n",
    "                            else:\n",
    "                                sl_used_x = sl_used_x.value\n",
    "                                \n",
    "                            kde_centers, hist_o_kk = kale.density(\n",
    "                                    sl_used_x,\n",
    "                                    points = kde_centers,\n",
    "                                    weights = model_weights[kk],\n",
    "                                    probability = False,\n",
    "                                )\n",
    "                            \n",
    "                            # Check for delta function, which kale can't handle\n",
    "                            if np.isclose( hist_o_kk.sum(), 0. ):\n",
    "                                value = np.mean( sl_used_x )\n",
    "                                hist_o_kk[np.argmin((kde_centers - value)**2.)] = model_weights[kk].sum()\n",
    "                            \n",
    "                            hist_o_kk /= hist_o_kk.sum() * dx[x_key]\n",
    "                            hist_o_kk *= median_NHI[kk]\n",
    "\n",
    "                            # Cycle through colors, skipping the color reserved for the ray data\n",
    "                            if kk == 0:\n",
    "                                cmap_kk = kk\n",
    "                            else:\n",
    "                                cmap_kk = kk + 1\n",
    "\n",
    "                            ax.plot(\n",
    "                                centers[x_key],\n",
    "                                hist_o_kk,\n",
    "                                linewidth = 2,\n",
    "                                color = cmap[cmap_kk],\n",
    "                                label = r'    component $\\log N_{\\rm H\\,I}=$' + '{:.3g}'.format( np.log10( median_NHI[kk] ) ),\n",
    "                            )\n",
    "\n",
    "                            # Store data\n",
    "                            figure_data.setitem( public_label, hist_o_kk, sls[i], 'estimated', '1D distributions', 'individual distributions', sl_compkeys[i][kk], x_key )\n",
    "\n",
    "                        # Store data\n",
    "                        figure_data.setitem( public_label, hist_o, sls[i], 'estimated', '1D distributions', 'combined distributions', x_key )\n",
    "                        figure_data.setitem( public_label, centers[x_key], sls[i], 'estimated', '1D distributions', 'centers', x_key )\n",
    "                        figure_data.setitem( public_label, bins[x_key], sls[i], 'estimated', '1D distributions', 'bins', x_key )\n",
    "\n",
    "                # Ray\n",
    "                bins = all_bins['n_bins_data_1D']\n",
    "                dx = all_dx['n_bins_data_1D']\n",
    "                centers = all_centers['n_bins_data_1D']\n",
    "                \n",
    "                if pm['1D_dist_estimation_data'] == 'histogram':\n",
    "                    hist_r, edges = np.histogram(\n",
    "                        ray_data[x_key],\n",
    "                        bins = bins[x_key],\n",
    "                        weights = weights,\n",
    "                        density = False,\n",
    "                    )\n",
    "                    hist_r /= hist_r.sum() * dx[x_key]\n",
    "                    hist_r *= ray_data['NHI'].sum()\n",
    "                    ax.fill_between(\n",
    "                        edges[:-1],\n",
    "                        hist_r,\n",
    "                        color = color_data,\n",
    "                        step = 'post',\n",
    "                        label = 'source',\n",
    "                    )\n",
    "                elif pm['1D_dist_estimation_data'] == 'kde':\n",
    "                    # Change to logspace for kde\n",
    "                    if logscale[x_key]:\n",
    "                        sl_kde = np.log10( ray_data[x_key] )\n",
    "                        kde_centers = np.log10( centers[x_key] )\n",
    "                    else:\n",
    "                        sl_kde = ray_data[x_key].value\n",
    "                        kde_centers = centers[x_key].value\n",
    "                    kde_centers, hist_r = kale.density(\n",
    "                            sl_kde,\n",
    "                            points = kde_centers,\n",
    "                            weights = weights,\n",
    "                            probability = False,\n",
    "                        )\n",
    "                    hist_r /= hist_r.sum() * dx[x_key]\n",
    "                    hist_r *= ray_data['NHI'].sum()\n",
    "                    ax.fill_between(\n",
    "                        centers[x_key],\n",
    "                        hist_r,\n",
    "                        color = color_data,\n",
    "                        step = 'mid',\n",
    "                    )\n",
    "\n",
    "#                 y_min = 10.**np.nanmin( [ np.nanmin( np.log10( hist_o[hist_o>0] ) ), np.nanmin( np.log10( hist_r[hist_r>0] ) ) ] )\n",
    "#                 y_min = lims_1D[x_key]\n",
    "#                 y_max = np.nanmax([ np.nanmax( hist_r ), np.nanmax( hist_o ) ])\n",
    "#                 ax.set_ylim( y_min, y_max * 1.05 )\n",
    "                ax.set_ylim( lims_1D[x_key] )\n",
    "                ax.set_xlim( bins[x_key][0], bins[x_key][-1] )\n",
    "\n",
    "                if logscale[x_key]:\n",
    "                    ax.set_xscale( 'log' )\n",
    "                ax.set_yscale( 'log' )\n",
    "                \n",
    "                if x_key in [ 'T', 'nH', 'Z' ]:\n",
    "                    ax.yaxis.set_label_position( 'right' )\n",
    "                    ax.set_ylabel( y_label, fontsize=16 )\n",
    "\n",
    "                ax.tick_params(\n",
    "                    which = 'both',\n",
    "                    right = True,\n",
    "                    labelright = True,\n",
    "                )\n",
    "                \n",
    "                # Save ray data\n",
    "                figure_data.setitem( public_label, hist_r, sls[i], 'source', '1D distributions', 'distributions', x_key )\n",
    "                figure_data.setitem( public_label, centers[x_key], sls[i], 'source', '1D distributions', 'centers', x_key )\n",
    "                figure_data.setitem( public_label, bins[x_key], sls[i], 'source', '1D distributions', 'bins', x_key )\n",
    "\n",
    "            # 2D histogram\n",
    "            else:\n",
    "                \n",
    "                bins = all_bins['n_bins_2D']\n",
    "                dx = all_dx['n_bins_2D']\n",
    "                centers = all_centers['n_bins_2D']\n",
    "                \n",
    "                centers_x = copy.copy( centers[x_key] )\n",
    "                centers_y = copy.copy( centers[y_key] )\n",
    "                \n",
    "                try:\n",
    "                    ax = ax_dict['{}_{}'.format( x_key, y_key )]\n",
    "                except KeyError:\n",
    "                    ax = ax_dict['{}_{}'.format( y_key, x_key )]\n",
    "                subplotspec = ax.get_subplotspec()\n",
    "                              \n",
    "                # Upsample centers\n",
    "                upsample = pm['upsample_2D_dist']\n",
    "                if upsample is not None:\n",
    "                    centers_x = scipy.ndimage.zoom( centers_x, upsample )\n",
    "                    centers_y = scipy.ndimage.zoom( centers_y, upsample )\n",
    "                \n",
    "                # Observational per component\n",
    "                img_arr_comps = []\n",
    "                    \n",
    "                if ( x_key in sl_used ) and ( y_key in sl_used ):\n",
    "                    for kk, sl_used_x in enumerate( sl_used[x_key] ):\n",
    "                        sl_used_y = sl_used[y_key][kk]\n",
    "                        norm_kk = total_weights[kk] * dx[x_key] * dx[y_key]\n",
    "\n",
    "                        # Histogram version\n",
    "                        if pm['2D_dist_estimation'] == 'histogram':\n",
    "                            hist2d_kk, x_edges, y_edges = np.histogram2d(\n",
    "                                sl_used_x,\n",
    "                                sl_used_y,\n",
    "                                bins = [ bins[x_key], bins[y_key], ],\n",
    "                                weights = model_weights[kk] / norm_kk,\n",
    "                            )\n",
    "                            img_arr_kk = np.transpose( hist2d_kk )\n",
    "\n",
    "                        # KDE version\n",
    "                        elif pm['2D_dist_estimation'] == 'kde':\n",
    "\n",
    "                            # Change to logspace for kde\n",
    "                            if logscale[x_key]:\n",
    "                                sl_used_x = np.log10( sl_used_x )\n",
    "                                kde_centers_x = np.log10( centers_x )\n",
    "                            else:\n",
    "                                kde_centers_x = centers[x_key]\n",
    "                            if logscale[y_key]:\n",
    "                                sl_used_y = np.log10( sl_used_y )\n",
    "                                kde_centers_y = np.log10( centers_y )\n",
    "                            else:\n",
    "                                kde_centers_y = centers[y_key]\n",
    "\n",
    "                            kde_arr = np.array([ sl_used_x, sl_used_y ])\n",
    "                            points, img_arr_kk = kale.density(\n",
    "                                kde_arr,\n",
    "                                points = [ kde_centers_x, kde_centers_y ],\n",
    "                                grid = True,\n",
    "                                weights = model_weights[kk] / norm_kk,\n",
    "                            )\n",
    "\n",
    "                        # Upsample and smooth\n",
    "                        if upsample is not None:\n",
    "                            img_arr_kk = scipy.ndimage.zoom( img_arr_kk, upsample )                                                                                                \n",
    "\n",
    "                        if pm['smooth_2D_dist'] is not None:\n",
    "                            if upsample is not None:                                                   \n",
    "                                sigma = upsample * pm['smooth_2D_dist']\n",
    "                            else:\n",
    "                                sigma = pm['smooth_2D_dist']\n",
    "                            img_arr_kk = scipy.ndimage.gaussian_filter( img_arr_kk, sigma )\n",
    "\n",
    "                        # Get levels corresponding to percentages enclose\n",
    "                        c_calc_kk = ContourCalc( img_arr_kk )\n",
    "                        levels = c_calc_kk.get_level( contour_levels )\n",
    "\n",
    "                        # Cycle through colors, skipping the color reserved for the ray data\n",
    "                        if kk == 0:\n",
    "                            cmap_kk = kk\n",
    "                        else:\n",
    "                            cmap_kk = kk + 1\n",
    "                        contour_colors = [ cmap[cmap_kk], ] * len( levels )\n",
    "\n",
    "                        # Prevent invisible low-contribution components\n",
    "                        alpha_min = 0.5\n",
    "                        contour_alpha = ( total_weights[kk] / total_weights.max() ) * ( 1. - alpha_min ) + alpha_min\n",
    "\n",
    "                        ax.contour(\n",
    "                            centers_x,\n",
    "                            centers_y,\n",
    "                            img_arr_kk,\n",
    "                            levels,\n",
    "                            colors = contour_colors,\n",
    "                            linewidths = contour_linewidths,\n",
    "                            alpha = contour_alpha\n",
    "                        )\n",
    "\n",
    "                        # Store data\n",
    "                        figure_data.setitem( public_label, img_arr_kk, sls[i], 'estimated', '2D distributions', 'component distributions', sl_compkeys[i][kk], xy_key  )\n",
    "                        figure_data.setitem( public_label, levels, sls[i], 'estimated', '2D distributions', 'levels', xy_key )\n",
    "                        figure_data.setitem( public_label, contour_alpha, sls[i], 'estimated', '2D distributions', 'alpha', xy_key )\n",
    "                    figure_data.setitem( public_label, centers_x, sls[i], 'estimated', '2D distributions', 'x centers', x_key )\n",
    "                    figure_data.setitem( public_label, centers_y, sls[i], 'estimated', '2D distributions', 'y centers', y_key )\n",
    "                    figure_data.setitem( public_label, bins[x_key], sls[i], 'estimated', '2D distributions', 'x bins', x_key )\n",
    "                    figure_data.setitem( public_label, bins[y_key], sls[i], 'estimated', '2D distributions', 'y bins', y_key )\n",
    "                    \n",
    "                # Source data\n",
    "                bins = all_bins['n_bins_data_2D']\n",
    "                dx = all_dx['n_bins_data_2D']\n",
    "                centers = all_centers['n_bins_data_2D']\n",
    "                centers_x = centers[x_key]\n",
    "                centers_y = centers[y_key]\n",
    "                \n",
    "                hist2d_r, x_edges, y_edges = np.histogram2d(\n",
    "                    ray_data[x_key],\n",
    "                    ray_data[y_key],\n",
    "                    bins = [ bins[x_key], bins[y_key], ],\n",
    "                    weights = weights,\n",
    "                )\n",
    "                hist2d_r /= dx[x_key] * dx[y_key]\n",
    "                img_arr_r = np.transpose( hist2d_r )\n",
    "                \n",
    "                if pm['2D_dist_data_display'] == 'histogram':\n",
    "                    ax.pcolormesh(\n",
    "                        centers_x,\n",
    "                        centers_y,\n",
    "                        img_arr_r,\n",
    "                        cmap = cmap_data,\n",
    "                        shading = 'nearest',\n",
    "                        norm = matplotlib.colors.LogNorm(),\n",
    "                    )\n",
    "                elif pm['2D_dist_data_display'] == 'contour':\n",
    "                    \n",
    "                    centers_x = copy.copy( centers_x )\n",
    "                    centers_y = copy.copy( centers_x )\n",
    "\n",
    "                    # Upsample and smooth\n",
    "                    if upsample is not None:\n",
    "                        img_arr_r = scipy.ndimage.zoom( img_arr_r, upsample )\n",
    "                        \n",
    "                        centers_x = scipy.ndimage.zoom( centers_x, upsample )\n",
    "                        centers_x = scipy.ndimage.zoom( centers_x, upsample )\n",
    "\n",
    "                    if pm['smooth_2D_dist'] is not None:\n",
    "                        if upsample is not None:                                                   \n",
    "                            sigma = upsample * pm['smooth_2D_dist']\n",
    "                        else:\n",
    "                            sigma = pm['smooth_2D_dist']\n",
    "                        img_arr_r = scipy.ndimage.filters.gaussian_filter( img_arr_r, sigma )\n",
    "\n",
    "                    # Get levels corresponding to percentages enclose\n",
    "                    c_calc_kk = ContourCalc( img_arr_r )\n",
    "                    levels = c_calc_kk.get_level( contour_levels + [ -1, ], False )\n",
    "                    \n",
    "                    ax.contourf(\n",
    "                        centers_x,\n",
    "                        centers_y,\n",
    "                        img_arr_r,\n",
    "                        levels,\n",
    "                        cmap = cmap_data,\n",
    "                        zorder = -100,\n",
    "                    )\n",
    "                    \n",
    "                figure_data.setitem( public_label, img_arr_r, sls[i], 'source', '2D distributions', 'distribution', xy_key  )\n",
    "                figure_data.setitem( public_label, levels, sls[i], 'source', '2D distributions', 'levels', xy_key )\n",
    "                figure_data.setitem( public_label, centers_x, sls[i], 'source', '2D distributions', 'x centers', x_key )\n",
    "                figure_data.setitem( public_label, centers_y, sls[i], 'source', '2D distributions', 'y centers', y_key )\n",
    "                figure_data.setitem( public_label, bins[x_key], sls[i], 'source', '2D distributions', 'x bins', x_key )\n",
    "                figure_data.setitem( public_label, bins[y_key], sls[i], 'source', '2D distributions', 'y bins', y_key )\n",
    "\n",
    "                ax.set_xlim( bins[x_key][0], bins[x_key][-1] )\n",
    "                ax.set_ylim( bins[y_key][0], bins[y_key][-1] )\n",
    "\n",
    "                if logscale[x_key]:\n",
    "                    ax.set_xscale( 'log' )\n",
    "                if logscale[y_key]:\n",
    "                    ax.set_yscale( 'log' )\n",
    "\n",
    "                x_label = labels[x_key]\n",
    "                y_label = labels[y_key]\n",
    "\n",
    "            if subplotspec.is_last_row():\n",
    "                ax.set_xlabel( x_label, fontsize=16 )\n",
    "            if subplotspec.is_first_col():\n",
    "                ax.set_ylabel( y_label, fontsize=16 )\n",
    "                \n",
    "            # Correlation coefficient annotation\n",
    "            if calc_correlations:\n",
    "                coeff = correlations['log']['matrix'][sls[i]][j,k]\n",
    "                text = r'$r =$' + '{:.2f}'.format( coeff ) \n",
    "                annot = ax.annotate(\n",
    "                    text = text,\n",
    "                    xy = ( 1, 1 ),\n",
    "                    xycoords = 'axes fraction',\n",
    "                    xytext = ( -5, -5 ),\n",
    "                    textcoords = 'offset points',\n",
    "                    va = 'top',\n",
    "                    ha = 'right',\n",
    "                    fontsize = 14,\n",
    "                    color = 'w',\n",
    "                )\n",
    "                coeff_color = corr_cmap( corr_norm( coeff ) )\n",
    "                annot.set_path_effects([\n",
    "                    path_effects.Stroke( linewidth=3, foreground=coeff_color ),\n",
    "                    path_effects.Normal()\n",
    "                ])\n",
    "                \n",
    "    # Add a legend\n",
    "    h, l = ax_dict['vlos'].get_legend_handles_labels()\n",
    "    ax_dict['legend'].legend( h, l, loc='lower left', prop={'size': 12}, )\n",
    "    ax_dict['legend'].axis( 'off' )\n",
    "    \n",
    "    # Ray number\n",
    "    annot = ax_dict['legend'].annotate(\n",
    "        text = 'sightline {}'.format( sls[i][-2:] ),\n",
    "        xy = ( 0.5, 1 ),\n",
    "        xycoords = 'axes fraction',\n",
    "        xytext = ( 5, -5 ),\n",
    "        textcoords = 'offset points',\n",
    "        va = 'top',\n",
    "        ha = 'center',\n",
    "        fontsize = 16,\n",
    "        color = 'k',\n",
    "    )\n",
    "\n",
    "    # Save\n",
    "    savedir = os.path.join( pm['data_dir'], 'figures' )\n",
    "    os.makedirs( savedir, exist_ok=True )\n",
    "    savefile = 'sightline_{}.png'.format( os.path.basename( sls[i] ) )\n",
    "    save_fp = os.path.join( savedir, savefile )\n",
    "    print( 'Saving figure to {}'.format( save_fp ) )\n",
    "    plt.savefig( save_fp, bbox_inches='tight', dpi=150 )\n",
    "    \n",
    "    if not pm['show_plots_in_nb']:\n",
    "        plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "924e9ea1-631f-49f8-bf0e-0fc50d94d6bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data\n",
    "if pm['global_variation'] == '':\n",
    "    figure_data.to_hdf5( figure_data_fp )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5215269b-bee0-4f31-8b56-b3235e641964",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
