{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "21c38063-dde6-421f-a5b0-8a1d3320f44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import tqdm\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3b816548-85d0-44e9-abf7-9be458930aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt\n",
    "import trident\n",
    "import unyt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0164758a-30ce-4190-9afe-911c96c2dd84",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kalepy as kale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6a5de870-5fd1-4465-b7dd-bd452f2b13cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7eadfbae-2549-4873-8e31-47c5ad3048a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "matplotlib.style.use( '/Users/zhafen/repos/clean-bold/clean-bold.mplstyle' )\n",
    "import palettable"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67801afa-d127-43dc-a211-164c72b318b0",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1ede9ac4-52f5-4e59-9eeb-b720128fc41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pm = {\n",
    "    # Analysis \n",
    "    'broaden_models': True,\n",
    "    '1D_dist_estimation': 'kde',\n",
    "    '1D_dist_estimation_data': 'histogram',\n",
    "    '2D_dist_estimation': 'histogram',\n",
    "    'n_bins_convolve': 16,\n",
    "    \n",
    "    # Plotting Choices\n",
    "    'smooth_2D_dist': 0.5,\n",
    "    'upsample_2D_dist': 3,\n",
    "    '2D_dist_data_display': 'histogram',\n",
    "    'n_bins_1D': 128,\n",
    "    'n_bins_data_1D': 128,\n",
    "    'n_bins_2D': 32,\n",
    "    'n_bins_data_2D': 20,\n",
    "    'n_sample_turb': 1000,\n",
    "    'contour_levels': [ 90, 50 ],\n",
    "    'contour_linewidths': [ 1, 3 ],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59d8a89b-eb8b-4082-a33e-f5ae75481b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parameters\n",
    "pm = trove.link_params_to_config(\n",
    "    '/Users/zhafen/repos/cgm_modeling_challenge/modeling_challenge.trove',\n",
    "    script_id = 'nb.2',\n",
    "    variation = 'sample2',\n",
    "    **pm\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7fd61cc8-4320-4690-9cf4-93e2d6989359",
   "metadata": {},
   "outputs": [],
   "source": [
    "redshift = pm['redshift']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cb4395b3-67c4-44ab-bbdb-771c8feb75dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = pm['data_dir']\n",
    "ray_dir = os.path.join( data_dir, 'rays' )\n",
    "results_dir = os.path.join( data_dir, 'modeling' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b062f9ae-b695-4534-896d-8081ccfa49ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = palettable.cartocolors.qualitative.Safe_10.mpl_colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51dbc319-9e87-4454-a9ea-e815bc152616",
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = {\n",
    "    'vlos': r'$v_{\\rm LOS}$ [km/s]',\n",
    "    'T': r'T [K]',\n",
    "    'nH': r'$n_{\\rm H}$ [cm$^{-3}$]',\n",
    "    'Z': r'$Z$ [$Z_{\\odot}$]',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "fc194a2e-881d-4afa-8f01-d5eea240f85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "lims = {\n",
    "    'vlos': [ -100, 100 ],\n",
    "    'T': [ 1e2, 2.5e6 ],\n",
    "    'nH': [ 1e-7, 100 ],\n",
    "    'Z': [ 1e-3, 30 ],\n",
    "}\n",
    "autolims = {\n",
    "    'vlos': False,\n",
    "    'T': False,\n",
    "    'nH': False,\n",
    "    'Z': False,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "02a2e0cd-10e8-4d93-a75c-d1dd55ee5b19",
   "metadata": {},
   "outputs": [],
   "source": [
    "lims_1D = {\n",
    "    'vlos': [ 1e8, 1e18 ],\n",
    "    'T': [ 1e12, 1e20 ],\n",
    "    'nH': [ 1e12, 1e20 ],\n",
    "    'Z': [ 1e12, 1e20 ],\n",
    "}\n",
    "labels_1D = {\n",
    "    'vlos': r'$\\frac{ d N_{\\rm H\\,I} }{d v_{\\rm LOS}}$',\n",
    "    'T': r'$\\frac{ d N_{\\rm H\\,I} }{d \\log T}$',\n",
    "    'nH': r'$\\frac{ d N_{\\rm H\\,I} }{d \\log n_{\\rm H}}$',\n",
    "    'Z': r'$\\frac{ d N_{\\rm H\\,I} }{d \\log Z}$',\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1293db0b-c166-453b-ac70-4b1b5e751125",
   "metadata": {},
   "outputs": [],
   "source": [
    "dvs = {\n",
    "    'vlos': 5.,\n",
    "    'T': 0.05,\n",
    "    'nH': 0.05,\n",
    "    'Z': 0.05,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a8e0429a-4359-43f1-835d-09c099a9543e",
   "metadata": {},
   "outputs": [],
   "source": [
    "logscale = {\n",
    "    'vlos': False,\n",
    "    'T': True,\n",
    "    'nH': True,\n",
    "    'Z': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b03ea2c2-ae09-44a6-8ea4-a74a1a8f278a",
   "metadata": {},
   "outputs": [],
   "source": [
    "bar_format = pm['bar_format']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6412343f-f4f3-4354-9f0b-bbaec6742fce",
   "metadata": {},
   "source": [
    "# Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b6689cb9-a83a-451a-a2f8-69904c7d45a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "read_sets: Using set file -- \n",
      "  /Users/zhafen/repos/linetools/linetools/lists/sets/llist_v1.3.ascii\n",
      "Loading abundances from Asplund2009\n",
      "Abundances are relative by number on a logarithmic scale with H=12\n"
     ]
    }
   ],
   "source": [
    "ldb = trident.LineDatabase(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "429e5062-1c75-4e21-9ddd-d669402c2960",
   "metadata": {},
   "outputs": [],
   "source": [
    "def zeff_to_vel( zeff ):\n",
    "    \n",
    "    ainv2 = ( ( 1. + zeff ) / ( 1. + redshift ) )**2.\n",
    "    \n",
    "    v_div_c = ( ainv2 - 1. ) / ( ainv2 + 1. )\n",
    "    return ( v_div_c * unyt.c ).to( 'km/s' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1600ccba-a75e-460b-ac46-21db0888e377",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71b1f640-983b-4c87-a1b3-713f1b1808ec",
   "metadata": {},
   "source": [
    "## Modeled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "91da438d-84f0-4942-b249-556506474395",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get sightline filepaths\n",
    "sl_fps = []\n",
    "sls = []\n",
    "for sl_fp in glob.glob( os.path.join( results_dir, '*' ) ):\n",
    "    sl_fps.append( sl_fp )\n",
    "    sls.append( os.path.split( sl_fp )[-1] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4db585e3-39d9-4557-b943-3ecf638052df",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_sample_turb = pm['n_sample_turb']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "417bb82e-449a-4faf-b67b-42a2e2de2ba2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "     100%|██████████| 2/2 [00:01<00:00,  1.22it/s]\n"
     ]
    }
   ],
   "source": [
    "sl_datas = []\n",
    "sl_useds = []\n",
    "sl_stackeds = []\n",
    "model_weightss = []\n",
    "stacked_weightss = []\n",
    "total_weightss = []\n",
    "median_NHIs = []\n",
    "for i, sl in enumerate( sls ):\n",
    "    \n",
    "    sl_fp = sl_fps[i]\n",
    "    \n",
    "    # Get text files\n",
    "    col_names = [ 'prob', 'likelihood', 'Z', 'nH', 'T', 'NHI', 'bturb', 'z', ]\n",
    "    col_units = [ 1., 1., unyt.Zsun, unyt.cm**-3, unyt.K, unyt.cm**-2, unyt.km / unyt.s, 1. ]\n",
    "    sl_data = {}\n",
    "    for component_fp in glob.glob( os.path.join( sl_fp, '*' ) ):\n",
    "        component_key = os.path.splitext( os.path.split( component_fp )[-1] )[0]\n",
    "        sl_data[component_key] = pd.read_csv( component_fp, sep=' ', names=col_names )\n",
    "        \n",
    "    # Add LOS velocity and reformat\n",
    "    for component_key, df in sl_data.items():\n",
    "\n",
    "        # Reformat\n",
    "        new_entry = {}\n",
    "        for name in col_names:\n",
    "            values = unyt.unyt_array( df[name].values )\n",
    "            if name in [ 'nH', 'T', 'Z', 'NHI', ]:\n",
    "                new_entry[name] = 10.**values\n",
    "            else:\n",
    "                new_entry[name] = values\n",
    "\n",
    "        # Add LOS velocity\n",
    "        new_entry['vlos'] = zeff_to_vel( df['z'].values )\n",
    "\n",
    "        # Setup units\n",
    "        for j, name in enumerate( col_names ):\n",
    "            new_entry[name] *= col_units[j]\n",
    "\n",
    "        sl_data[component_key] = new_entry\n",
    "    col_names.append( 'vlos' )\n",
    "        \n",
    "    # Turn samples into a list\n",
    "    keys = list( sl_data.keys() )\n",
    "    sl_formatted = {}\n",
    "    for name in col_names:\n",
    "        sl_formatted[name] = [ sl_data[_][name] for _ in keys ]\n",
    "\n",
    "    # Generate modeled sample to plot (\"generate\" because we're sampling the doppler broadening)\n",
    "    if pm['broaden_models']:\n",
    "        sl_tiled = {}\n",
    "        for name in col_names:\n",
    "            sl_tiled[name] = []\n",
    "\n",
    "        for j, vlos_j in enumerate( tqdm.tqdm( sl_formatted['vlos'], bar_format=bar_format ) ):\n",
    "            sample_dist = scipy.stats.norm( loc=vlos_j, scale=sl_formatted['bturb'][j]/np.sqrt( 2. ) )\n",
    "            sampled_values = sample_dist.rvs( ( n_sample_turb, vlos_j.size ) )\n",
    "\n",
    "            for name in col_names:\n",
    "                if name != 'vlos':\n",
    "                    arr_tiled = np.hstack( np.tile( sl_formatted[name][j], ( n_sample_turb, 1 ),  ) )\n",
    "                else:\n",
    "                    arr_tiled = np.hstack( sampled_values )\n",
    "\n",
    "                arr_tiled *= sl_formatted[name][j].units\n",
    "\n",
    "                sl_tiled[name].append( arr_tiled )\n",
    "\n",
    "        sl_used = sl_tiled\n",
    "    else:\n",
    "        sl_used = sl_formatted\n",
    "\n",
    "    # Minimum weighting is to make sure no component is overweighted due to number of samples\n",
    "    n_samples_max = np.max([ _.size for _ in sl_used['nH'] ])\n",
    "    if pm['weighting'] is None:\n",
    "        model_weights = [ np.full( _.size, n_samples_max / _.size ) for _ in sl_used['nH'] ]\n",
    "    else:\n",
    "        model_weights = [ np.full( _.size, np.nanmedian( _ ) * n_samples_max / _.size ) for _ in sl_used[pm['weighting']] ]\n",
    "    model_weightss.append( model_weights )\n",
    "    stacked_weightss.append( np.hstack( model_weights ) )\n",
    "    total_weightss.append( np.array([ _.sum() for _ in model_weights ]) )\n",
    "    median_NHIs.append( np.array([ np.nanmedian( _ ) for _ in sl_used['NHI'] ]) )\n",
    "\n",
    "    sl_stacked = {}\n",
    "    for key, item in sl_used.items():\n",
    "        sl_stacked[key] = np.hstack( item ) * item[0].units\n",
    "    \n",
    "    sl_datas.append( sl_data )\n",
    "    sl_useds.append( sl_used )\n",
    "    sl_stackeds.append( sl_stacked )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8094aca-220b-4b0d-8d46-c50d11157138",
   "metadata": {},
   "source": [
    "## Actual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "81ad3c8e-64e9-4a25-841f-55df6fa617ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "yt : [INFO     ] 2022-02-02 16:50:32,932 Parameters: current_time              = 0.0\n",
      "yt : [INFO     ] 2022-02-02 16:50:32,933 Parameters: domain_dimensions         = [1 1 1]\n",
      "yt : [INFO     ] 2022-02-02 16:50:32,933 Parameters: domain_left_edge          = [0. 0. 0.] kpc\n",
      "yt : [INFO     ] 2022-02-02 16:50:32,934 Parameters: domain_right_edge         = [95.7322 95.7322 95.7322] kpc\n",
      "yt : [INFO     ] 2022-02-02 16:50:32,935 Parameters: cosmological_simulation   = 0.0\n",
      "yt : [INFO     ] 2022-02-02 16:50:32,994 Parameters: current_time              = 0.0\n",
      "yt : [INFO     ] 2022-02-02 16:50:32,994 Parameters: domain_dimensions         = [1 1 1]\n",
      "yt : [INFO     ] 2022-02-02 16:50:32,995 Parameters: domain_left_edge          = [0. 0. 0.] kpc\n",
      "yt : [INFO     ] 2022-02-02 16:50:32,996 Parameters: domain_right_edge         = [95.7322 95.7322 95.7322] kpc\n",
      "yt : [INFO     ] 2022-02-02 16:50:32,997 Parameters: cosmological_simulation   = 0.0\n",
      "yt : [INFO     ] 2022-02-02 16:50:33,055 Parameters: current_time              = 0.0\n",
      "yt : [INFO     ] 2022-02-02 16:50:33,056 Parameters: domain_dimensions         = [1 1 1]\n",
      "yt : [INFO     ] 2022-02-02 16:50:33,056 Parameters: domain_left_edge          = [0. 0. 0.] kpc\n",
      "yt : [INFO     ] 2022-02-02 16:50:33,057 Parameters: domain_right_edge         = [95.7322 95.7322 95.7322] kpc\n",
      "yt : [INFO     ] 2022-02-02 16:50:33,058 Parameters: cosmological_simulation   = 0.0\n",
      "yt : [INFO     ] 2022-02-02 16:50:33,114 Parameters: current_time              = 0.0\n",
      "yt : [INFO     ] 2022-02-02 16:50:33,115 Parameters: domain_dimensions         = [1 1 1]\n",
      "yt : [INFO     ] 2022-02-02 16:50:33,115 Parameters: domain_left_edge          = [0. 0. 0.] kpc\n",
      "yt : [INFO     ] 2022-02-02 16:50:33,116 Parameters: domain_right_edge         = [95.7322 95.7322 95.7322] kpc\n",
      "yt : [INFO     ] 2022-02-02 16:50:33,116 Parameters: cosmological_simulation   = 0.0\n",
      "yt : [INFO     ] 2022-02-02 16:50:33,176 Parameters: current_time              = 0.0\n",
      "yt : [INFO     ] 2022-02-02 16:50:33,177 Parameters: domain_dimensions         = [1 1 1]\n",
      "yt : [INFO     ] 2022-02-02 16:50:33,177 Parameters: domain_left_edge          = [0. 0. 0.] kpc\n",
      "yt : [INFO     ] 2022-02-02 16:50:33,178 Parameters: domain_right_edge         = [95.7322 95.7322 95.7322] kpc\n",
      "yt : [INFO     ] 2022-02-02 16:50:33,179 Parameters: cosmological_simulation   = 0.0\n",
      "yt : [INFO     ] 2022-02-02 16:50:33,238 Parameters: current_time              = 0.0\n",
      "yt : [INFO     ] 2022-02-02 16:50:33,238 Parameters: domain_dimensions         = [1 1 1]\n",
      "yt : [INFO     ] 2022-02-02 16:50:33,239 Parameters: domain_left_edge          = [0. 0. 0.] kpc\n",
      "yt : [INFO     ] 2022-02-02 16:50:33,240 Parameters: domain_right_edge         = [95.7322 95.7322 95.7322] kpc\n",
      "yt : [INFO     ] 2022-02-02 16:50:33,240 Parameters: cosmological_simulation   = 0.0\n",
      "yt : [INFO     ] 2022-02-02 16:50:33,298 Parameters: current_time              = 0.0\n",
      "yt : [INFO     ] 2022-02-02 16:50:33,299 Parameters: domain_dimensions         = [1 1 1]\n",
      "yt : [INFO     ] 2022-02-02 16:50:33,299 Parameters: domain_left_edge          = [0. 0. 0.] kpc\n",
      "yt : [INFO     ] 2022-02-02 16:50:33,300 Parameters: domain_right_edge         = [95.7322 95.7322 95.7322] kpc\n",
      "yt : [INFO     ] 2022-02-02 16:50:33,300 Parameters: cosmological_simulation   = 0.0\n",
      "yt : [INFO     ] 2022-02-02 16:50:33,357 Parameters: current_time              = 0.0\n",
      "yt : [INFO     ] 2022-02-02 16:50:33,358 Parameters: domain_dimensions         = [1 1 1]\n",
      "yt : [INFO     ] 2022-02-02 16:50:33,358 Parameters: domain_left_edge          = [0. 0. 0.] kpc\n",
      "yt : [INFO     ] 2022-02-02 16:50:33,359 Parameters: domain_right_edge         = [95.7322 95.7322 95.7322] kpc\n",
      "yt : [INFO     ] 2022-02-02 16:50:33,360 Parameters: cosmological_simulation   = 0.0\n",
      "yt : [INFO     ] 2022-02-02 16:50:33,418 Parameters: current_time              = 0.0\n",
      "yt : [INFO     ] 2022-02-02 16:50:33,419 Parameters: domain_dimensions         = [1 1 1]\n",
      "yt : [INFO     ] 2022-02-02 16:50:33,419 Parameters: domain_left_edge          = [0. 0. 0.] kpc\n",
      "yt : [INFO     ] 2022-02-02 16:50:33,420 Parameters: domain_right_edge         = [95.7322 95.7322 95.7322] kpc\n",
      "yt : [INFO     ] 2022-02-02 16:50:33,420 Parameters: cosmological_simulation   = 0.0\n",
      "yt : [INFO     ] 2022-02-02 16:50:33,479 Parameters: current_time              = 0.0\n",
      "yt : [INFO     ] 2022-02-02 16:50:33,480 Parameters: domain_dimensions         = [1 1 1]\n",
      "yt : [INFO     ] 2022-02-02 16:50:33,480 Parameters: domain_left_edge          = [0. 0. 0.] kpc\n",
      "yt : [INFO     ] 2022-02-02 16:50:33,481 Parameters: domain_right_edge         = [95.7322 95.7322 95.7322] kpc\n",
      "yt : [INFO     ] 2022-02-02 16:50:33,482 Parameters: cosmological_simulation   = 0.0\n"
     ]
    }
   ],
   "source": [
    "# Get rays\n",
    "ray_fps = [ os.path.join( ray_dir, 'ray_{}.h5'.format( _[1:] ) ) for _ in sls ]\n",
    "rays = [ yt.load( _ ) for _ in ray_fps ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ffa36c63-10d6-40ac-a680-02a24a6f637c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "yt : [INFO     ] 2022-02-02 16:50:33,500 Allocating for 1.024e+03 particles\n"
     ]
    }
   ],
   "source": [
    "ray_datas = []\n",
    "ray_weights = []\n",
    "for ray in rays:\n",
    "    # Ray properties\n",
    "    trident.add_ion_fields(ray, ions=[ 'H I', ], line_database=ldb)\n",
    "    den = ray.r[('gas', 'number_density')] * 0.75\n",
    "    ray_data = {\n",
    "        'nH': den,\n",
    "        'NH': ( den * ray.r[('gas', 'dl')] ),\n",
    "        'NHI': ray.r[('gas', 'H_p0_number_density')] * ray.r[('gas', 'dl')],\n",
    "        'z': ray.r[('gas', 'redshift_eff')],\n",
    "        'T': ray.r[('gas', 'temperature')],\n",
    "        'Z': ray.r[('gas', 'metallicity')],\n",
    "    }\n",
    "    ray_data['vlos'] = zeff_to_vel( ray_data['z'] )\n",
    "    \n",
    "    if pm['sim_weighting'] is None:\n",
    "        weights = np.ones( ray_data['NH'].shape )\n",
    "    else:\n",
    "        weights = copy.copy( ray_data[pm['sim_weighting']].value )\n",
    "        weights[np.isclose(weights,0.)] = np.nan\n",
    "    \n",
    "    ray_datas.append( ray_data )\n",
    "    ray_weights.append( weights )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73772698-1f28-4f08-b14f-8cd87622c93b",
   "metadata": {},
   "source": [
    "## Bins and Limits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "585f891c-b83f-40c1-81a1-4312c90a17b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      10%|█         | 1/10 [00:00<00:00, 261.57it/s]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/8x/jtlm_9mn1gd4ptwqw1yvxfrm0000gn/T/ipykernel_36095/2049316944.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mray_data\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mray_datas\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbar_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbar_format\u001b[0m \u001b[0;34m)\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0msl_stacked\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msl_stackeds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m# Make bins, dx, and centers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "all_binss = []\n",
    "all_dxs = []\n",
    "all_centerss = []\n",
    "for i, ray_data in enumerate( tqdm.tqdm( ray_datas, bar_format=bar_format ) ):\n",
    "    \n",
    "    sl_stacked = sl_stackeds[i]\n",
    "\n",
    "    # Make bins, dx, and centers\n",
    "    all_bins = {}\n",
    "    all_dx = {}\n",
    "    all_centers = {}\n",
    "    for n_bins_key in [ 'n_bins_1D', 'n_bins_2D', 'n_bins_data_1D', 'n_bins_data_2D', 'n_bins_convolve' ]:\n",
    "        n_bins = pm[n_bins_key]\n",
    "        bins = {}\n",
    "        for key, item in lims.items():\n",
    "            if autolims[key]:\n",
    "                low = np.nanmin(np.hstack([ sl_stacked[key], ray_data[key] ]))\n",
    "                high = np.nanmax(np.hstack([ sl_stacked[key], ray_data[key] ]))\n",
    "            else:\n",
    "                low = item[0]\n",
    "                high = item[1]\n",
    "            if logscale[key]:\n",
    "                bins[key] = np.logspace( np.log10( low ), np.log10( high ), n_bins )\n",
    "            else:\n",
    "                bins[key] = np.linspace( low, high, n_bins )\n",
    "\n",
    "            bins[key] *= sl_stacked[key].units\n",
    "        all_bins[n_bins_key] = bins\n",
    "\n",
    "        dx = {}\n",
    "        for key, bins_j in bins.items():\n",
    "            if logscale[key]:\n",
    "                dx[key] = np.log10( bins_j[1] ) - np.log10( bins_j[0] )\n",
    "            else:\n",
    "                dx[key] = float( ( bins_j[1] - bins_j[0] ).value )\n",
    "        all_dx[n_bins_key] = dx\n",
    "\n",
    "        centers = {}\n",
    "        for key, bins_j in bins.items():\n",
    "\n",
    "            if logscale[key]:\n",
    "                bins_j = np.log10( bins_j )\n",
    "\n",
    "            centers[key] = bins_j[:-1] + 0.5 * np.diff( bins_j )\n",
    "\n",
    "            if logscale[key]:\n",
    "                centers[key] = 10.**centers[key]\n",
    "        all_centers[n_bins_key] = centers\n",
    "        \n",
    "    all_binss.append( all_bins )\n",
    "    all_dxs.append( all_dx )\n",
    "    all_centerss.append( all_centers )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83dc4f11-67d6-4347-ad74-e225d46523fc",
   "metadata": {},
   "source": [
    "# Comparison Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d2a9974-68a0-4610-b33a-c604650048f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45c939ce-20ed-4cdd-a9c8-9e478d997957",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray_data = ray_datas[i]\n",
    "weights = ray_weights[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eba4cc9-7115-431d-8c3c-d3c53b44be2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "sl_stacked = sl_stackeds[i]\n",
    "model_weights = model_weightss[i]\n",
    "stacked_weights = stacked_weightss[i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9f98cc-7bc4-491c-9379-cabcab712ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "prop_keys = list( labels.keys() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "373d70df-3e96-4f0f-b2d5-cc04de9bbb35",
   "metadata": {},
   "outputs": [],
   "source": [
    "bins = all_binss[i]['n_bins_convolve']\n",
    "dx = all_dxs[i]['n_bins_convolve']\n",
    "centers = all_centerss[i]['n_bins_convolve']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ef03ce9-c85c-4c59-866b-98340cc3924f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Format for calculating distributions\n",
    "ray_data_histdd = []\n",
    "sl_stacked_histdd = []\n",
    "bins_histdd = []\n",
    "for key in prop_keys:\n",
    "    \n",
    "    arr_modeled = sl_stacked[key].value\n",
    "    arr = ray_data[key].value\n",
    "    bins_key = bins[key]\n",
    "    \n",
    "    if logscale[key]:\n",
    "        arr_modeled = np.log10( arr_modeled )\n",
    "        arr = np.log10( arr )\n",
    "        bins_key = np.log10( bins_key )\n",
    "    \n",
    "    sl_stacked_histdd.append( arr_modeled )\n",
    "    ray_data_histdd.append( arr )\n",
    "    bins_histdd.append( bins_key )\n",
    "    \n",
    "sl_stacked_histdd = np.array( sl_stacked_histdd ).transpose()\n",
    "ray_data_histdd = np.array( ray_data_histdd ).transpose()\n",
    "bins_histdd = np.array( bins_histdd )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b156f83b-dca3-4aa8-a2d7-6b8e1b5751ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "ray_dist_dd, bins_dd = np.histogramdd(\n",
    "    ray_data_histdd,\n",
    "    bins = bins_histdd,\n",
    "    weights = weights,\n",
    ")\n",
    "ray_dist_dd /= ray_dist_dd.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2664b300-f60b-46fa-b220-d9d3c4d61d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "modeled_dist_dd, bins_dd = np.histogramdd(\n",
    "    sl_stacked_histdd,\n",
    "    bins = bins_histdd,\n",
    "    weights = stacked_weights,\n",
    ")\n",
    "modeled_dist_dd /= modeled_dist_dd.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9361e634-a868-4d07-9ae3-dd2750be7104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through all properties\n",
    "for j, x_key in enumerate( tqdm.tqdm( prop_keys, bar_format=bar_format ) ):\n",
    "    for k, y_key in enumerate( prop_keys ):\n",
    "\n",
    "        # Avoid duplicates\n",
    "        if k < j:\n",
    "            continue\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5371e0-17b6-4cfa-a582-90e499cfa66f",
   "metadata": {},
   "outputs": [],
   "source": [
    "( modeled_dist_dd * ray_dist_dd ).sum() / ( ray_dist_dd**2. ).sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6266e09-ed13-46af-9a1b-880c90c2cb31",
   "metadata": {},
   "source": [
    "# Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98604e08-5caa-457e-97bf-76969f7e2246",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1df7212-1baf-4a65-9dac-72ee8ac7d760",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73914015-37fd-4640-ace6-cb8804759dd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "contour_levels = pm['contour_levels']\n",
    "contour_linewidths = pm['contour_linewidths']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1fdbc53-bfbe-4433-968f-56b5e859a4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "mosaic = [\n",
    "    [ 'vlos', 'legend', '.', '.' ],\n",
    "    [ 'T_vlos', 'T', '.', '.' ],\n",
    "    [ 'nH_vlos', 'nH_T', 'nH', '.' ],\n",
    "    [ 'Z_vlos', 'Z_T', 'Z_nH', 'Z', ],\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8fd8049-150e-4b53-a309-1d4ad2c43029",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_color_linear_cmap( color, name, f_white=0.95, f_saturated=1.0, ):\n",
    "    '''A function that turns a single color into linear colormap that\n",
    "    goes from a color that is whiter than the original color to a color\n",
    "    that is more saturated than the original color.\n",
    "    '''\n",
    "    \n",
    "    color_hsv = matplotlib.colors.rgb_to_hsv( color )\n",
    "    start_color_hsv = copy.copy( color_hsv )\n",
    "    \n",
    "    start_color_hsv = copy.copy( color_hsv )\n",
    "    start_color_hsv[1] -= f_white * start_color_hsv[1]\n",
    "    start_color_hsv[2] += f_white * ( 1. - start_color_hsv[2] )\n",
    "    start_color = matplotlib.colors.hsv_to_rgb( start_color_hsv )\n",
    "    \n",
    "    end_color_hsv = copy.copy( color_hsv )\n",
    "    end_color_hsv[1] += f_saturated * ( 1. - end_color_hsv[1] )\n",
    "    end_color = matplotlib.colors.hsv_to_rgb( end_color_hsv )\n",
    "    \n",
    "    return matplotlib.colors.LinearSegmentedColormap.from_list( name, [ start_color, end_color ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761f1f2e-0fa1-4586-9814-bc04435fb229",
   "metadata": {},
   "outputs": [],
   "source": [
    "color_modeled = cmap[0]\n",
    "color_data = cmap[1]\n",
    "cmap_modeled = one_color_linear_cmap( color_modeled, 'modeled' )\n",
    "cmap_data = one_color_linear_cmap( color_data, 'data' )\n",
    "cmap_data = matplotlib.colors.LinearSegmentedColormap.from_list( 'data', [ 'w', color_data ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93ba3ffa-8b81-4556-aaec-30986c4fa34e",
   "metadata": {},
   "outputs": [],
   "source": [
    "panel_length = 4."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a838c557-9ed1-4038-b44c-950a45d993a8",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Other Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a256508-86d9-46fe-baff-b990b6e3c6f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContourCalc( object ):\n",
    "    \n",
    "    def __init__( self, arr ):\n",
    "        \n",
    "        is_not_nan = np.invert( np.isnan( arr ) )\n",
    "        is_finite = np.invert( np.isinf( arr ) )\n",
    "        is_valid = is_not_nan & is_finite\n",
    "        self.values_sorted = np.sort( arr[is_valid] )[::-1]\n",
    "        \n",
    "        self.values_fraction = np.cumsum( self.values_sorted )\n",
    "        self.values_fraction /= self.values_fraction[-1]\n",
    "        \n",
    "        self.interp_fn = scipy.interpolate.interp1d( self.values_fraction, self.values_sorted )\n",
    "        \n",
    "    def get_level( self, q, f_min_is_average=True ):\n",
    "        \n",
    "        f = np.array( q ) / 100.\n",
    "        \n",
    "        if f_min_is_average:\n",
    "            f_min = 0.5 * ( self.values_fraction[0] + self.values_fraction[1] )\n",
    "        else:\n",
    "            f_min = self.values_fraction[0]\n",
    "        \n",
    "        if pd.api.types.is_list_like( f ):\n",
    "            f = np.array( f )\n",
    "            f[f<f_min] = f_min\n",
    "        else:\n",
    "            if f < f_min:\n",
    "                f = f_min\n",
    "\n",
    "        return self.interp_fn( f ) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7262beef-4dd7-4c3f-9588-871db33d3512",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52fac918-84e7-4fac-b05f-f65cbbd7e01e",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, ray in enumerate( rays ):\n",
    "    \n",
    "#     # DEBUG\n",
    "#     if i != 7:\n",
    "#         continue\n",
    "    \n",
    "    print( '\\nMaking comparison for ray {}\\n'.format( i ) )\n",
    "\n",
    "    ray_data = ray_datas[i]\n",
    "    weights = ray_weights[i]\n",
    "\n",
    "    # Modeled sightline\n",
    "    sl_data = sl_datas[i]\n",
    "    sl_used = sl_useds[i]\n",
    "    sl_stacked = sl_stackeds[i]\n",
    "    model_weights = model_weightss[i]\n",
    "    stacked_weights = stacked_weightss[i]\n",
    "    total_weights = total_weightss[i]\n",
    "    median_NHI = median_NHIs[i]\n",
    "    \n",
    "    all_bins = all_binss[i]\n",
    "    all_dx = all_dxs[i]\n",
    "    all_centers = all_centerss[i]\n",
    "    \n",
    "    bins = all_bins['n_bins_1D']\n",
    "    dx = all_dx['n_bins_1D']\n",
    "    centers = all_centers['n_bins_1D']\n",
    "    \n",
    "    # Setup Figure\n",
    "    n_cols = len( prop_keys )\n",
    "    fig = plt.figure( figsize=( panel_length*n_cols, panel_length*n_cols ), facecolor='w' )\n",
    "    ax_dict = fig.subplot_mosaic( mosaic )\n",
    "\n",
    "    # Loop through all properties\n",
    "    for j, x_key in enumerate( tqdm.tqdm( prop_keys, bar_format=bar_format ) ):\n",
    "        for k, y_key in enumerate( prop_keys ):\n",
    "\n",
    "            # Avoid duplicates\n",
    "            if k < j:\n",
    "                continue\n",
    "            \n",
    "#             # DEBUG\n",
    "#             if j != 1 and k != 0:\n",
    "#                 continue\n",
    "\n",
    "            # Check for out-of-bounds\n",
    "            oob_labels = [ 'modeled', 'ray' ]\n",
    "            for ii, key in enumerate([ x_key, y_key ]):\n",
    "                for jj, values in enumerate([ sl_stacked[key], ray_data[key] ]):\n",
    "                    n_low = ( values < bins[key][0] ).sum()\n",
    "                    n_high = ( values > bins[key][-1] ).sum()\n",
    "                    bounds = [ 'below', 'above' ]\n",
    "                    for kk, n_oob in enumerate([ n_low, n_high ]):\n",
    "                        if n_oob / values.size > 0.02:\n",
    "                            warnings.warn(\n",
    "                                '{} {} points ({:.2g}%) with {} {} {:.3g}'.format(\n",
    "                                    n_oob,\n",
    "                                    oob_labels[jj],\n",
    "                                    n_oob / values.size * 100,\n",
    "                                    key,\n",
    "                                    bounds[kk],\n",
    "                                    lims[key][kk],\n",
    "                                )\n",
    "                            )\n",
    "\n",
    "            # 1D histogram\n",
    "            if j == k:\n",
    "                ax = ax_dict[x_key]\n",
    "                subplotspec = ax.get_subplotspec()\n",
    "\n",
    "                x_label = labels[x_key]\n",
    "                y_label = labels_1D[x_key]\n",
    "                \n",
    "                bins = all_bins['n_bins_1D']\n",
    "                dx = all_dx['n_bins_1D']\n",
    "                centers = all_centers['n_bins_1D']\n",
    "\n",
    "                # Observational\n",
    "                if pm['1D_dist_estimation'] == 'histogram':\n",
    "                    hist_o, edges = np.histogram(\n",
    "                        sl_stacked[x_key],\n",
    "                        bins = bins[x_key],\n",
    "                        weights = stacked_weights,\n",
    "                        density = False,\n",
    "                    )\n",
    "                    hist_o /= hist_o.sum() * dx[x_key]\n",
    "                    ax.step(\n",
    "                        edges[:-1],\n",
    "                        hist_o,\n",
    "                        color = color_modeled,\n",
    "                        where = 'post',\n",
    "                        linewidth = 2,\n",
    "                    )\n",
    "                elif pm['1D_dist_estimation'] == 'kde':\n",
    "                    # Change to logspace for kde\n",
    "                    if logscale[x_key]:\n",
    "                        sl_kde = np.log10( sl_stacked[x_key] )\n",
    "                        kde_centers = np.log10( centers[x_key] )\n",
    "                    else:\n",
    "                        sl_kde = sl_stacked[x_key].value\n",
    "                        kde_centers = centers[x_key].value\n",
    "                    kde_centers, hist_o = kale.density(\n",
    "                            sl_kde,\n",
    "                            points = kde_centers,\n",
    "                            weights = stacked_weights,\n",
    "                            probability = False,\n",
    "                        )\n",
    "                    hist_o /= hist_o.sum() * dx[x_key]\n",
    "                    hist_o *= median_NHI.sum()\n",
    "                    ax.plot(\n",
    "                        centers[x_key],\n",
    "                        hist_o,\n",
    "                        linewidth = 5,\n",
    "                        color = 'k',\n",
    "                        label = 'modeled',\n",
    "                    )\n",
    "                    \n",
    "                    # Individual components\n",
    "                    for kk, sl_used_x in enumerate( sl_used[x_key] ):       \n",
    "                        if logscale[x_key]:\n",
    "                            sl_used_x = np.log10( sl_used_x )\n",
    "                        else:\n",
    "                            sl_used_x = sl_used_x.value\n",
    "                        kde_centers, hist_o = kale.density(\n",
    "                                sl_used_x,\n",
    "                                points = kde_centers,\n",
    "                                weights = model_weights[kk],\n",
    "                                probability = False,\n",
    "                            )\n",
    "                        hist_o /= hist_o.sum() * dx[x_key]\n",
    "                        hist_o *= median_NHI[kk]\n",
    "                        \n",
    "                        # Cycle through colors, skipping the color reserved for the ray data\n",
    "                        if kk == 0:\n",
    "                            cmap_kk = kk\n",
    "                        else:\n",
    "                            cmap_kk = kk + 1\n",
    "                    \n",
    "                        ax.plot(\n",
    "                            centers[x_key],\n",
    "                            hist_o,\n",
    "                            linewidth = 2,\n",
    "                            color = cmap[cmap_kk],\n",
    "                            label = r'    component $\\log N_{\\rm H\\,I}=$' + '{:.3g}'.format( np.log10( median_NHI[kk] ) ),\n",
    "                        )\n",
    "\n",
    "                # Ray\n",
    "                bins = all_bins['n_bins_data_1D']\n",
    "                dx = all_dx['n_bins_data_1D']\n",
    "                centers = all_centers['n_bins_data_1D']\n",
    "                \n",
    "                if pm['1D_dist_estimation_data'] == 'histogram':\n",
    "                    hist_r, edges = np.histogram(\n",
    "                        ray_data[x_key],\n",
    "                        bins = bins[x_key],\n",
    "                        weights = weights,\n",
    "                        density = False,\n",
    "                    )\n",
    "                    hist_r /= hist_r.sum() * dx[x_key]\n",
    "                    hist_r *= ray_data['NHI'].sum()\n",
    "                    ax.fill_between(\n",
    "                        edges[:-1],\n",
    "                        hist_r,\n",
    "                        color = color_data,\n",
    "                        step = 'post',\n",
    "                        label = 'data',\n",
    "                    )\n",
    "                elif pm['1D_dist_estimation_data'] == 'kde':\n",
    "                    # Change to logspace for kde\n",
    "                    if logscale[x_key]:\n",
    "                        sl_kde = np.log10( ray_data[x_key] )\n",
    "                        kde_centers = np.log10( centers[x_key] )\n",
    "                    else:\n",
    "                        sl_kde = ray_data[x_key].value\n",
    "                        kde_centers = centers[x_key].value\n",
    "                    kde_centers, hist_r = kale.density(\n",
    "                            sl_kde,\n",
    "                            points = kde_centers,\n",
    "                            weights = weights,\n",
    "                            probability = False,\n",
    "                        )\n",
    "                    hist_r /= hist_r.sum() * dx[x_key]\n",
    "                    hist_r *= ray_data['NHI'].sum()\n",
    "                    ax.fill_between(\n",
    "                        centers[x_key],\n",
    "                        hist_r,\n",
    "                        color = color_data,\n",
    "                    )\n",
    "\n",
    "#                 y_min = 10.**np.nanmin( [ np.nanmin( np.log10( hist_o[hist_o>0] ) ), np.nanmin( np.log10( hist_r[hist_r>0] ) ) ] )\n",
    "#                 y_min = lims_1D[x_key]\n",
    "#                 y_max = np.nanmax([ np.nanmax( hist_r ), np.nanmax( hist_o ) ])\n",
    "#                 ax.set_ylim( y_min, y_max * 1.05 )\n",
    "                ax.set_ylim( lims_1D[x_key] )\n",
    "                ax.set_xlim( bins[x_key][0], bins[x_key][-1] )\n",
    "\n",
    "                if logscale[x_key]:\n",
    "                    ax.set_xscale( 'log' )\n",
    "                ax.set_yscale( 'log' )\n",
    "                \n",
    "                if x_key in [ 'T', 'nH', 'Z' ]:\n",
    "                    ax.yaxis.set_label_position( 'right' )\n",
    "                    ax.set_ylabel( y_label, fontsize=16 )\n",
    "\n",
    "                ax.tick_params(\n",
    "                    which = 'both',\n",
    "                    labelleft = subplotspec.is_first_col(),\n",
    "                    right = True,\n",
    "                    labelright = True,\n",
    "                )\n",
    "\n",
    "            # 2D histogram\n",
    "            else:\n",
    "                \n",
    "                bins = all_bins['n_bins_2D']\n",
    "                dx = all_dx['n_bins_2D']\n",
    "                centers = all_centers['n_bins_2D']\n",
    "                \n",
    "                centers_x = copy.copy( centers[x_key] )\n",
    "                centers_y = copy.copy( centers[y_key] )\n",
    "                \n",
    "                try:\n",
    "                    ax = ax_dict['{}_{}'.format( x_key, y_key )]\n",
    "                except KeyError:\n",
    "                    ax = ax_dict['{}_{}'.format( y_key, x_key )]\n",
    "                subplotspec = ax.get_subplotspec()\n",
    "                              \n",
    "                # Upsample centers\n",
    "                upsample = pm['upsample_2D_dist']\n",
    "                if upsample is not None:\n",
    "                    centers_x = scipy.ndimage.zoom( centers_x, upsample )\n",
    "                    centers_y = scipy.ndimage.zoom( centers_y, upsample )\n",
    "                \n",
    "                # Observational per component\n",
    "                img_arr_comps = []\n",
    "                    \n",
    "                for kk, sl_used_x in enumerate( sl_used[x_key] ):\n",
    "                    sl_used_y = sl_used[y_key][kk]\n",
    "                    norm_kk = total_weights[kk] * dx[x_key] * dx[y_key]\n",
    "                    \n",
    "                    # Histogram version\n",
    "                    if pm['2D_dist_estimation'] == 'histogram':\n",
    "                        hist2d_kk, x_edges, y_edges = np.histogram2d(\n",
    "                            sl_used_x,\n",
    "                            sl_used_y,\n",
    "                            bins = [ bins[x_key], bins[y_key], ],\n",
    "                            weights = model_weights[kk] / norm_kk,\n",
    "                        )\n",
    "                        img_arr_kk = np.transpose( hist2d_kk )\n",
    "                        \n",
    "                    # KDE version\n",
    "                    elif pm['2D_dist_estimation'] == 'kde':\n",
    "                        \n",
    "                        # Change to logspace for kde\n",
    "                        if logscale[x_key]:\n",
    "                            sl_used_x = np.log10( sl_used_x )\n",
    "                            kde_centers_x = np.log10( centers_x )\n",
    "                        else:\n",
    "                            kde_centers_x = centers[x_key]\n",
    "                        if logscale[y_key]:\n",
    "                            sl_used_y = np.log10( sl_used_y )\n",
    "                            kde_centers_y = np.log10( centers_y )\n",
    "                        else:\n",
    "                            kde_centers_y = centers[y_key]\n",
    "                            \n",
    "                        kde_data = np.array([ sl_used_x, sl_used_y ])\n",
    "                        points, img_arr_kk = kale.density(\n",
    "                            kde_data,\n",
    "                            points = [ kde_centers_x, kde_centers_y ],\n",
    "                            grid = True,\n",
    "                            weights = model_weights[kk] / norm_kk,\n",
    "                        )\n",
    "                        \n",
    "                    # Upsample and smooth\n",
    "                    if upsample is not None:\n",
    "                        img_arr_kk = scipy.ndimage.zoom( img_arr_kk, upsample )                                                                                                \n",
    "                    \n",
    "                    if pm['smooth_2D_dist'] is not None:\n",
    "                        if upsample is not None:                                                   \n",
    "                            sigma = upsample * pm['smooth_2D_dist']\n",
    "                        else:\n",
    "                            sigma = pm['smooth_2D_dist']\n",
    "                        img_arr_kk = scipy.ndimage.filters.gaussian_filter( img_arr_kk, sigma )\n",
    "        \n",
    "                    # Get levels corresponding to percentages enclose\n",
    "                    c_calc_kk = ContourCalc( img_arr_kk )\n",
    "                    levels = c_calc_kk.get_level( contour_levels )\n",
    "                \n",
    "                    # Cycle through colors, skipping the color reserved for the ray data\n",
    "                    if kk == 0:\n",
    "                        cmap_kk = kk\n",
    "                    else:\n",
    "                        cmap_kk = kk + 1\n",
    "                    contour_colors = [ cmap[cmap_kk], ] * len( levels )\n",
    "                        \n",
    "                    # Prevent invisible low-contribution components\n",
    "                    alpha_min = 0.5\n",
    "                    contour_alpha = ( total_weights[kk] / total_weights.max() ) * ( 1. - alpha_min ) + alpha_min\n",
    "                    \n",
    "                    ax.contour(\n",
    "                        centers_x,\n",
    "                        centers_y,\n",
    "                        img_arr_kk,\n",
    "                        levels,\n",
    "                        colors = contour_colors,\n",
    "                        linewidths = contour_linewidths,\n",
    "                        alpha = contour_alpha\n",
    "                    )\n",
    "                    \n",
    "                # Ray\n",
    "                bins = all_bins['n_bins_data_2D']\n",
    "                dx = all_dx['n_bins_data_2D']\n",
    "                centers = all_centers['n_bins_data_2D']\n",
    "                \n",
    "                used_weights = weights / ( weights.sum() * dx[x_key] * dx[y_key] )\n",
    "                hist2d_r, x_edges, y_edges = np.histogram2d(\n",
    "                    ray_data[x_key],\n",
    "                    ray_data[y_key],\n",
    "                    bins = [ bins[x_key], bins[y_key], ],\n",
    "                    weights = used_weights,\n",
    "                )\n",
    "                img_arr_r = np.transpose( hist2d_r )\n",
    "                \n",
    "                if pm['2D_dist_data_display'] == 'histogram':\n",
    "                    ax.pcolormesh(\n",
    "                        centers[x_key],\n",
    "                        centers[y_key],\n",
    "                        img_arr_r,\n",
    "                        cmap = cmap_data,\n",
    "                        shading = 'nearest',\n",
    "                        norm = matplotlib.colors.LogNorm(),\n",
    "                    )\n",
    "                elif pm['2D_dist_data_display'] == 'contour':\n",
    "                    \n",
    "                    contour_centers_x = copy.copy( all_centers['n_bins_data_2D'][x_key] )\n",
    "                    contour_centers_y = copy.copy( all_centers['n_bins_data_2D'][y_key] )\n",
    "\n",
    "                    # Upsample and smooth\n",
    "                    if upsample is not None:\n",
    "                        img_arr_r = scipy.ndimage.zoom( img_arr_r, upsample )\n",
    "                        \n",
    "                        contour_centers_x = scipy.ndimage.zoom( contour_centers_x, upsample )\n",
    "                        contour_centers_y = scipy.ndimage.zoom( contour_centers_y, upsample )\n",
    "\n",
    "                    if pm['smooth_2D_dist'] is not None:\n",
    "                        if upsample is not None:                                                   \n",
    "                            sigma = upsample * pm['smooth_2D_dist']\n",
    "                        else:\n",
    "                            sigma = pm['smooth_2D_dist']\n",
    "                        img_arr_r = scipy.ndimage.filters.gaussian_filter( img_arr_r, sigma )\n",
    "\n",
    "                    # Get levels corresponding to percentages enclose\n",
    "                    c_calc_kk = ContourCalc( img_arr_r )\n",
    "                    levels = c_calc_kk.get_level( contour_levels + [ -1, ], False )\n",
    "                    \n",
    "                    ax.contourf(\n",
    "                        contour_centers_x,\n",
    "                        contour_centers_y,\n",
    "                        img_arr_r,\n",
    "                        levels,\n",
    "                        cmap = cmap_data,\n",
    "                        zorder = -100,\n",
    "                    )\n",
    "\n",
    "                ax.set_xlim( bins[x_key][0], bins[x_key][-1] )\n",
    "                ax.set_ylim( bins[y_key][0], bins[y_key][-1] )\n",
    "\n",
    "                if logscale[x_key]:\n",
    "                    ax.set_xscale( 'log' )\n",
    "                if logscale[y_key]:\n",
    "                    ax.set_yscale( 'log' )\n",
    "\n",
    "                x_label = labels[x_key]\n",
    "                y_label = labels[y_key]\n",
    "\n",
    "            if subplotspec.is_last_row():\n",
    "                ax.set_xlabel( x_label, fontsize=16 )\n",
    "            if subplotspec.is_first_col():\n",
    "                ax.set_ylabel( y_label, fontsize=16 )\n",
    "                \n",
    "    # Add a legend\n",
    "    h, l = ax_dict['vlos'].get_legend_handles_labels()\n",
    "    ax_dict['legend'].legend( h, l, loc='lower left' )\n",
    "    ax_dict['legend'].axis( 'off' )\n",
    "\n",
    "    # Save\n",
    "    savedir = './figures/sample2/comparison'\n",
    "    if pm['weighting'] == 'density':\n",
    "        savedir = os.path.join( savedir, 'density_weighting' )\n",
    "    os.makedirs( savedir, exist_ok=True )\n",
    "    savefile = 'sightline_{}.png'.format( os.path.basename( sl_fps[i] ) )\n",
    "    save_fp = os.path.join( savedir, savefile )\n",
    "    plt.savefig( save_fp, bbox_inches='tight' )\n",
    "    \n",
    "    plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1054f005-e621-441e-aed2-794298ae8428",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
