{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71dc1b6d-4adb-4157-87a3-03622632e927",
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import glob\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import scipy\n",
    "import scipy.stats\n",
    "import tqdm\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00fed0f2-7d47-4955-ac25-640d09f77aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yt\n",
    "import trident\n",
    "import unyt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53fc0084-d12d-45ad-a209-d9078dd8f196",
   "metadata": {},
   "outputs": [],
   "source": [
    "import kalepy as kale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f22441-2234-40ec-baae-ebe1edd862e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import trove\n",
    "import verdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ad1af51-24eb-47a0-aadf-175ecdcbd276",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "# # Currently need to call this to get matplotlib selected style to load...\n",
    "plt.plot()\n",
    "matplotlib.style.use( '/Users/zhafen/repos/clean-bold/clean-bold-mnras.mplstyle' )\n",
    "import palettable\n",
    "import matplotlib.patheffects as path_effects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bddb0fd2-1b28-4f48-ba5b-c8a24a134c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "import helpers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12f085e-92e7-4860-a9a8-7c038bef9947",
   "metadata": {},
   "source": [
    "# Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "deb637a3-81dd-4abb-a1ed-a13f18c8fa00",
   "metadata": {},
   "outputs": [],
   "source": [
    "variations = [ \n",
    "    'original',\n",
    "    'high-z',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ede9ac4-52f5-4e59-9eeb-b720128fc41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "params = {\n",
    "    # Analysis \n",
    "    'prop_keys': [ 'vlos', 'T', 'nH', 'Z' ],\n",
    "    'vel_prop_keys': [ 'vlos', 'T', 'nH', 'Z', 'NHI' ],\n",
    "    'broaden_models': True,\n",
    "    '1D_dist_estimation': 'kde',\n",
    "    '1D_dist_estimation_data': 'histogram',\n",
    "    '2D_dist_estimation': 'histogram',\n",
    "    'export_data_for_proposal': False,\n",
    "    'f_enclosed': [ 0.5, 0.68, 0.75, 0.9, 0.99, 0.997 ],\n",
    "    \n",
    "    # Plotting Choices\n",
    "    'smooth_2D_dist': 0.5,\n",
    "    'upsample_2D_dist': 3,\n",
    "    '2D_dist_data_display': 'histogram',\n",
    "    'contour_levels': [ 90, 50 ],\n",
    "    'contour_linewidths': [ 1, 3 ],\n",
    "    'show_plots_in_nb': False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f4aca5-0604-4f41-bf5f-4ec87fe98db7",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "696d5523-475b-4166-a97a-368fd2e4593c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6ecdfad-1324-4c0a-aa85-14c0827e7c62",
   "metadata": {},
   "outputs": [],
   "source": [
    "variation_plotting_params = {\n",
    "    'original': {\n",
    "        'color': helpers.blinded_color,\n",
    "        'label': 'estimated',\n",
    "        'offset': -0.2,\n",
    "    },\n",
    "    'high-z': {\n",
    "        'color': helpers.revised_color,\n",
    "        'label': 'revised',\n",
    "        'offset': 0.2,\n",
    "    },\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d7d18d5-7087-4197-a7ef-ffc8030c71e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "min_column = 1e13\n",
    "# min_column = 'auto_per_sl'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ebd7e37-7f32-46c9-8313-73d64f78ee2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlation_markers = {\n",
    "    'one-sided': '^',\n",
    "    'log one-sided': '^',\n",
    "    'two-sided': 'D',\n",
    "    'linear': 'o',\n",
    "    'log': 'o',\n",
    "}\n",
    "correlation_sizes = {\n",
    "    'one-sided': 100,\n",
    "    'log one-sided': 100,\n",
    "    'two-sided': 80,\n",
    "    'linear': 100,\n",
    "    'log': 100,\n",
    "}\n",
    "correlations_plotted = [ 'linear', 'log' ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e4170d-07c9-442f-902b-a3fe0de1c4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "mosaic = [\n",
    "    [ 'vlos', 'legend', '.', '.' ],\n",
    "    [ 'T_vlos', 'T', '.', '.' ],\n",
    "    [ 'nH_vlos', 'nH_T', 'nH', '.' ],\n",
    "    [ 'Z_vlos', 'Z_T', 'Z_nH', 'Z', ],\n",
    "]\n",
    "velocity_mosaic = [\n",
    "    [ 'nH_vlos', 'vlos', ],\n",
    "    [ 'Z_vlos', 'T_vlos', ],\n",
    "]\n",
    "mosaic_dist = [ [ 'vlos', 'Z' ], [ 'T', 'nH' ] ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5a7c229-5fe0-40cb-8760-bc52354aa57f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = palettable.cartocolors.qualitative.Safe_10.mpl_colors\n",
    "corr_cmap = palettable.cartocolors.diverging.Temps_2_r.mpl_colormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85890ae-1d72-4668-82e1-ca6459cf8967",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_norm = matplotlib.colors.Normalize( vmin=0, vmax=1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9be3186f-3044-4a63-ada9-0288422c3af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_color_linear_cmap( color, name, f_white=0.95, f_saturated=1.0, ):\n",
    "    '''A function that turns a single color into linear colormap that\n",
    "    goes from a color that is whiter than the original color to a color\n",
    "    that is more saturated than the original color.\n",
    "    '''\n",
    "    \n",
    "    color_hsv = matplotlib.colors.rgb_to_hsv( color )\n",
    "    start_color_hsv = copy.copy( color_hsv )\n",
    "    \n",
    "    start_color_hsv = copy.copy( color_hsv )\n",
    "    start_color_hsv[1] -= f_white * start_color_hsv[1]\n",
    "    start_color_hsv[2] += f_white * ( 1. - start_color_hsv[2] )\n",
    "    start_color = matplotlib.colors.hsv_to_rgb( start_color_hsv )\n",
    "    \n",
    "    end_color_hsv = copy.copy( color_hsv )\n",
    "    end_color_hsv[1] += f_saturated * ( 1. - end_color_hsv[1] )\n",
    "    end_color = matplotlib.colors.hsv_to_rgb( end_color_hsv )\n",
    "    \n",
    "    return matplotlib.colors.LinearSegmentedColormap.from_list( name, [ start_color, end_color ] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "384e4210-7fdf-4d0e-b3fb-2c4771a3f153",
   "metadata": {},
   "source": [
    "## Process analysis parameteres"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b938d2a-72bf-4f17-8534-3379712e2a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load parameters\n",
    "pms = {}\n",
    "for variation in variations:\n",
    "    pm = trove.link_params_to_config(\n",
    "        '/Users/zhafen/analysis/cgm_modeling_challenge/sample2.trove',\n",
    "        script_id = 'nb.2',\n",
    "        variation = variation,\n",
    "        global_variation = '',\n",
    "        **params\n",
    "    )\n",
    "    pms[variation] = pm\n",
    "pm = list( pms.values() )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d69f647-8db4-49a0-94d8-ff88d4855d86",
   "metadata": {},
   "outputs": [],
   "source": [
    "if min_column != 'auto_per_sl':\n",
    "    dNHI_dx_min = {}\n",
    "    for prop_key, lim in helpers.lims.items():\n",
    "        if helpers.logscale[prop_key]:\n",
    "            lim = np.log10( lim )\n",
    "        dNHI_dx_min[prop_key] = min_column / ( lim[1] - lim[0] )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fce6e87-f254-417a-9ace-01a3c5f5e79c",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8b356c5-80c2-40c8-b2ae-7cadf31283b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "correlations_fp = os.path.join( pm['polished_data_dir'], 'correlation_coefficients.h5' )\n",
    "correlations_all = verdict.Dict.from_hdf5( correlations_fp )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6f1a29-83b6-48cb-9732-8731c3533117",
   "metadata": {},
   "outputs": [],
   "source": [
    "absorption_properties_fp = os.path.join( pm['polished_data_dir'], 'absorption_system_properties.h5' )\n",
    "absorption_properties = verdict.Dict.from_hdf5( absorption_properties_fp, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e016b1a2-fde5-406b-8aaa-e036eae29bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "sls = list( correlations_all[pm['public_label']]['linear']['ndim'].keys() )\n",
    "n_sls = len( sls )\n",
    "xs = np.linspace( -0.5, 0.5, n_sls ) / 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a117ee2-40ec-43b3-8769-6dc3cd5b1014",
   "metadata": {},
   "source": [
    "# Images"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77b6f77-f1e0-4de6-a5a1-8d496cfcbf41",
   "metadata": {},
   "source": [
    "## Remap colormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "009c4bb7-cacc-4ecd-bfe1-6ffbe19c494e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# New, preferred colormap\n",
    "cmap_new = matplotlib.colormaps.get( 'magma' )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d211c4d-472e-42e2-ae03-c58792971212",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Colorbar location\n",
    "colorbar_loc = [[1027, 60], [1038, 485]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d14a9b4-d793-4131-8720-7fb1de1102a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup filepaths\n",
    "image_fps = glob.glob( os.path.join( pm['code_dir'], 'figures', 'sample2', 'projectionmapsofsightlines', '*', ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25fc64e4-5c9a-411a-aea2-7344d196f383",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, image_fp in enumerate( tqdm.tqdm( image_fps ) ):\n",
    "\n",
    "    img_old = plt.imread( image_fp ).astype( float ) / 255.\n",
    "\n",
    "    # Get the old colorbar\n",
    "    cbar_old = img_old[colorbar_loc[0][1]:colorbar_loc[1][1],colorbar_loc[0][0]:colorbar_loc[1][0]][::-1]\n",
    "\n",
    "    # Smooth, in case artifacts were introduced\n",
    "    cbar_old = np.mean( cbar_old, axis=1 )\n",
    "    cbar_old = scipy.signal.savgol_filter( cbar_old, 51, 1, axis=0 )\n",
    "    cbar_old[cbar_old>1.] = 1.\n",
    "    cbar_old[cbar_old<0.] = 0.\n",
    "\n",
    "    # Get values for interpolation\n",
    "    cbar_values_old = np.arange( cbar_old.shape[0], dtype=float )\n",
    "    cbar_values_old /= cbar_values_old.max()\n",
    "\n",
    "    # Interpolate to extract input values\n",
    "    color_interp_fn = scipy.interpolate.NearestNDInterpolator( cbar_old, cbar_values_old, )\n",
    "    img_values = color_interp_fn( img_old )\n",
    "\n",
    "    # Revise the map to give more room for whitespace\n",
    "    cmap_new_revised_colors = [ [ 0., 0., 0. ], ] + cmap_new.colors + [ [ 1., 1., 1. ], ] * 5\n",
    "    cmap_new_revised = matplotlib.colors.ListedColormap( cmap_new_revised_colors )\n",
    "\n",
    "    # Apply new colormap\n",
    "    img_new = cmap_new_revised( img_values )\n",
    "\n",
    "    # Save\n",
    "    output_fp = os.path.join( pm['figure_dir'], 'projections', 'projection_{:04d}.jpg'.format( i ) )\n",
    "    plt.imsave( output_fp, img_new )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a087b92c-644b-445e-9e2f-5c494763930f",
   "metadata": {},
   "source": [
    "# Metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30080e62-61da-44b7-b857-5aa6901b919e",
   "metadata": {},
   "source": [
    "## Calculate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdca93e-9ed4-4070-8735-705eea90e1e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "averages = verdict.Dict({})\n",
    "distances_between = verdict.Dict({})\n",
    "percentiles = verdict.Dict({})\n",
    "columns = verdict.Dict({})\n",
    "for i, sl in enumerate( sls ):\n",
    "\n",
    "    for prop_key in absorption_properties[pm['public_label']][sl]['source']['1D distributions']['centers'].keys():\n",
    "\n",
    "        for fraction in pm['f_enclosed']:\n",
    "            \n",
    "            onesided_fraction = ( 1 - fraction ) / 2.\n",
    "            percentile_str = helpers.percentile_str_fn( fraction )\n",
    "            lower_percentile_str = helpers.percentile_str_fn( onesided_fraction )\n",
    "            upper_percentile_str = helpers.percentile_str_fn( 1. - onesided_fraction )\n",
    "            \n",
    "            for variation, pm in pms.items():\n",
    "                \n",
    "                # Get data distributions\n",
    "                centers_data = absorption_properties[pm['public_label']][sl]['source']['1D distributions']['centers'][prop_key]\n",
    "                if helpers.logscale[prop_key]:\n",
    "                    centers_data = np.log10( centers_data )\n",
    "                dist_data = absorption_properties[pm['public_label']][sl]['source']['1D distributions']['distributions'][prop_key]\n",
    "                dist_data_sum = dist_data.sum()\n",
    "                cdf_data = np.cumsum( dist_data ) / dist_data_sum\n",
    "                interp_data = scipy.interpolate.interp1d( cdf_data, centers_data )\n",
    "\n",
    "                # Calculate data average\n",
    "                avg_data = interp_data( 0.5 )\n",
    "                averages.setitem( pm['public_label'], avg_data, 'source', prop_key, sl )\n",
    "\n",
    "                # Calculate data percentiles\n",
    "                try:\n",
    "                    lower = interp_data( onesided_fraction )\n",
    "                except ValueError:\n",
    "                    lower = centers_data[0]\n",
    "                try:\n",
    "                    upper = interp_data( 1. - onesided_fraction )\n",
    "                except ValueError:\n",
    "                    upper = centers_data[-1]\n",
    "                interval_data = np.array([ lower, upper ])\n",
    "                width_data = interval_data[1] - interval_data[0]\n",
    "                percentiles.setitem( pm['public_label'], interval_data, 'source', prop_key, 'interval', percentile_str, sl )\n",
    "                percentiles.setitem( pm['public_label'], width_data, 'source', prop_key, 'width', percentile_str, sl )\n",
    "                percentiles.setitem( pm['public_label'], lower, 'source', prop_key, 'percentile', lower_percentile_str, sl )\n",
    "                percentiles.setitem( pm['public_label'], upper, 'source', prop_key, 'percentile', upper_percentile_str, sl )\n",
    "                \n",
    "                # Calculate integrated column for data\n",
    "                column_data = np.trapz( dist_data, centers_data )\n",
    "                y_min_data = dist_data[dist_data > 0].min()\n",
    "                column_min_data = y_min_data * ( centers_data[-1] - centers_data[0] )\n",
    "                columns.setitem( pm['public_label'], column_data, 'source', 'total column', prop_key, sl )\n",
    "                columns.setitem( pm['public_label'], column_min_data, 'source', 'minimum column', prop_key, sl )\n",
    "\n",
    "                # Get modeled distributions\n",
    "                centers_estimated = absorption_properties[pm['public_label']][sl]['estimated']['1D distributions']['centers'][prop_key]\n",
    "                if helpers.logscale[prop_key]:\n",
    "                    centers_estimated = np.log10( centers_estimated )\n",
    "                dist_estimated = absorption_properties[pm['public_label']][sl]['estimated']['1D distributions']['combined distributions'][prop_key]\n",
    "                dist_estimated_sum = dist_estimated.sum()\n",
    "                cdf_estimated = np.cumsum( dist_estimated ) / dist_estimated_sum\n",
    "                interp_estimated = scipy.interpolate.interp1d( cdf_estimated, centers_estimated )\n",
    "                \n",
    "                # Calculate MLE and averages\n",
    "                x_max = centers_estimated[np.argmax( dist_estimated )]\n",
    "                x_avg = interp_estimated( 0.5 )\n",
    "                averages.setitem( pm['public_label'], x_max, 'estimated', 'MLE', prop_key, sl )\n",
    "                averages.setitem( pm['public_label'], x_avg, 'estimated', 'average', prop_key, sl )\n",
    "\n",
    "                # Calculate parameter estimation percentiles\n",
    "                try:\n",
    "                    lower = interp_estimated( onesided_fraction )\n",
    "                except ValueError:\n",
    "                    lower = centers_estimated[0]\n",
    "                try:\n",
    "                    upper = interp_estimated( 1. - onesided_fraction )\n",
    "                except ValueError:\n",
    "                    lower = centers_estimated[-1]\n",
    "                interval_data = np.array([ lower, upper ])\n",
    "                width_data = interval_data[1] - interval_data[0]\n",
    "                percentiles.setitem( pm['public_label'], interval_data, 'estimated', 'combined', prop_key, 'interval', percentile_str, sl )\n",
    "                percentiles.setitem( pm['public_label'], width_data, 'estimated', 'combined', prop_key, 'width', percentile_str, sl )\n",
    "                percentiles.setitem( pm['public_label'], lower, 'estimated', 'combined', prop_key, 'percentile', lower_percentile_str, sl )\n",
    "                percentiles.setitem( pm['public_label'], upper, 'estimated', 'combined', prop_key, 'percentile', upper_percentile_str, sl )\n",
    "\n",
    "                # Calculate necessary width\n",
    "                width_necessary = np.max( np.abs( interval_data - x_max ) )\n",
    "                width_between_peaks = np.abs( avg_data - x_max )\n",
    "                distances_between.setitem( pm['public_label'], width_necessary, prop_key, 'distance_to_enclose', str( fraction ), sl )\n",
    "                distances_between.setitem( pm['public_label'], width_between_peaks, prop_key, 'distance_to_avg', sl )\n",
    "                \n",
    "                # Calculate integrated column for estimation\n",
    "                column_estimated = np.trapz( dist_estimated, centers_estimated )\n",
    "                columns.setitem( pm['public_label'], column_estimated, 'estimated', 'combined column', prop_key, sl )\n",
    "                \n",
    "                # Some of the calculations for individual components\n",
    "                comp_dists = absorption_properties[pm['public_label']][sl]['estimated']['1D distributions']['individual distributions']\n",
    "                for comp_key, comp in comp_dists.items():\n",
    "                    dist_comp = comp[prop_key]\n",
    "                    \n",
    "                    # Percentiles\n",
    "                    dist_comp_sum = dist_comp.sum()\n",
    "                    cdf_comp = np.cumsum( dist_comp ) / dist_comp_sum\n",
    "                    interp_comp = scipy.interpolate.interp1d( cdf_comp, centers_estimated )\n",
    "                    try:\n",
    "                        lower = interp_comp( onesided_fraction )\n",
    "                    except ValueError:\n",
    "                        lower = centers_estimated[0]\n",
    "                    try:\n",
    "                        upper = interp_comp( 1. - onesided_fraction )\n",
    "                    except ValueError:\n",
    "                        lower = centers_estimated[-1]\n",
    "                    interval_data = np.array([ lower, upper ])\n",
    "                    width_data = interval_data[1] - interval_data[0]\n",
    "                    percentiles.setitem( pm['public_label'], interval_data, 'estimated', 'individual', sl, prop_key, 'interval', percentile_str, comp_key )\n",
    "                    percentiles.setitem( pm['public_label'], width_data, 'estimated', 'individual', sl, prop_key, 'width', percentile_str, comp_key )\n",
    "                    percentiles.setitem( pm['public_label'], lower, 'estimated', 'individual', sl, prop_key, 'percentile', lower_percentile_str, comp_key )\n",
    "                    percentiles.setitem( pm['public_label'], upper, 'estimated', 'individual', sl, prop_key, 'percentile', upper_percentile_str, comp_key )\n",
    "                    \n",
    "                    # Integrated column\n",
    "                    column_comp = np.trapz( comp[prop_key], centers_estimated )\n",
    "                    columns.setitem( pm['public_label'], column_comp, 'estimated', 'individual columns', sl, comp_key, prop_key )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba5f34eb-d918-439f-a517-b81e749629ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate fraction enclosed\n",
    "f_estimated_to_enclose_f_data = verdict.Dict({})\n",
    "f_data_enclosed_by_f_estimated = verdict.Dict({})\n",
    "for i, sl in enumerate( sls ):\n",
    "\n",
    "    for prop_key in absorption_properties[pm['public_label']][sl]['source']['1D distributions']['centers'].keys():\n",
    "        \n",
    "        for variation, pm in pms.items():\n",
    "\n",
    "            # Get data distributions\n",
    "            centers_data = absorption_properties[pm['public_label']][sl]['source']['1D distributions']['centers'][prop_key]\n",
    "            dist_data = absorption_properties[pm['public_label']][sl]['source']['1D distributions']['distributions'][prop_key]\n",
    "            dist_data_sum = dist_data.sum()\n",
    "\n",
    "            # Get modeled distributions\n",
    "            centers_estimated = absorption_properties[pm['public_label']][sl]['estimated']['1D distributions']['centers'][prop_key]\n",
    "            bins_estimated = np.full( centers_estimated.shape, np.nan )\n",
    "            bin_widths = np.diff( centers_estimated )\n",
    "            bins_estimated[0:centers_estimated.size-1] = centers_estimated[:-1] - bin_widths / 2.\n",
    "            bins_estimated[centers_estimated.size-1] = centers_estimated[-1] + bin_widths[-1] / 2.\n",
    "            dist_estimated = absorption_properties[pm['public_label']][sl]['estimated']['1D distributions']['combined distributions'][prop_key]\n",
    "            dist_estimated_sum = dist_estimated.sum()\n",
    "            max_ind_estimated = dist_estimated.argmax()\n",
    "\n",
    "            # Calculate fraction included arrays\n",
    "            j = 0\n",
    "            f_included_data = []\n",
    "            f_included_estimated = []\n",
    "            while True:\n",
    "\n",
    "                # Get range to include\n",
    "                left = max_ind_estimated - j\n",
    "                right = max_ind_estimated + j + 1\n",
    "\n",
    "                # Check if looping is done\n",
    "                left_in_bounds = left >= 0\n",
    "                right_in_bounds = right < centers_estimated.size\n",
    "                continue_looping = left_in_bounds or right_in_bounds\n",
    "                if not continue_looping:\n",
    "                    break\n",
    "                    \n",
    "                if not left_in_bounds:\n",
    "                    left = 0\n",
    "                if not right_in_bounds:\n",
    "                    right = centers_estimated.size - 1\n",
    "\n",
    "                # Get fraction included\n",
    "                x_value_left = bins_estimated[left]\n",
    "                x_value_right = bins_estimated[right]\n",
    "                is_in_range_data = ( centers_data > x_value_left ) & (centers_data < x_value_right )\n",
    "\n",
    "                f_included_data.append( dist_data[is_in_range_data].sum() / dist_data_sum )\n",
    "                f_included_estimated.append( dist_estimated[left:right].sum() / dist_estimated_sum )\n",
    "\n",
    "                j += 1\n",
    "            f_included_data = np.array( f_included_data )\n",
    "            f_included_estimated = np.array( f_included_estimated )\n",
    "            \n",
    "            # Extract a summary statistic\n",
    "            interp_data_estimated = scipy.interpolate.interp1d( f_included_data, f_included_estimated )\n",
    "            interp_estimated_data = scipy.interpolate.interp1d( f_included_estimated, f_included_data )\n",
    "            for fraction in pm['f_enclosed']:\n",
    "                \n",
    "                ## Fraction needed to enclose\n",
    "                try:\n",
    "                    f_to_enclose = interp_data_estimated( fraction )\n",
    "                    \n",
    "                    # If a value of 1 is needed, then the distribution doesn't actually enclose the data.\n",
    "                    if np.isclose( f_to_enclose, 1. ):\n",
    "                        f_to_enclose = np.nan\n",
    "                # If above or below the interpolation range\n",
    "                except ValueError:\n",
    "                    # If below, that means the fraction was reached with the first cell\n",
    "                    if fraction <= f_included_data[0]:\n",
    "                        f_to_enclose = f_included_estimated[0]\n",
    "                    # If above, then invalid\n",
    "                    elif fraction >= f_included_data[-1]:\n",
    "                        f_to_enclose = np.nan\n",
    "                f_estimated_to_enclose_f_data.setitem( pm['public_label'], f_to_enclose, prop_key, str( fraction ), sl )\n",
    "                \n",
    "                ## Fraction enclosed by a fixed value\n",
    "                try:\n",
    "                    f_data_enclosed = interp_estimated_data( fraction )\n",
    "                # If above or below the interpolation range\n",
    "                except ValueError:\n",
    "                    # If below the interpolation range, then f_estimated jumps sharply early,\n",
    "                    # so the enclosed amount is just the first entry in f_included_data\n",
    "                    if fraction < f_included_estimated[0]:\n",
    "                        f_data_enclosed = f_included_data[0]\n",
    "                    # If above the interpolation range, then invalid\n",
    "                    if fraction >= f_included_estimated[-1]:\n",
    "                        f_data_enclosed = np.nan\n",
    "                f_data_enclosed_by_f_estimated.setitem( pm['public_label'], f_data_enclosed, prop_key, str( fraction ), sl )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5331e51b-cd38-4afc-8d23-d7d0656763cc",
   "metadata": {},
   "source": [
    "## Plotting Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0142080-2bec-49b0-8e7a-54b1fb6b266a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_metric( get_ys, fig_tuple=None, y_fn_is_per_component=False, **scatter_kwargs ):\n",
    "    \n",
    "    if fig_tuple is None:\n",
    "        # Setup figure\n",
    "        panel_width = plt.rcParams['figure.figsize'][0]\n",
    "        n_rows = len( mosaic_dist )\n",
    "        n_cols = len( mosaic_dist[0] )\n",
    "        fig = plt.figure( figsize=(n_cols*panel_width, n_rows*panel_width), facecolor='w' )\n",
    "        main_ax = plt.gca()\n",
    "        main_ax.tick_params( left=False, labelleft=False, bottom=False, labelbottom=False )\n",
    "        for spine in main_ax.spines.values():\n",
    "            spine.set_visible( False )\n",
    "\n",
    "        ax_dict = fig.subplot_mosaic(\n",
    "            mosaic_dist,\n",
    "            gridspec_kw = { 'hspace': 0.15, 'wspace': 0.12 },\n",
    "        )\n",
    "    else:\n",
    "        fig, main_ax, ax_dict = fig_tuple\n",
    "    \n",
    "    # Axes tweaks\n",
    "    for ax_key, ax in ax_dict.items():\n",
    "\n",
    "        ax.set_xticks( xs )\n",
    "        xtick_labels = [ _[-2:] for _ in sls ]\n",
    "        ax.set_xticklabels( xtick_labels )\n",
    "        \n",
    "        # Axes tweaks\n",
    "        axis_label = helpers.property_labels_no_units[ax_key]\n",
    "        if helpers.logscale[ax_key]:\n",
    "            axis_label = 'log' + axis_label\n",
    "\n",
    "        ax.annotate(\n",
    "            text = axis_label,\n",
    "            xy = ( 0, 1 ),\n",
    "            xycoords = 'axes fraction',\n",
    "            xytext = ( 5, -5 ),\n",
    "            textcoords = 'offset points',\n",
    "            va = 'top',\n",
    "            ha = 'left',\n",
    "        )\n",
    "        \n",
    "    for variation, pm in pms.items():\n",
    "\n",
    "        for ax_key, ax in ax_dict.items():\n",
    "        \n",
    "            if not y_fn_is_per_component:\n",
    "                ys = get_ys( variation=variation, prop_key=ax_key )\n",
    "                if ys is None:\n",
    "                    continue\n",
    "                    \n",
    "                color = helpers.colors_for_variations[pm['variation']]\n",
    "                fill_color = color\n",
    "                used_scatter_kwargs = dict(\n",
    "                    s = 100,\n",
    "                    edgecolor = color,\n",
    "                    color = fill_color,\n",
    "                )\n",
    "                used_scatter_kwargs.update( scatter_kwargs )\n",
    "\n",
    "                scatter = ax.scatter(\n",
    "                    xs,\n",
    "                    ys,\n",
    "                    **used_scatter_kwargs\n",
    "                )\n",
    "\n",
    "            else:\n",
    "                ys_comps = get_ys( variation=variation, prop_key=ax_key )\n",
    "                \n",
    "                if ys_comps is None:\n",
    "                    continue\n",
    "                    \n",
    "                for i, sl in enumerate( sls ):\n",
    "                    for comp_key, ys in ys_comps[sl].items():\n",
    "\n",
    "                        color = helpers.colors_for_variations[pm['variation']]\n",
    "                        fill_color = color\n",
    "                        used_scatter_kwargs = dict(\n",
    "                            s = 100,\n",
    "                            edgecolor = color,\n",
    "                            color = fill_color,\n",
    "                        )\n",
    "                        used_scatter_kwargs.update( scatter_kwargs )\n",
    "\n",
    "                        scatter = ax.scatter(\n",
    "                            xs[i],\n",
    "                            ys[ax_key],\n",
    "                            **used_scatter_kwargs\n",
    "                        )\n",
    "\n",
    "    \n",
    "    return fig, main_ax, ax_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffaf34de-caa2-4934-bd75-80f1e9279ce0",
   "metadata": {},
   "source": [
    "## Columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd5f05f-5426-4770-ba66-68d6ff8ce88d",
   "metadata": {},
   "source": [
    "### Source, Other Source"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "059fcefe-08bc-4d01-8a32-d85a2fd6ab8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source_ys( variation, prop_key ):\n",
    "\n",
    "    pm = pms[variation]\n",
    "\n",
    "    ys = columns[pm['public_label']]['source']['total column'][prop_key].array()\n",
    "    return ys\n",
    "\n",
    "fig, main_ax, ax_dict = plot_metric( get_source_ys, color='k', linewidth=2, )\n",
    "\n",
    "for ax_key, ax in ax_dict.items():\n",
    "    ax.set_yscale( 'log' )\n",
    "    \n",
    "    if ax.get_subplotspec().is_first_col():\n",
    "        ax.set_ylabel( r'$N_{\\rm{HI}}$ [cm$^{-2}$]' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5360b415-ae3b-4e36-b03e-994bee5697b6",
   "metadata": {},
   "source": [
    "Different total columns because of different UVBs, but reassuringly the total column is the same regardless of the property the integration is done over."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cbfd6c-82ab-4d65-9593-4af81f268217",
   "metadata": {},
   "source": [
    "### Source, Estimated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796142c8-36b9-4535-b0ef-4c9c522804b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source_ys( variation, prop_key ):\n",
    "\n",
    "    pm = pms[variation]\n",
    "\n",
    "    ys = columns[pm['public_label']]['source']['total column'][prop_key].array()\n",
    "    return ys\n",
    "\n",
    "def get_estimated_ys( variation, prop_key ):\n",
    "\n",
    "    pm = pms[variation]\n",
    "\n",
    "    ys = columns[pm['public_label']]['estimated']['combined column'][prop_key].array()\n",
    "    return ys\n",
    "\n",
    "fig, main_ax, ax_dict = plot_metric( get_source_ys, color='k', linewidth=2, )\n",
    "\n",
    "fig, main_ax, ax_dict = plot_metric( get_estimated_ys, (fig, main_ax, ax_dict) )\n",
    "\n",
    "for ax_key, ax in ax_dict.items():\n",
    "    ax.set_yscale( 'log' )\n",
    "    \n",
    "    if ax.get_subplotspec().is_first_col():\n",
    "        ax.set_ylabel( r'$N_{\\rm{HI}}$ [cm$^{-2}$]' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d50927f-9673-441b-935a-bcd53acffbf3",
   "metadata": {},
   "source": [
    "### Source, Estimated per Component"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7f2d22e-cb0a-48cd-be9e-27b35e8fe2d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source_ys( variation, prop_key ):\n",
    "\n",
    "    pm = pms[variation]\n",
    "\n",
    "    ys = columns[pm['public_label']]['source']['total column'][prop_key].array()\n",
    "    return ys\n",
    "\n",
    "def get_source_ys_min( variation, prop_key ):\n",
    "\n",
    "    pm = pms[variation]\n",
    "\n",
    "    ys = columns[pm['public_label']]['source']['minimum column'][prop_key].array()\n",
    "    return ys\n",
    "\n",
    "def get_estimated_ys_comp( variation, prop_key ):\n",
    "\n",
    "    pm = pms[variation]\n",
    "    \n",
    "    ys_comps = columns[pm['public_label']]['estimated']['individual columns']\n",
    "    return ys_comps\n",
    "\n",
    "fig, main_ax, ax_dict = plot_metric( get_source_ys, color='k', linewidth=2, marker='v' )\n",
    "\n",
    "fig, main_ax, ax_dict = plot_metric( get_source_ys_min, (fig, main_ax, ax_dict), color='k', linewidth=2, marker='^' )\n",
    "\n",
    "fig, main_ax, ax_dict = plot_metric( get_estimated_ys_comp, (fig, main_ax, ax_dict), y_fn_is_per_component=True, s=50, )\n",
    "\n",
    "for ax_key, ax in ax_dict.items():\n",
    "    ax.set_yscale( 'log' )\n",
    "    \n",
    "    if ax.get_subplotspec().is_first_col():\n",
    "        ax.set_ylabel( r'$N_{\\rm{HI}}$ [cm$^{-2}$]' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b5e1653-ed75-41d6-8ff8-a04d5d6ff1e9",
   "metadata": {},
   "source": [
    "The arrows indicate the range for which components are of interest, e.g. enough to plot.\n",
    "Bottom arrow shows column below which dist drops to zero.\n",
    "Top arrow shows total integrated column."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c7c3b61-2730-4c46-b3d0-0b83bad3c4cc",
   "metadata": {
    "tags": []
   },
   "source": [
    "## MLEs and Average Values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdafa17b-74ca-49cd-891a-300f4140c49e",
   "metadata": {},
   "source": [
    "### Straight-up values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebb76e1c-cfbb-4f88-bd02-501f45a4d1fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_estimated_ys( variation, prop_key ):\n",
    "\n",
    "    pm = pms[variation]\n",
    "\n",
    "    mle = averages[pm['public_label']]['estimated']['MLE'][prop_key].array()\n",
    "    return mle\n",
    "\n",
    "fig, main_ax, ax_dict = plot_metric( get_estimated_ys )\n",
    "\n",
    "def get_source_ys( variation, prop_key ):\n",
    "    \n",
    "    if variation != 'original':\n",
    "        return None\n",
    "\n",
    "    pm = pms[variation]\n",
    "\n",
    "    avg = averages[pm['public_label']]['source'][prop_key].array()\n",
    "    return avg\n",
    "\n",
    "fig, main_ax, ax_dict = plot_metric( get_source_ys, ( fig, main_ax, ax_dict ), marker='x', color='k', )\n",
    "\n",
    "# Axes tweaks\n",
    "for ax_key, ax in ax_dict.items():         \n",
    "    ylims = helpers.lims[ax_key]\n",
    "    if helpers.logscale[ax_key]:\n",
    "        ylims = np.log10( ylims )\n",
    "\n",
    "    ax.set_ylim( ylims )\n",
    "\n",
    "main_ax.set_xlabel( 'sightline ID', labelpad=30 )\n",
    "main_ax.set_ylabel( r'$x_{\\rm MLE}$', labelpad=40 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47106b2-288a-4b51-9883-2d71111c28ad",
   "metadata": {},
   "source": [
    "### Difference between values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2544bff7-3e98-4669-b88c-6b77e8b07527",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_ys( variation, prop_key ):\n",
    "\n",
    "    pm = pms[variation]\n",
    "\n",
    "    d_between = distances_between[pm['public_label']][prop_key]['distance_to_avg'].array()\n",
    "    return d_between\n",
    "\n",
    "fig, main_ax, ax_dict = plot_metric( get_ys )\n",
    "\n",
    "main_ax.set_xlabel( 'sightline ID', labelpad=30 )\n",
    "main_ax.set_ylabel( r'$\\left| x_{\\rm MLE} - \\langle x \\rangle \\right|$', labelpad=30 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bfc4218-1a28-4fb2-9e5c-9767e83f60fa",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Distribution Widths"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30b9b09e-760c-4a8b-803d-289163eac1f0",
   "metadata": {},
   "source": [
    "### Scaled by error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "448f371a-f251-418c-958f-5f4fe6c418c8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for fraction in pm['f_enclosed']:\n",
    "    \n",
    "    def get_widths( variation, prop_key ):\n",
    "        \n",
    "        pm = pms[variation]\n",
    "        \n",
    "        percentile = percentiles[pm['public_label']]['estimated']['combined'][prop_key]['width'][str(100.*fraction)].array()\n",
    "        d_between = distances_between[pm['public_label']][prop_key]['distance_to_avg'].array()\n",
    "        \n",
    "        if helpers.logscale[prop_key]:\n",
    "            return percentile - d_between\n",
    "        else:\n",
    "            return percentile / d_between\n",
    "    \n",
    "    fig, main_ax, ax_dict = plot_metric( get_widths )\n",
    "    \n",
    "    for ax_key, ax in ax_dict.items():\n",
    "        \n",
    "        hline = 1\n",
    "        \n",
    "        if helpers.logscale[ax_key]:\n",
    "            hline = np.log10( hline )\n",
    "            \n",
    "        ax.axhline( hline, color='0.5', )\n",
    "\n",
    "    main_ax.set_xlabel( 'sightline ID', labelpad=30 )\n",
    "    main_ax.set_ylabel( '{}'.format( int( fraction * 100 ) ) + r'th percentile / $\\left| x_{\\rm MLE} - \\langle x \\rangle \\right|$', labelpad=60 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f5499e0-5928-46de-80e0-e0ad06e1c158",
   "metadata": {},
   "source": [
    "### Scaled by distance to enclose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99e11855-078d-4037-bc26-b0168b9ef8f2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for fraction in pm['f_enclosed']:\n",
    "    \n",
    "    def get_widths( variation, prop_key ):\n",
    "        \n",
    "        pm = pms[variation]\n",
    "        \n",
    "        percentile = percentiles[pm['public_label']]['estimated']['combined'][prop_key]['width'][str(100.*fraction)].array()\n",
    "        d_between = distances_between[pm['public_label']][prop_key]['distance_to_enclose'][str(fraction)].array()\n",
    "        \n",
    "        if helpers.logscale[prop_key]:\n",
    "            return percentile - d_between\n",
    "        else:\n",
    "            return percentile / d_between\n",
    "    \n",
    "    fig, main_ax, ax_dict = plot_metric( get_widths )\n",
    "    \n",
    "    for ax_key, ax in ax_dict.items():\n",
    "        \n",
    "        hline = 1\n",
    "        \n",
    "        if helpers.logscale[ax_key]:\n",
    "            hline = np.log10( hline )\n",
    "            \n",
    "        ax.axhline( hline, color='0.5', )\n",
    "\n",
    "    main_ax.set_xlabel( 'sightline ID', labelpad=30 )\n",
    "    main_ax.set_ylabel( '{}'.format( int( fraction * 100 ) ) + r'th percentile / $\\left| x_{\\rm MLE} - x_{' + '{}'.format( int( fraction * 100 ) ) + r'} \\right|$', labelpad=60 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "434f7b75-4ce9-4189-b8ca-58050a01a738",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fraction enclosed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23af2a85-ce7e-4998-8fcb-ce22b2ac811e",
   "metadata": {},
   "source": [
    "### % of likelihood to enclose"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe589659-8e31-454f-9861-7bcd303c9325",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fraction in pm['f_enclosed']:\n",
    "    \n",
    "    def get_ys( variation, prop_key ):\n",
    "        \n",
    "        pm = pms[variation]\n",
    "        \n",
    "        ys = f_estimated_to_enclose_f_data[pm['public_label']][prop_key][str(fraction)].array() * 100\n",
    "        \n",
    "        return ys\n",
    "    \n",
    "    fig, main_ax, ax_dict = plot_metric( get_ys )\n",
    "    \n",
    "    for ax_key, ax in ax_dict.items():\n",
    "        \n",
    "        ax.set_ylim( 0, 100 )\n",
    "        \n",
    "        hline = fraction * 100\n",
    "            \n",
    "        ax.axhline( hline, color='0.5', )\n",
    "\n",
    "    main_ax.set_xlabel( 'sightline ID', labelpad=30 )\n",
    "    main_ax.set_ylabel( '% of likelihood to enclose {}% of the data'.format( int( fraction * 100 ) ), labelpad=60 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef29745d-f02a-4495-8176-525445c811a8",
   "metadata": {},
   "source": [
    "### % of data enclosed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f56b91bf-10fc-467c-ba28-59d59c957215",
   "metadata": {},
   "outputs": [],
   "source": [
    "for fraction in pm['f_enclosed']:\n",
    "    \n",
    "    def get_ys( variation, prop_key ):\n",
    "        \n",
    "        pm = pms[variation]\n",
    "        \n",
    "        ys = f_data_enclosed_by_f_estimated[pm['public_label']][ax_key][str(fraction)].array() * 100\n",
    "        \n",
    "        return ys\n",
    "    \n",
    "    fig, main_ax, ax_dict = plot_metric( get_ys )\n",
    "    \n",
    "    for ax_key, ax in ax_dict.items():\n",
    "        \n",
    "        ax.set_ylim( 0, 100 )\n",
    "        \n",
    "        hline = fraction * 100\n",
    "            \n",
    "        ax.axhline( hline, color='0.5', )\n",
    "\n",
    "    main_ax.set_xlabel( 'sightline ID', labelpad=30 )\n",
    "    main_ax.set_ylabel( '% of data enclosed by {}% of the likelihood'.format( int( fraction * 100 ) ), labelpad=30 )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5970c7c2-60fc-4868-bdee-1d4bf2048b2d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Correlation Coefficients"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50461f2f-f186-452a-8a09-302cc6062d74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup Figure\n",
    "n_rows = len( mosaic_dist )\n",
    "n_cols = len( mosaic_dist[0] )\n",
    "panel_width = plt.rcParams['figure.figsize'][0]\n",
    "fig = plt.figure( figsize=(n_cols*panel_width, n_rows*panel_width), facecolor='w' )\n",
    "ax_dict = fig.subplot_mosaic(\n",
    "    mosaic_dist,\n",
    "    gridspec_kw = { 'wspace': 0.7 },\n",
    ")\n",
    "ax_dict['legend'] = ax_dict['vlos']\n",
    "\n",
    "def r_scatter( ax, ys, c_key, color=None, label_tag=None ):\n",
    "    c_params = helpers.correlation_coefficients[c_key]\n",
    "    if 'logscale' in c_params:\n",
    "        if c_params['logscale']:\n",
    "            facecolors = 'none'\n",
    "    else:\n",
    "        facecolors = color\n",
    "        \n",
    "    scatter = ax.scatter(\n",
    "        xs,\n",
    "        ys,\n",
    "        label = '{}, {}'.format( label_tag, c_key ),\n",
    "        edgecolors = color,\n",
    "        facecolors = facecolors,\n",
    "        marker = correlation_markers[c_key],\n",
    "        # s = correlation_sizes[c_key],\n",
    "        linewidth = 2,\n",
    "    )\n",
    "\n",
    "    \n",
    "# Overall\n",
    "for variation, pm in pms.items():\n",
    "    \n",
    "    correlations = correlations_all[pm['public_label']]\n",
    "    plotting_params = variation_plotting_params[variation]\n",
    "    \n",
    "    # for c_key in correlations_plotted:\n",
    "    #     r_scatter(\n",
    "    #         ax_dict['all'],\n",
    "    #         correlations[c_key]['ndim'].array(),\n",
    "    #         c_key,\n",
    "    #         color = plotting_params['color'],\n",
    "    #     )\n",
    "\n",
    "    # Each property\n",
    "    for j, x_key in enumerate( tqdm.tqdm( pm['prop_keys'], bar_format=pm['bar_format'] ) ):\n",
    "\n",
    "        ax = ax_dict[x_key]\n",
    "\n",
    "        for c_key in correlations_plotted:\n",
    "            r_scatter(\n",
    "                ax,\n",
    "                correlations[c_key]['matrix'].array()[:,j,j],\n",
    "                c_key,\n",
    "                color = plotting_params['color'],\n",
    "                label_tag = plotting_params['label'],\n",
    "            )\n",
    "    \n",
    "        \n",
    "# Add a legend\n",
    "h, l = ax_dict['vlos'].get_legend_handles_labels()\n",
    "legend = ax_dict['legend'].legend(\n",
    "    h,\n",
    "    l,\n",
    "    loc = 'lower left',\n",
    "    # prop = {'size': 14},\n",
    "    ncol = 2,\n",
    "    framealpha = 1,\n",
    ")\n",
    "# ax_dict['legend'].axis( 'off' )\n",
    "# ax_dict['legend'].annotate(\n",
    "#     text = r'$r = \\frac{ \\langle {\\rm actual } \\vert  {\\rm found } \\rangle }{ \\vert {\\rm actual} \\vert \\vert {\\rm found } \\vert }$',\n",
    "#     xy = ( 0, 1 ),\n",
    "#     xycoords = 'axes fraction',\n",
    "#     xytext = ( 5, -5 ),\n",
    "#     textcoords = 'offset points',\n",
    "#     ha = 'center',\n",
    "#     va = 'top',\n",
    "#     fontsize = 18,\n",
    "# )\n",
    "        \n",
    "# Cleanup\n",
    "for x_key, ax in ax_dict.items():\n",
    "    \n",
    "    if x_key in [ 'legend', 'empty' ]:\n",
    "        continue\n",
    "    \n",
    "    subplotspec = ax.get_subplotspec()\n",
    "    \n",
    "    for value in [ -1, 0, 1 ]:\n",
    "        ax.axhline(\n",
    "            value,\n",
    "            color = pm['background_linecolor'],\n",
    "            linewidth = 1,\n",
    "            zorder = -100,\n",
    "        )\n",
    "        \n",
    "    ax.set_ylabel( helpers.correlation_coefficient_property_labels[x_key] )\n",
    "    if subplotspec.is_last_row():\n",
    "        ax.set_xlabel( 'sightline ID', )\n",
    "        \n",
    "    ax.set_xticks( xs )\n",
    "    xtick_labels = [ _[-2:] for _ in correlations[c_key]['ndim'].keys_array() ]\n",
    "    ax.set_xticklabels( xtick_labels )\n",
    "        \n",
    "    ax.set_ylim( -0.3, 1.1 )\n",
    "    \n",
    "# Save\n",
    "savedir = pm['figure_dir']\n",
    "os.makedirs( savedir, exist_ok=True )\n",
    "savefile = 'correlations.pdf'\n",
    "save_fp = os.path.join( savedir, savefile )\n",
    "print( 'Saving figure to {}'.format( save_fp ) )\n",
    "plt.savefig( save_fp, bbox_inches='tight' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e67c3b-62df-4f61-9658-ddf7956a20f2",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Distribution Comparisons"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192cb146-1410-4089-9aa9-f94a62a14bd1",
   "metadata": {},
   "source": [
    "## 1D Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44660630-d839-499a-a69f-8b23e1af9d4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "panel_length = plt.rcParams['figure.figsize'][0]\n",
    "n_row = len( mosaic_dist )\n",
    "n_col = len( mosaic_dist[0] )\n",
    "figsize = ( panel_length * n_col, panel_length / 2. * n_row )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c6a6b76-f29f-4a43-a203-abff5e4f6b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "dist_figs = []\n",
    "\n",
    "for i, sl in enumerate( sls ):\n",
    "    \n",
    "    if sl != '0012':\n",
    "        continue\n",
    "\n",
    "    # Setup figure\n",
    "    fig = plt.figure( figsize=figsize )\n",
    "    main_ax = plt.gca()\n",
    "    main_ax.tick_params( left=False, labelleft=False, bottom=False, labelbottom=False )\n",
    "    for spine in main_ax.spines.values():\n",
    "        spine.set_visible( False )\n",
    "\n",
    "    ax_dict = fig.subplot_mosaic(\n",
    "        mosaic_dist,\n",
    "    )\n",
    "\n",
    "    for ax_key, ax in ax_dict.items():\n",
    "\n",
    "        ax.annotate(\n",
    "            text = helpers.property_labels_1D[ax_key],\n",
    "            xy = ( 0, 1 ),\n",
    "            xycoords = 'axes fraction',\n",
    "            xytext = ( 5, -5 ),\n",
    "            textcoords = 'offset points',\n",
    "            va = 'top',\n",
    "            ha = 'left',\n",
    "        )\n",
    "        \n",
    "        xlim = helpers.lims[ax_key]\n",
    "        if helpers.logscale[ax_key]:\n",
    "            xlim = np.log10( xlim )\n",
    "\n",
    "        ax.set_xlim( xlim )\n",
    "\n",
    "        ax.set_yscale( 'log' )\n",
    "\n",
    "        ax.set_xlabel( helpers.property_labels[ax_key] )\n",
    "        \n",
    "    main_ax.set_title(\n",
    "        label = sl,\n",
    "        loc = 'left',\n",
    "    )\n",
    "\n",
    "    y_maxs = verdict.Dict({})\n",
    "    for j, prop_key in enumerate( ax_dict.keys() ):\n",
    "\n",
    "        # Get plot panel\n",
    "        ax = ax_dict[prop_key]\n",
    "        subplotspec = ax.get_subplotspec()\n",
    "\n",
    "        # Get data distributions\n",
    "        centers_data = absorption_properties[pm['public_label']][sl]['source']['1D distributions']['centers'][prop_key]\n",
    "        if helpers.logscale[prop_key]:\n",
    "            centers_data = np.log10( centers_data )\n",
    "        dist_data = absorption_properties[pm['public_label']][sl]['source']['1D distributions']['distributions'][prop_key]\n",
    "        y_maxs[prop_key] = [ dist_data.max(), ]\n",
    "        \n",
    "        ax.fill_between(\n",
    "            centers_data,\n",
    "            dist_data,\n",
    "            color = 'k',\n",
    "            step = 'mid',\n",
    "            label = 'source',\n",
    "        )\n",
    "        \n",
    "        for variation, pm in pms.items():\n",
    "\n",
    "            # Modeled distribution\n",
    "            centers_estimated = absorption_properties[pm['public_label']][sl]['estimated']['1D distributions']['centers'][prop_key]\n",
    "            if helpers.logscale[prop_key]:\n",
    "                centers_estimated = np.log10( centers_estimated )\n",
    "            dist_estimated = absorption_properties[pm['public_label']][sl]['estimated']['1D distributions']['combined distributions'][prop_key]\n",
    "            comp_dist_estimated = absorption_properties[pm['public_label']][sl]['estimated']['1D distributions']['individual distributions']\n",
    "            y_maxs[prop_key].append( dist_estimated.max() )\n",
    "            \n",
    "            \n",
    "            # Smooth low contributions\n",
    "            percentile = percentiles[pm['public_label']]['estimated']['combined'][prop_key]['interval']['99.0'][sl]\n",
    "            is_tail_low = centers_estimated < percentile[0]\n",
    "            is_tail_high = centers_estimated > percentile[1]\n",
    "            is_tail = is_tail_low | is_tail_high\n",
    "            is_not_tail= np.invert( is_tail )\n",
    "            dist_plotted = copy.copy( dist_estimated )\n",
    "            window_length = 3\n",
    "            if is_tail_low.sum() >= window_length:\n",
    "                dist_plotted[is_tail_low] = scipy.signal.savgol_filter( dist_plotted[is_tail_low], window_length, 0 )\n",
    "            if is_tail_high.sum() >= window_length:\n",
    "                dist_plotted[is_tail_high] = scipy.signal.savgol_filter( dist_plotted[is_tail_high], window_length, 0 )            \n",
    "            \n",
    "            # Truncate low contributions, if desired\n",
    "            # if prop_key != 'vlos':\n",
    "            if False:\n",
    "                alphas = [ 0.6, 1, ]\n",
    "                ps_trunc = [ None, '99.7', ]\n",
    "            else:\n",
    "                alphas = [ 1, ]\n",
    "                ps_trunc = [ None, ]\n",
    "            for j, p in enumerate( ps_trunc ):\n",
    "                if p is not None:\n",
    "                    percentile = percentiles[pm['public_label']]['estimated']['combined'][prop_key]['interval'][p][sl]\n",
    "                    is_truncated_low = centers_estimated < percentile[0]\n",
    "                    is_truncated_high = centers_estimated > percentile[1]\n",
    "                    is_truncated = is_truncated_low | is_truncated_high\n",
    "                    is_not_truncated = np.invert( is_truncated )\n",
    "                else:\n",
    "                    is_not_truncated = np.ones( centers_estimated.shape ).astype( bool )\n",
    "            \n",
    "                ax.plot(\n",
    "                    centers_estimated[is_not_truncated],\n",
    "                    dist_plotted[is_not_truncated],\n",
    "                    linewidth = 3,\n",
    "                    color = helpers.colors_for_variations[variation],\n",
    "                    alpha = alphas[j],\n",
    "                    label = 'estimated\\u2014{}'.format( pm['public_label'] ),\n",
    "                )\n",
    "            \n",
    "            if pm['variation'] == 'original':\n",
    "                continue\n",
    "            for comp_key, comp_dist in comp_dist_estimated.items():\n",
    "                \n",
    "                if prop_key != 'vlos':\n",
    "                    alphas = [ 0., 1, 1, ]\n",
    "                    ps_trunc = [ None, '99.0', '68.0', ]\n",
    "                    linewidths = [ 0, 1.5, 2.5 ]\n",
    "                else:\n",
    "                    alphas = [ 1, ]\n",
    "                    ps_trunc = [ None, ]\n",
    "                    linewidths = [ 1.5, ]\n",
    "                for j, p in enumerate( ps_trunc ):\n",
    "                    if p is not None:\n",
    "                        percentile = percentiles[pm['public_label']]['estimated']['individual'][sl][prop_key]['interval'][p][comp_key]\n",
    "                        is_not_truncated = ( centers_estimated > percentile[0] ) & ( centers_estimated < percentile[1] )\n",
    "                    else:\n",
    "                        is_not_truncated = np.ones( centers_estimated.shape ).astype( bool )\n",
    "                \n",
    "                    comp_plot = ax.plot(\n",
    "                        centers_estimated[is_not_truncated],\n",
    "                        comp_dist[prop_key][is_not_truncated],\n",
    "                        linewidth = 1,\n",
    "                        color = helpers.colors_for_variations[variation],\n",
    "                        alpha = alphas[j],\n",
    "                    )\n",
    "                    comp_plot[0].set_path_effects(\n",
    "                        [ path_effects.Stroke( linewidth=linewidths[j], foreground='k', ), path_effects.Normal() ],\n",
    "                    )\n",
    "            \n",
    "    # Final loop-through to finalize some tweaking\n",
    "    for ax_key, ax in ax_dict.items():\n",
    "        \n",
    "        # Adjust y limits\n",
    "        ylim = list( ax.get_ylim() )\n",
    "        if min_column != 'auto_per_sl':\n",
    "            ylim[0] = dNHI_dx_min[ax_key]\n",
    "        ylim[1] = np.array( y_maxs[ax_key] ).max() * 10**0.5\n",
    "        ax.set_ylim( ylim )\n",
    "        \n",
    "    # Add legend\n",
    "    ax_dict[mosaic_dist[0][-1]].legend(\n",
    "        bbox_to_anchor = (0.75, 1, -0.75, 0 ),\n",
    "        # bbox_transform = fig.transFigure,\n",
    "        loc = 'lower right',\n",
    "        ncol = 3,\n",
    "        framealpha = 0.,\n",
    "        borderaxespad = 0.,\n",
    "        mode = 'expand',\n",
    "    )\n",
    "        \n",
    "    save_fp = os.path.join( pm['figure_dir'], pm['variation'], 'sightline_{}.pdf'.format( sls[i] ) )\n",
    "    fig.savefig( save_fp, bbox_inches='tight' )\n",
    "            \n",
    "    dist_figs.append( fig )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "780aa8b2-4c6d-49a1-a7f9-0e54de3aa24f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "dist_figs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21b0debd-c985-4e75-8382-11da09c19177",
   "metadata": {},
   "source": [
    "## Violin Plots"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2efa1791-4b3b-4372-8ad7-9900b29f9e14",
   "metadata": {},
   "source": [
    "### Plotting Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0297d479-cee0-4620-b5bc-08ef2655f376",
   "metadata": {},
   "outputs": [],
   "source": [
    "panel_length = matplotlib.rcParams['figure.figsize'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "873ec109-6a1f-4c4a-9ba5-e5018b491124",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_violin(\n",
    "    get_violin_input,\n",
    "    pms,\n",
    "    fig_tuple=None,\n",
    "    color='k',\n",
    "    use_helper_colors=True,\n",
    "    alpha=0.5,\n",
    "    logscale_distributions=False,\n",
    "    use_builtin_violin_plot=False,\n",
    "    mosaic_dist=mosaic_dist,\n",
    "    panel_dimensions=( panel_length, panel_length/2. ),\n",
    "    label = None,\n",
    "):\n",
    "\n",
    "    if fig_tuple is None:\n",
    "        n_rows = len( mosaic_dist )\n",
    "        n_cols = len( mosaic_dist[0] )\n",
    "        panel_width, panel_height = panel_dimensions\n",
    "        fig = plt.figure( figsize=(n_cols*panel_width, n_rows*panel_height), facecolor='w' )\n",
    "        main_ax = plt.gca()\n",
    "        main_ax.tick_params( left=False, labelleft=False, bottom=False, labelbottom=False )\n",
    "        for spine in main_ax.spines.values():\n",
    "            spine.set_visible( False )\n",
    "\n",
    "        ax_dict = fig.subplot_mosaic(\n",
    "            mosaic_dist,\n",
    "        )\n",
    "    else:\n",
    "        fig, main_ax, ax_dict = fig_tuple\n",
    "        \n",
    "    # Axes tweaks\n",
    "    for ax_key, ax in ax_dict.items():\n",
    "\n",
    "        ylim = helpers.lims[ax_key]\n",
    "\n",
    "        # Logscale handling\n",
    "        axis_label = helpers.property_labels_no_units[ax_key]\n",
    "        if helpers.logscale[ax_key]:\n",
    "            axis_label = 'log' + axis_label\n",
    "            ylim = np.log10( ylim )\n",
    "\n",
    "        ax.set_ylim( ylim )\n",
    "        ax.set_xticks( xs )\n",
    "        xtick_labels = [ _[-2:] for _ in sls ]\n",
    "        ax.set_xticklabels( xtick_labels )\n",
    "                \n",
    "        ax.set_ylabel( helpers.property_labels[ax_key] )\n",
    "        \n",
    "        if ax.get_subplotspec().is_last_row():\n",
    "            ax.set_xlabel( 'sightline ID' )\n",
    "            \n",
    "        if ax.get_subplotspec().is_first_row() and ax.get_subplotspec().is_first_col():\n",
    "            text = r'width $\\propto ' + [ '', '\\log' ][int(logscale_distributions)] + r' N_{\\rm{HI}}$'\n",
    "            ax.annotate(\n",
    "                text,\n",
    "                xy = ( 0, 1 ),\n",
    "                xytext = ( 2, 2 ),\n",
    "                xycoords = 'axes fraction',\n",
    "                textcoords = 'offset points',\n",
    "                ha = 'left',\n",
    "                va = 'bottom',\n",
    "            )\n",
    "        \n",
    "    label_set = False\n",
    "    for variation, pm in pms.items():\n",
    "\n",
    "        for ax_key, ax in ax_dict.items():\n",
    "                \n",
    "            if use_helper_colors:\n",
    "                color = helpers.colors_for_variations[pm['variation']]\n",
    "\n",
    "            values = []\n",
    "            invalid_in_list = False\n",
    "            for i, sl in enumerate( sls ):\n",
    "                \n",
    "                centers, dist = get_violin_input( variation, sl, ax_key )\n",
    "                \n",
    "                if centers is None:\n",
    "                    invalid_in_list = True\n",
    "                    break\n",
    "\n",
    "                if helpers.logscale[ax_key]:\n",
    "                    centers = np.log10( centers )\n",
    "\n",
    "                # \"Y\" limits\n",
    "                if min_column == 'auto_per_sl':\n",
    "                    dist_min = dist[dist > 0].min()\n",
    "                else:\n",
    "                    dist_min = dNHI_dx_min[ax_key]\n",
    "                    \n",
    "                # Convert to logscale, apply limit\n",
    "                if logscale_distributions:\n",
    "                    dist[dist<dist_min] = dist_min\n",
    "                    dist = np.log10( dist )\n",
    "                    dist -= np.log10( dist_min )\n",
    "                else:\n",
    "                    dist[dist<dist_min] = 0.\n",
    "\n",
    "                # Resample, necessary when using built-in violin plot\n",
    "                if use_builtin_violin_plot:\n",
    "                    kde = kale.KDE.from_hist(\n",
    "                        centers,\n",
    "                        dist[:-1],\n",
    "                    )\n",
    "                    values.append( kde.resample( 1000 ) )\n",
    "                else:\n",
    "                    \n",
    "                    # Normalize\n",
    "                    dist /= dist.max()\n",
    "                    dist *= np.diff( xs )[0] * helpers.violin_width / 2.\n",
    "                    \n",
    "                    # Avoid drawing a long zero line\n",
    "                    nonzero_inds = np.arange( centers.size )[dist>0.]\n",
    "                    min_ind = nonzero_inds[0]\n",
    "                    max_ind = nonzero_inds[-1] + 1\n",
    "                    \n",
    "                    if not label_set:\n",
    "                        used_label = label\n",
    "                        label_set = True\n",
    "                    else:\n",
    "                        used_label = None\n",
    "                    ax.fill_betweenx(\n",
    "                        centers[min_ind:max_ind],\n",
    "                        xs[i] + dist[min_ind:max_ind],\n",
    "                        xs[i] - dist[min_ind:max_ind],\n",
    "                        color = color,\n",
    "                        alpha = alpha,\n",
    "                        label = used_label,\n",
    "                    )\n",
    "                \n",
    "            if invalid_in_list:\n",
    "                continue\n",
    "\n",
    "            if use_builtin_violin_plot:\n",
    "                v = ax.violinplot(\n",
    "                    values,\n",
    "                    xs,\n",
    "                    widths = np.diff( xs )[0] * helpers.violin_width,\n",
    "                    showextrema = False\n",
    "                )\n",
    "                for i, poly in enumerate( v['bodies'] ):\n",
    "                    poly.set_alpha( alpha )\n",
    "                    poly.set_color( color )\n",
    "                \n",
    "    return fig, main_ax, ax_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "959b1648-7563-4914-84b5-d6e3067657cb",
   "metadata": {},
   "source": [
    "### Source Violin, Rotated Distribution\n",
    "Want to make sure the violin plots are working..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a57544c-6eb3-4ec6-b526-333d7842c921",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source( variation, sl, prop_key ):\n",
    "    \n",
    "    pm = pms[variation]\n",
    "    \n",
    "    centers = copy.copy( absorption_properties[pm['public_label']][sl]['source']['1D distributions']['centers'][prop_key] )\n",
    "    dist = copy.copy( absorption_properties[pm['public_label']][sl]['source']['1D distributions']['distributions'][prop_key] )\n",
    "    \n",
    "    return centers, dist\n",
    "\n",
    "fig, main_ax, ax_dict = plot_violin(\n",
    "    get_source,\n",
    "    { variation: pm },\n",
    "    color='k',\n",
    "    use_helper_colors=False,\n",
    "    alpha=0.5,\n",
    "    logscale_distributions=True,\n",
    "    use_builtin_violin_plot=True,\n",
    ")\n",
    "\n",
    "fig, main_ax, ax_dict = plot_violin(\n",
    "    get_source,\n",
    "    { variation: pm },\n",
    "    fig_tuple = ( fig, main_ax, ax_dict ),\n",
    "    color='k',\n",
    "    use_helper_colors=False,\n",
    "    alpha=0.5,\n",
    "    logscale_distributions=True,\n",
    "    use_builtin_violin_plot=False,\n",
    ")\n",
    "\n",
    "# variation = 'original'\n",
    "# pm = pms[variation]\n",
    "# logscale_distributions = True\n",
    "\n",
    "# for ax_key, ax in ax_dict.items():\n",
    "\n",
    "#     values = []\n",
    "#     invalid_in_list = False\n",
    "#     for i, sl in enumerate( sls ):\n",
    "\n",
    "#         centers, dist = get_source( variation, sl, ax_key )\n",
    "\n",
    "#         if centers is None:\n",
    "#             invalid_in_list = True\n",
    "#             break\n",
    "\n",
    "#         if helpers.logscale[ax_key]:\n",
    "#             centers = np.log10( centers )\n",
    "\n",
    "#         # Convert distribution to logscale\n",
    "#         if logscale_distributions:\n",
    "\n",
    "#             if min_column == 'auto_per_sl':\n",
    "#                 dist_min = dist[dist > 0].min()\n",
    "#             else:\n",
    "#                 dist_min = dNHI_dx_min[ax_key]\n",
    "#             dist[dist<dist_min] = dist_min\n",
    "#             dist = np.log10( dist )\n",
    "#             dist -= np.log10( dist_min )    \n",
    "            \n",
    "#         dist /= dist.max()\n",
    "#         dist *= np.diff( xs )[0] * helpers.violin_width / 2.\n",
    "#         ax.fill_betweenx(\n",
    "#             centers,\n",
    "#             xs[i] + dist,\n",
    "#             xs[i] - dist,\n",
    "#             color = 'k',\n",
    "#             alpha = 0.5,\n",
    "#         )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d75050e-0476-4725-b90c-cc6f36a2fffc",
   "metadata": {},
   "source": [
    "Looks like something funky is going on with the built-in violin plot when it comes to $v_{\\rm LOS}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0eef189-86c8-46e5-ad6d-7ef8937b8d3c",
   "metadata": {},
   "source": [
    "### Source Violin, Other Source Violin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08c7a0b-ee84-4263-8811-a5289ffa6300",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source( variation, sl, prop_key ):\n",
    "    \n",
    "    pm = pms[variation]\n",
    "    \n",
    "    centers = copy.copy( absorption_properties[pm['public_label']][sl]['source']['1D distributions']['centers'][prop_key] )\n",
    "    dist = copy.copy( absorption_properties[pm['public_label']][sl]['source']['1D distributions']['distributions'][prop_key] )\n",
    "    \n",
    "    return centers, dist\n",
    "\n",
    "fig, main_ax, ax_dict = plot_violin( get_source, pms, color='k', use_helper_colors=False )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2dff86-b246-45af-b133-2317f061fda0",
   "metadata": {},
   "source": [
    "As expected, no significant differences between the source distribution between revisions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a16de83-1358-4bb1-8797-8baac27b8ed4",
   "metadata": {},
   "source": [
    "### Separated ( Source Violins, Estimated Violins )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ba66fd8-3ad4-4a1b-80d4-93ad90bf148d",
   "metadata": {},
   "source": [
    "#### Linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c7b4b30-6c2d-4b4f-872b-c44c667c6dcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "for variation, pm in pms.items():\n",
    "\n",
    "    def get_source( variation, sl, prop_key ):\n",
    "\n",
    "        pm = pms[variation]\n",
    "\n",
    "        centers = copy.copy( absorption_properties[pm['public_label']][sl]['source']['1D distributions']['centers'][prop_key] )\n",
    "        dist = copy.copy( absorption_properties[pm['public_label']][sl]['source']['1D distributions']['distributions'][prop_key] )\n",
    "\n",
    "        return centers, dist\n",
    "\n",
    "    def get_estimate( variation, sl, prop_key ):\n",
    "\n",
    "        centers = copy.copy( absorption_properties[pm['public_label']][sl]['estimated']['1D distributions']['centers'][prop_key] )\n",
    "        dist = copy.copy( absorption_properties[pm['public_label']][sl]['estimated']['1D distributions']['combined distributions'][prop_key] )\n",
    "\n",
    "        return centers, dist\n",
    "\n",
    "    fig_tuple = plot_violin( get_source, { variation: pm }, color='k', use_helper_colors=False, alpha=1 )\n",
    "\n",
    "    fig_tuple = plot_violin( get_estimate, { variation: pm }, fig_tuple=fig_tuple )\n",
    "    \n",
    "    save_fp = os.path.join( pm['figure_dir'], 'violin_{}.pdf'.format( pm['public_label'] ) )\n",
    "    plt.savefig( save_fp, bbox_inches='tight' )\n",
    "    \n",
    "    fig_tuple[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd3f8db7-4e19-4b4d-99e4-64bbe57a75f7",
   "metadata": {},
   "source": [
    "#### Logscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "654b6014-b878-4981-8b1e-1e3b84a5b28b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for variation, pm in pms.items():\n",
    "\n",
    "    def get_source( variation, sl, prop_key ):\n",
    "\n",
    "        pm = pms[variation]\n",
    "\n",
    "        centers = copy.copy( absorption_properties[pm['public_label']][sl]['source']['1D distributions']['centers'][prop_key] )\n",
    "        dist = copy.copy( absorption_properties[pm['public_label']][sl]['source']['1D distributions']['distributions'][prop_key] )\n",
    "\n",
    "        return centers, dist\n",
    "\n",
    "    def get_estimate( variation, sl, prop_key ):\n",
    "\n",
    "        centers = copy.copy( absorption_properties[pm['public_label']][sl]['estimated']['1D distributions']['centers'][prop_key] )\n",
    "        dist = copy.copy( absorption_properties[pm['public_label']][sl]['estimated']['1D distributions']['combined distributions'][prop_key] )\n",
    "\n",
    "        return centers, dist\n",
    "\n",
    "    fig_tuple = plot_violin( get_source, { variation: pm }, color='k', use_helper_colors=False, alpha=1, logscale_distributions=True )\n",
    "\n",
    "    fig_tuple = plot_violin( get_estimate, { variation: pm }, fig_tuple=fig_tuple, logscale_distributions=True )\n",
    "    \n",
    "    fig_tuple[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a3d5a7b-181f-4ee9-ba45-037ac075d5b4",
   "metadata": {},
   "source": [
    "### Source Violin, Estimated Violin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86a9f8a-5de3-4c1c-91e6-fe925b549ae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source( variation, sl, prop_key ):\n",
    "    \n",
    "    if variation != 'original':\n",
    "        return None, None\n",
    "\n",
    "    pm = pms[variation]\n",
    "\n",
    "    centers = copy.copy( absorption_properties[pm['public_label']][sl]['source']['1D distributions']['centers'][prop_key] )\n",
    "    dist = copy.copy( absorption_properties[pm['public_label']][sl]['source']['1D distributions']['distributions'][prop_key] )\n",
    "\n",
    "    return centers, dist\n",
    "\n",
    "def get_estimate( variation, sl, prop_key ):\n",
    "    \n",
    "    pm = pms[variation]\n",
    "\n",
    "    centers = copy.copy( absorption_properties[pm['public_label']][sl]['estimated']['1D distributions']['centers'][prop_key] )\n",
    "    dist = copy.copy( absorption_properties[pm['public_label']][sl]['estimated']['1D distributions']['combined distributions'][prop_key] )\n",
    "\n",
    "    return centers, dist\n",
    "\n",
    "fig_tuple = plot_violin( get_source, pms, color='k', use_helper_colors=False, alpha=1 )\n",
    "\n",
    "fig_tuple = plot_violin( get_estimate, pms, fig_tuple=fig_tuple )\n",
    "\n",
    "save_fp = os.path.join( pm['figure_dir'], 'violin.pdf' )\n",
    "plt.savefig( save_fp, bbox_inches='tight' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "856c7b5e-f45d-46ea-bca5-9875b82ed148",
   "metadata": {},
   "source": [
    "### Source Violin, Estimated Points"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a355c39-0817-4869-9f41-b0e313a173e1",
   "metadata": {},
   "source": [
    "#### Scaled alpha with error bars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf5aa617-af53-4022-8f46-fd3bf3adc84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "elongated_mosaic_dist = [ [ 'vlos', ], [ 'Z', ], [ 'T', ], [ 'nH' ], ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1476102-78d7-4e36-8939-0ebfd1981e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_variations = [\n",
    "    None,\n",
    "    'revised',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c3ce01e-e88a-40f8-b2d6-0a0d240e4161",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source( variation, sl, prop_key ):\n",
    "    \n",
    "    if variation != 'original':\n",
    "        return None, None\n",
    "\n",
    "    pm = pms[variation]\n",
    "\n",
    "    centers = copy.copy( absorption_properties[pm['public_label']][sl]['source']['1D distributions']['centers'][prop_key] )\n",
    "    dist = copy.copy( absorption_properties[pm['public_label']][sl]['source']['1D distributions']['distributions'][prop_key] )\n",
    "\n",
    "    return centers, dist\n",
    "\n",
    "# Plot individual components\n",
    "for ii, plot_variation in enumerate( plot_variations ):\n",
    "    \n",
    "    fig, main_ax, ax_dict = plot_violin(\n",
    "        get_source,\n",
    "        pms,\n",
    "        color='k',\n",
    "        use_helper_colors=False,\n",
    "        alpha=1,\n",
    "        logscale_distributions=True,\n",
    "        mosaic_dist=elongated_mosaic_dist,\n",
    "        panel_dimensions=(panel_length*2., panel_length/2.),\n",
    "        label = 'source',\n",
    "    )\n",
    "\n",
    "    for variation, pm in pms.items():\n",
    "        \n",
    "        if plot_variation is not None:\n",
    "            if pm['public_label'] != plot_variation:\n",
    "                continue\n",
    "        \n",
    "        for ax_key, ax in ax_dict.items():\n",
    "            label_set = False\n",
    "            for i, sl in enumerate( sls ):\n",
    "\n",
    "                components = absorption_properties[pm['public_label']][sl]['estimated']['1D distributions']['individual distributions']\n",
    "                centers = copy.copy( absorption_properties[pm['public_label']][sl]['estimated']['1D distributions']['centers'][ax_key] )\n",
    "\n",
    "                # Get source data for normalization\n",
    "                # Normalization spans between the integral of the full distribution\n",
    "                # to the integral of the min_column.\n",
    "                column_source = columns[pm['public_label']]['source']['total column'][ax_key][sl]\n",
    "                if min_column == 'auto_per_sl':\n",
    "                    column_min_source = columns[pm['public_label']]['source']['minimum column'][ax_key][sl]\n",
    "                else:\n",
    "                    column_min_source = min_column\n",
    "                alpha_mapping = matplotlib.colors.LogNorm( column_min_source, column_source )\n",
    "\n",
    "                if helpers.logscale[ax_key]:\n",
    "                    centers = np.log10( centers )\n",
    "\n",
    "                # Sort components to put highest col components at center\n",
    "                comp_columns = []\n",
    "                comp_keys = []\n",
    "                for comp_key, comp in components.items():\n",
    "                    comp_columns.append( columns[pm['public_label']]['estimated']['individual columns'][sl][comp_key][ax_key] )\n",
    "                    comp_keys.append( comp_key )\n",
    "                comp_keys_sorted = np.array( comp_keys )[np.argsort( comp_columns )][::-1]\n",
    "\n",
    "                if plot_variation is None:\n",
    "                    xs_comps = (\n",
    "                        xs[i] +\n",
    "                        (\n",
    "                            ( xs[1] - xs[0] ) / 2. *\n",
    "                            np.linspace( 0, 1, len( components ) + 2 )[1:-1] *\n",
    "                            np.sign( variation_plotting_params[variation]['offset'] )\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "                else:\n",
    "                    xs_comps = []\n",
    "                    for j in range( len( comp_keys ) ):\n",
    "                        # counting up\n",
    "                        x_comp = ( j + 1 ) // 2\n",
    "                        # j_offset = ( j + 1 ) // 2\n",
    "                        # x_comp = ( j_offset / ( len( comp_keys ) - 1 ) )\n",
    "                        # scale to distance between\n",
    "                        # if j % 2 == 1:\n",
    "                        x_comp *= ( xs[1] - xs[0] ) * ( helpers.violin_width / 2 ) / 2\n",
    "                        # x_comp *= x_comp_offset\n",
    "                    \n",
    "                        # alternate sides\n",
    "                        x_comp *= (-1. )**j\n",
    "                        # Move to right location\n",
    "                        x_comp += xs[i]\n",
    "                        xs_comps.append( x_comp )\n",
    "                    \n",
    "                for j, comp_key in enumerate( comp_keys_sorted ):\n",
    "                    comp = components[comp_key]\n",
    "\n",
    "                    # Get alpha\n",
    "                    comp_column = columns[pm['public_label']]['estimated']['individual columns'][sl][comp_key][ax_key]\n",
    "                    # Components that integrate to more than full distribution\n",
    "                    # are set to an alpha value of 1\n",
    "                    if comp_column > column_source:\n",
    "                        comp_column = column_source\n",
    "                    # Set components less than the minimum to alpha of zero\n",
    "                    elif comp_column < column_min_source:\n",
    "                        comp_column = column_min_source\n",
    "                    alpha = alpha_mapping( comp_column )\n",
    "\n",
    "                    # Scale alphas to alpha min\n",
    "                    alpha_min = 0.2\n",
    "                    alpha = alpha * ( 1. - alpha_min ) + alpha_min\n",
    "\n",
    "                    mle = centers[comp[ax_key].argmax()]\n",
    "\n",
    "                    if not label_set:\n",
    "                        used_label = 'estimated\\u2014{}'.format( pm['public_label'] )\n",
    "                        label_set = True\n",
    "                    else:\n",
    "                        used_label = None\n",
    "                    s = ax.scatter(\n",
    "                        xs_comps[j],\n",
    "                        mle,\n",
    "                        color = helpers.colors_for_variations[variation],\n",
    "                        edgecolor = 'none',\n",
    "                        linewidth = 0.5,\n",
    "                        alpha = alpha,\n",
    "                        s = matplotlib.rcParams['lines.markersize'] * 3,\n",
    "                        zorder = 100,\n",
    "                        label = used_label\n",
    "                    )\n",
    "                    s = ax.scatter(\n",
    "                        xs_comps[j],\n",
    "                        mle,\n",
    "                        color = 'none',\n",
    "                        edgecolor = helpers.colors_for_variations[variation],\n",
    "                        linewidth = 1,\n",
    "                        s = matplotlib.rcParams['lines.markersize'] * 3 * 1.5,\n",
    "                        zorder = 99,\n",
    "                    )\n",
    "\n",
    "                    percentile_interval = percentiles[pm['public_label']]['estimated']['individual'][sl][ax_key]['interval']['68.0'][comp_key]\n",
    "                    ax.plot(\n",
    "                        [ xs_comps[j], ] * 2,\n",
    "                        percentile_interval,\n",
    "                        color = helpers.colors_for_variations[variation],\n",
    "                        linewidth = 1,\n",
    "                        alpha = alpha,\n",
    "                    )\n",
    "                    \n",
    "    # Add legend\n",
    "    ax_dict[elongated_mosaic_dist[0][-1]].legend(\n",
    "        bbox_to_anchor = (1, 1),\n",
    "        loc = 'lower right',\n",
    "        ncol = 3 - ii,\n",
    "        framealpha = 0.,\n",
    "        borderaxespad = 0.,\n",
    "    )\n",
    "\n",
    "    if plot_variation is not None:\n",
    "        savefile = 'violin_vs_components_{}.pdf'.format( plot_variation )\n",
    "    else:\n",
    "        savefile = 'violin_vs_components.pdf'\n",
    "                    \n",
    "    save_fp = os.path.join( pm['figure_dir'], savefile )\n",
    "    plt.savefig( save_fp, bbox_inches='tight' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c06c8426-787f-43be-a92d-b3bda26224f4",
   "metadata": {},
   "source": [
    "#### Scaled alpha"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12360241-dcf1-4c21-a37e-98eff336869c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source( variation, sl, prop_key ):\n",
    "    \n",
    "    if variation != 'original':\n",
    "        return None, None\n",
    "\n",
    "    pm = pms[variation]\n",
    "\n",
    "    centers = copy.copy( absorption_properties[pm['public_label']][sl]['source']['1D distributions']['centers'][prop_key] )\n",
    "    dist = copy.copy( absorption_properties[pm['public_label']][sl]['source']['1D distributions']['distributions'][prop_key] )\n",
    "\n",
    "    return centers, dist\n",
    "\n",
    "fig, main_ax, ax_dict = plot_violin(\n",
    "    get_source,\n",
    "    pms,\n",
    "    color='k',\n",
    "    use_helper_colors=False,\n",
    "    alpha=1,\n",
    "    logscale_distributions=True,\n",
    ")\n",
    "\n",
    "# Plot individual components\n",
    "for ax_key, ax in ax_dict.items():\n",
    "    for variation, pm in pms.items():\n",
    "        for i, sl in enumerate( sls ):\n",
    "        \n",
    "            components = absorption_properties[pm['public_label']][sl]['estimated']['1D distributions']['individual distributions']\n",
    "            centers = copy.copy( absorption_properties[pm['public_label']][sl]['estimated']['1D distributions']['centers'][ax_key] )\n",
    "            \n",
    "            # Get source data for normalization\n",
    "            # Normalization spans between the integral of the full distribution\n",
    "            # to the integral of the min_column.\n",
    "            column_source = columns[pm['public_label']]['source']['total column'][ax_key][sl]\n",
    "            if min_column == 'auto_per_sl':\n",
    "                column_min_source = columns[pm['public_label']]['source']['minimum column'][ax_key][sl]\n",
    "            else:\n",
    "                column_min_source = min_column\n",
    "            alpha_mapping = matplotlib.colors.LogNorm( column_min_source, column_source )\n",
    "            \n",
    "            if helpers.logscale[ax_key]:\n",
    "                centers = np.log10( centers )\n",
    "            \n",
    "            xs_comp = xs[i] + variation_plotting_params[variation]['offset'] * ( xs[1] - xs[0] )\n",
    "            for j, ( comp_key, comp ) in enumerate( components.items() ):\n",
    "                \n",
    "                # Get alpha\n",
    "                comp_column = columns[pm['public_label']]['estimated']['individual columns'][sl][comp_key][ax_key]\n",
    "                # Components that integrate to more than full distribution\n",
    "                # are set to an alpha value of 1\n",
    "                if comp_column > column_source:\n",
    "                    comp_column = column_source\n",
    "                # Set components less than the minimum to alpha of zero\n",
    "                elif comp_column < column_min_source:\n",
    "                    comp_column = column_min_source\n",
    "                alpha = alpha_mapping( comp_column )\n",
    "                \n",
    "                # Scale alphas to alpha min\n",
    "                alpha_min = 0.2\n",
    "                alpha = alpha * ( 1. - alpha_min ) + alpha_min\n",
    "                \n",
    "                mle = centers[comp[ax_key].argmax()]\n",
    "                \n",
    "                ax.scatter(\n",
    "                    xs_comp,\n",
    "                    mle,\n",
    "                    color = helpers.colors_for_variations[variation],\n",
    "                    edgecolor = 'w',\n",
    "                    linewidth = 0.5,\n",
    "                    alpha = alpha,\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c324be5-8920-47a3-9bef-56c68454579d",
   "metadata": {},
   "source": [
    "#### Scaled length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b73d2dd-abe0-45a6-a9c4-e069248aae6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_source( variation, sl, prop_key ):\n",
    "    \n",
    "    if variation != 'original':\n",
    "        return None, None\n",
    "\n",
    "    pm = pms[variation]\n",
    "\n",
    "    centers = copy.copy( absorption_properties[pm['public_label']][sl]['source']['1D distributions']['centers'][prop_key] )\n",
    "    dist = copy.copy( absorption_properties[pm['public_label']][sl]['source']['1D distributions']['distributions'][prop_key] )\n",
    "\n",
    "    return centers, dist\n",
    "\n",
    "fig, main_ax, ax_dict = plot_violin( get_source, pms, color='k', use_helper_colors=False, alpha=1, logscale_distributions=True, )\n",
    "\n",
    "# Plot individual components\n",
    "for ax_key, ax in ax_dict.items():\n",
    "    for variation, pm in pms.items():\n",
    "        for i, sl in enumerate( sls ):\n",
    "        \n",
    "            components = absorption_properties[pm['public_label']][sl]['estimated']['1D distributions']['individual distributions']\n",
    "            centers = copy.copy( absorption_properties[pm['public_label']][sl]['estimated']['1D distributions']['centers'][ax_key] )\n",
    "            \n",
    "            if helpers.logscale[ax_key]:\n",
    "                centers = np.log10( centers )\n",
    "                \n",
    "            \n",
    "            # Get source data for normalization\n",
    "            # Normalization spans between the integral of the full distribution\n",
    "            # to the integral of the min_column.\n",
    "            column_source = columns[pm['public_label']]['source']['total column'][ax_key][sl]\n",
    "            if min_column == 'auto_per_sl':\n",
    "                column_min_source = columns[pm['public_label']]['source']['minimum column'][ax_key][sl]\n",
    "            else:\n",
    "                column_min_source = min_column\n",
    "            length_mapping = matplotlib.colors.LogNorm( column_min_source, column_source )\n",
    "            \n",
    "            for comp_key, comp in components.items():\n",
    "                \n",
    "                # Get length\n",
    "                comp_column = columns[pm['public_label']]['estimated']['individual columns'][sl][comp_key][ax_key]\n",
    "                # Components that integrate to more than full distribution\n",
    "                # are set to an length value of 1\n",
    "                if comp_column > column_source:\n",
    "                    comp_column = column_source\n",
    "                # Set components less than the minimum to alpha of zero\n",
    "                elif comp_column < column_min_source:\n",
    "                    comp_column = column_min_source\n",
    "                length = length_mapping( comp_column )\n",
    "                length *= helpers.violin_width * ( xs[1] - xs[0] )\n",
    "                \n",
    "                mle = centers[comp[ax_key].argmax()]\n",
    "                \n",
    "                ax.plot(\n",
    "                    [ xs[i] - length / 2., xs[i] + length / 2. ],\n",
    "                    [ mle, ] * 2,\n",
    "                    color = helpers.colors_for_variations[variation],\n",
    "                    linewidth = 2,\n",
    "                    # alpha = alphas[comp_key],\n",
    "                )\n",
    "\n",
    "save_fp = os.path.join( pm['figure_dir'], 'violin_vs_components_alternate.pdf' )\n",
    "plt.savefig( save_fp, bbox_inches='tight' )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc052bbc-be19-4512-b66e-428b4b2874eb",
   "metadata": {},
   "source": [
    "## Shape comparison\n",
    "Fraction enclosed compared to fraction enclosed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3282a84a-bbdd-444f-a651-4954acdcc54f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "dist_comparison_figs = []\n",
    "for i, sl in enumerate( sls ):\n",
    "\n",
    "    # Setup figure\n",
    "    fig = plt.figure()\n",
    "    main_ax = plt.gca()\n",
    "    main_ax.tick_params( left=False, labelleft=False, bottom=False, labelbottom=False )\n",
    "    for spine in main_ax.spines.values():\n",
    "        spine.set_visible( False )\n",
    "\n",
    "    ax_dict = fig.subplot_mosaic(\n",
    "        mosaic_dist,\n",
    "    )\n",
    "\n",
    "    main_ax.set_xlabel( '% of likelihood distribution enclosed', labelpad=30 )\n",
    "    main_ax.set_ylabel( '% of synthetic data enclosed', labelpad=30 )\n",
    "\n",
    "    for ax_key, ax in ax_dict.items():\n",
    "        ax.set_xlim( 0, 100 )\n",
    "        ax.set_ylim( 0, 100 )\n",
    "\n",
    "        ax.annotate(\n",
    "            text = helpers.property_labels_no_units[ax_key],\n",
    "            xy = ( 0, 1 ),\n",
    "            xycoords = 'axes fraction',\n",
    "            xytext = ( 5, -5 ),\n",
    "            textcoords = 'offset points',\n",
    "            va = 'top',\n",
    "            ha = 'left',\n",
    "        )\n",
    "        \n",
    "        ax.plot(\n",
    "            [ 0, 100 ],\n",
    "            [ 0, 100 ],\n",
    "            color = '0.5',\n",
    "            linewidth = 1.5,\n",
    "            zorder = -110,\n",
    "        )\n",
    "\n",
    "    main_ax.set_title(\n",
    "        label = sl,\n",
    "        loc = 'right',\n",
    "    )\n",
    "\n",
    "    for variation, pm in pms.items():\n",
    "\n",
    "        for prop_key in absorption_properties[pm['public_label']][sl]['source']['1D distributions']['centers'].keys():\n",
    "\n",
    "            # Get plot panel\n",
    "            ax = ax_dict[prop_key]\n",
    "            subplotspec = ax.get_subplotspec()\n",
    "\n",
    "            ax.plot(\n",
    "                f_included_estimated * 100,\n",
    "                f_included_data * 100,\n",
    "                color = helpers.colors_for_variations[pm['variation']],\n",
    "            )\n",
    "            \n",
    "    dist_comparison_figs.append( fig )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ad6dee-3e03-466a-b7cc-24ea820403f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_comparison_figs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "871c7029-064c-4638-bca6-4b69e257c264",
   "metadata": {},
   "source": [
    "# Compile Summary Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514fb606-c9dd-4034-bc01-94f42244704c",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = verdict.Dict({\n",
    "    'averages': averages,\n",
    "    'distances_between': distances_between,\n",
    "    'percentiles': percentiles,\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36a902c7-c716-465a-b83d-88cc9a2d5500",
   "metadata": {},
   "outputs": [],
   "source": [
    "total_summary = verdict.Dict.from_hdf5( pm['summary_data_fp'], create_nonexistent=True )\n",
    "total_summary['sample2'] = summary\n",
    "total_summary.to_hdf5(  pm['summary_data_fp'], )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36cf5d27-d418-4213-bd29-14b92a231aa0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
